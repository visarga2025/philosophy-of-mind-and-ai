is inadequate. Searle argues that the thought experiment underscores the fact that computers merely use syntactic rules to manipulate symbol strings, but have no understanding of meaning or semantics. The broader conclusion of the argument is that the theory that human minds are computer-like computational or information processing systems is refuted. Instead minds must result from biological processes; computers can at best simulate these biological processes. Thus the argument has large implications for semantics, philosophy of language and mind, theories of consciousness, computer science, and cognitive science generally. As a result, there have been many critical replies to the argument.

1. Overview
2. Historical Background
2.1 Leibniz’ Mill
2.2 Turing’s Paper Machine
2.3 The Chinese Nation
3. The Chinese Room Argument
4. Replies to the Chinese Room Argument
4.1 The Systems Reply
4.2 The Robot Reply
4.3 The Brain Simulator Reply
4.4 The Other Minds Reply
4.5 The Intuition Reply
4.6 Advances in Artificial intelligence
5. The Larger Philosophical Issues
5.1 Syntax and Semantics
5.2 Intentionality
5.3 Mind and Body
5.4 Simulation, duplication and evolution
Conclusion
Bibliography
Academic Tools
Other Internet Resources
Related Entries
1. Overview
Work in Artificial Intelligence (AI) has produced computer programs that can beat the world chess champion, control autonomous vehicles, and defeat the best human players on the television quiz show Jeopardy. By 2022 AI had evolved from personal digital assistants (Alexa, Siri, Google Assistant) translating and answering questions to using Large Language Models (LLMs) that could write poems, college level essays, and computer programs, and could pass exams designed to screen the entrants into graduate schools, the study and practice of Law, and other “learned professions”. Our experience shows that playing chess or Jeopardy, writing essays, passing difficult exams, and carrying on a conversation, are activities that require understanding and intelligence. Does computer prowess at conversation, writing essays, and passing difficult examinations then show that computers can understand language and be intelligent? Will further development result in digital computers that fully match or even exceed human intelligence?

Alan Turing (1950), one of the pioneer theoreticians of computing, believed the answer to these questions was “yes”. Turing proposed what is now known as ‘The Turing Test’: if a computer can pass for human in online chat, we should grant that it is intelligent. By the late 1970s some AI researchers claimed that computers already understood at least some natural language. In 1980 U.C. Berkeley philosopher John Searle introduced a short and widely-discussed argument intended to show conclusively that it is impossible for digital computers to understand language or think, now or in the future

Searle argues that a good way to test a theory of mind, say a theory that holds that understanding can be created by doing such and such, is to imagine what it would be like to actually do what the theory says will create understanding. Searle (1999) summarized his Chinese Room Argument (hereinafter, CRA) concisely:

Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese.
Searle goes on to say, “The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have.”

Thirty years after introducing the CRA Searle 2010 describes the conclusion in terms of consciousness and intentionality:

I demonstrated years ago with the so-called Chinese Room Argument that the implementation of the computer program is not by itself sufficient for consciousness or intentionality (Searle 1980). Computation is defined purely formally or syntactically, whereas minds have actual mental or semantic contents, and we cannot get from syntactical to the semantic just by having the syntactical operations and nothing else. To put this point slightly more technically, the notion “same implemented program” defines an equivalence class that is specified independently of any specific physical realization. But such a specification necessarily leaves out the biologically specific powers of the brain to cause cognitive processes. A system, me, for example, would not acquire an understanding of Chinese just by going through the steps of a computer program that simulated the behavior of a Chinese speaker (p.17).
“Intentionality” is a technical term for a feature of mental and certain other things, namely being about something. Thus a desire for a piece of chocolate as well as thoughts about real-world Manhattan or fictional Harry Potter all display intentionality, as will be discussed in more detail in section 5.2 below.

Searle’s shift from machine understanding to consciousness and intentionality is not directly supported by the original 1980 argument. However the re-description of the conclusion indicates the close connection between understanding and consciousness in Searle’s later accounts of meaning and intentionality. Those who don’t accept Searle’s linking of understanding and consciousness might hold that running a program can create understanding without necessarily creating consciousness, and conversely a fancy robot might have dog level consciousness, desires, and beliefs, without necessarily understanding natural language.

In moving to discussion of intentionality Searle seeks to develop the broader implications of his argument. It aims to refute the functionalist approach to understanding minds, that is, the approach that holds that mental states are defined by their causal roles, not by the stuff (neurons, transistors) that plays those roles. The argument counts especially against that form of functionalism known as the Computational Theory of Mind that treats minds as information processing systems. As a result of its scope, as well as Searle’s clear and forceful writing style, the Chinese Room argument has probably been the most widely discussed philosophical argument in cognitive science to appear since the Turing Test. By 1991 computer scientist Pat Hayes had defined Cognitive Science as the ongoing research project of refuting Searle’s argument. Cognitive psychologist Steven Pinker (1997) pointed out that by the mid-1990s well over 100 articles had been published on Searle’s thought experiment – and that discussion of it was so pervasive on the Internet that Pinker found it a compelling reason to remove his name from all Internet discussion lists.

This interest has not subsided, and the range of connections with the argument has broadened. A search on Google Scholar for “Chinese Room Argument” produces thousands of results, including papers making connections between the argument and topics ranging from embodied cognition to theater to talk psychotherapy to postmodern views of truth and “our post-human future” – as well as discussions of group or collective minds, and discussions of the role of intuitions in philosophy. In 2007 a UK game company took the name “The Chinese Room” in joking honor of “...Searle’s critique of AI – that you could create a system that gave the impression of intelligence without any actual internal smarts.” This wide-range of discussion and implications is a tribute to the argument’s simple clarity and centrality.

2. Historical Background
2.1 Leibniz’ Mill
Searle’s argument has four important antecedents. The first of these is an argument set out by the philosopher and mathematician Gottfried Leibniz (1646–1716). This argument, often known as “Leibniz’ Mill”, appears as section 17 of Leibniz’ Monadology. Like Searle’s argument, Leibniz’ argument takes the form of a thought experiment. Leibniz asks us to imagine a physical system, a machine, that behaves in such a way that it supposedly thinks and has experiences (“perception”).

17. Moreover, it must be confessed that perception and that which depends upon it are inexplicable on mechanical grounds, that is to say, by means of figures and motions. And supposing there were a machine, so constructed as to think, feel, and have perception, it might be conceived as increased in size, while keeping the same proportions, so that one might go into it as into a mill. That being so, we should, on examining its interior, find only parts which work one upon another, and never anything by which to explain a perception. Thus it is in a simple substance, and not in a compound or in a machine, that perception must be sought for. [Robert Latta translation]
Notice that Leibniz’s strategy here is to contrast the overt behavior of the machine, which might appear to be the product of conscious thought, with the way the machine operates internally. He points out that these internal mechanical operations are just parts moving from point to point, hence there is nothing that is conscious or that can explain thinking, feeling or perceiving. For Leibniz physical states are not sufficient for, nor constitutive of, mental states.

To this day the mystery of consciousness remains; one can still follow Leibniz’ suggestion and imagine a brain made so huge that one could walk between the neurons, and all one would see is, at best, squirts of neurotransmitters, and nothing to explain conscious experience, including the experience of understanding language. Leibniz’ argument, that no matter what a physical system does, there would be no consciousness (and so materialism is refuted), is parallel to Searle’s claim that no matter what syntactic processing there is, there would be no understanding of meaning (and so strong AI claims are refuted).

2.2 Turing’s Paper Machine
A second antecedent to the Chinese Room argument is the idea of a paper machine, a computer implemented by a human. This idea is found in the work of Alan Turing, for example in “Intelligent Machinery” (1948). Turing writes there that he wrote a program for a “paper machine” to play chess. A paper machine is a kind of program, a series of simple steps like a computer program, but written in natural language (e.g., English), and implemented by a human. The human operator of the paper chess-playing machine need not (otherwise) know how to play chess. All the operator does is follow the instructions for generating moves on the chess board. In fact, the operator need not even know that he or she is involved in playing chess – the input and output strings, such as “N–QB7” need mean nothing to the operator of the paper machine.

As part of the WWII project to decipher German military encryption, Turing had written English-language programs for human “computers”, as these specialized workers were then known, and these human computers did not need to know what the programs that they implemented were doing.

One reason the idea of a human-plus-paper machine is important is that it already raises questions about agency and understanding similar to those in the CRA. Suppose I am alone in a closed room and follow an instruction book for manipulating strings of symbols. I thereby implement a paper machine that generates symbol strings such as “N-KB3” that I write on pieces of paper and slip under the door to someone ouside the room. Suppose further that prior to going into the room I don’t know how to play chess, or even that there is such a game. However, unbeknownst to me, in the room I am running Turing’s chess program and the symbol strings I generate are chess notation and are taken as chess moves by those outside the room. They reply by sliding the symbols for their own moves back under the door into the room. If all you see is the resulting sequence of moves displayed on a chess board outside the room, you might think that someone in the room knows how to play chess very well. Do I now know how to play chess? Or is it the system (consisting of me, the manuals, and the paper on which I manipulate strings of symbols) that is playing chess? If I memorize the program and do the symbol manipulations inside my head, do I then know how to play chess, albeit with an odd phenomenology? Do someone’s conscious states matter for whether or not they know how to play chess? If a digital computer implements the same program, does the computer (or program or computer plus program) then play chess, or merely simulate this?

By mid-century Turing was optimistic that the newly developed electronic computers themselves would soon be able to exhibit apparently intelligent behavior, answering questions posed in English and carrying on conversations. Turing (1950) proposed what is now known as the Turing Test: if a computer could pass for human in on-line chat, it should be counted as intelligent.

A third antecedent of Searle’s argument was the work of Searle’s colleague at Berkeley, Hubert Dreyfus. Dreyfus was an early critic of the optimistic claims made by AI researchers. In 1965, when Dreyfus was at MIT, he published a circa hundred page report titled “Alchemy and Artificial Intelligence”. Dreyfus argued that key features of human mental life could not be captured by formal rules for manipulating symbols. Dreyfus moved to Berkeley in 1968 and in 1972 published his extended critique, “What Computers Can’t Do”. Dreyfus’ primary research interests were in Continental philosophy, with its focus on consciousness, intentionality, and the role of intuition and the inarticulated background in shaping our understandings. Dreyfus identified several problematic assumptions in AI, including the view that brains are like digital computers, and, again, the assumption that understanding can be codified as explicit rules.

However by the late 1970s, as computers became faster and less expensive, some in the burgeoning AI community started to claim that their programs could understand English sentences, using a database of background information. The work of one of these, Yale researcher Roger Schank (Schank & Abelson 1977) came to Searle’s attention. Schank’s team developed a technique called “conceptual representation” that used “scripts” to represent conceptual relations (related to Conceptual Role Semantics). Searle’s argument was originally presented in 1980 specifically as a response to the claim that AI programs such as Schank’s literally understand the sentences that they respond to.

2.3 The Chinese Nation
A fourth antecedent to the Chinese Room argument are thought experiments involving myriad humans acting as a computer. In 1961 Anatoly Mickevich (pseudonym A. Dneprov) published “The Game”, a story in which a stadium full of 1400 math students are arranged to function as a digital computer (see Dneprov 1961 and the English translation listed at Mickevich 1961, Other Internet Resources). For 4 hours each student repeatedly does a bit of calculation on binary numbers received from someone near them, then passes the binary result onto someone nearby. They learn the next day that they collectively translated a sentence from Portuguese into their native Russian. Mickevich’s protagonist concludes “We’ve proven that even the most perfect simulation of machine thinking is not the thinking process itself, which is a higher form of motion of living matter.”

Apparently independently, a similar consideration emerged in early discussion of functionalist theories of minds and cognition (see further discussion in section 5.3 below), Functionalists hold that mental states are defined by the causal role they play in a system (just as being a door stop is defined by what it does, not by what it is made out of). Critics of functionalism were quick to turn its proclaimed virtue of multiple realizability against it.

By emphasizing causal or information processing roles as the essence of mental states, functionalism allowed us to understand creatures with different physiology, for example extraterrestrials, to have the same types of mental states as humans – pains, for example. But it was pointed out that if extraterrestrial aliens, with some other complex system in place of brains, could realize the functional properties that constituted mental states, then, presumably so could systems even less like human brains. The computational form of functionalism, which holds that the defining role of each mental state is its role in information processing or computation, is particularly vulnerable to this maneuver, since a wide variety of systems with simple components are computationally equivalent (see e.g., Maudlin 1989 for discussion of a computer built from buckets of water). Critics asked if it was really plausible that these inorganic systems could have mental states or feel pain.

Daniel Dennett (1978) reports that in 1974 Lawrence Davis gave a colloquium at MIT in which he presented one such unorthodox implementation. Dennett summarizes Davis’ thought experiment as follows:

Let a functionalist theory of pain (whatever its details) be instantiated by a system the subassemblies of which are not such things as C-fibers and reticular systems but telephone lines and offices staffed by people. Perhaps it is a giant robot controlled by an army of human beings that inhabit it. When the theory’s functionally characterized conditions for pain are now met we must say, if the theory is true, that the robot is in pain. That is, real pain, as real as our own, would exist in virtue of the perhaps disinterested and businesslike activities of these bureaucratic teams, executing their proper functions.
In “Troubles with Functionalism”, also published in 1978, Ned Block envisions the entire population of China implementing the functions of neurons in the brain. This scenario has subsequently been called “The Chinese Nation” or “The Chinese Gym”. We can suppose that every Chinese citizen would be given a call-list of phone numbers, and at a preset time on implementation day, designated “input” citizens would initiate the process by calling those on their call-list. When any citizen’s phone rang, he or she would then phone those on his or her list, who would in turn contact yet others. No phone message need be exchanged; all that is required is the pattern of calling. The call-lists would be constructed in such a way that the patterns of calls implemented the same patterns of activation that occur between neurons in someone’s brain when that person is in a mental state – pain, for example. The phone calls play the same functional role as neurons causing one another to fire. Block was primarily interested in qualia, and in particular, whether it is plausible to hold that the population of China might collectively be in pain, while no individual member of the population experienced any pain, but the thought experiment applies to any mental states and operations, including understanding language.

Thus Block’s thought experiment, as with those of Davis and Dennett, is a system of many humans rather than one. The focus is on consciousness, but to the extent that Searle’s argument also involves consciousness, the thought experiment is closely related to Searle’s. Cole (1984) tries to pump intuitions in the reverse direction by setting out a thought experiment in which each of his neurons is itself conscious, and fully aware of its actions including being doused with neurotransmitters, undergoing action potentials, and squirting neurotransmitters at its neighbors. Cole argues that his conscious neurons would find it implausible that their collective activity produced a consciousness and other cognitive competences, including understanding English, that the neurons lack. That is, the mental states achieved by the activity of my neurons are my mental states, not those of any of my neurons – so if my neurons thought in Chinese (only), that would not show that they don’t collectively produce someone –me– who understands English but not Chinese.) Cole suggests that the intuitions of implementing systems are not to be trusted.

3. The Chinese Room Argument
In 1980 John Searle published “Minds, Brains and Programs” in the journal The Behavioral and Brain Sciences. In this article, Searle sets out the argument, and then replies to the half-dozen main objections that had been raised during his earlier presentations at various university campuses (see next section). In addition, Searle’s article in BBS was published along with comments and criticisms by 27 cognitive science researchers. These 27 comments were followed by Searle’s replies to his critics.

In the decades following its publication, the Chinese Room argument was the subject of very many discussions. By 1984, Searle presented the Chinese Room argument in a book, Minds, Brains and Science. In January 1990, the popular periodical Scientific American took the debate to a general scientific audience. Searle included the Chinese Room Argument in his contribution, “Is the Brain’s Mind a Computer Program?”, and Searle’s piece was followed by a responding article, “Could a Machine Think?”, written by philosophers Paul and Patricia Churchland. Soon thereafter Searle had a published exchange about the Chinese Room with another leading philosopher, Jerry Fodor (in Rosenthal (ed.) 1991).

The heart of the argument is Searle imagining himself following a symbol-processing program written in English (which is what Turing called “a paper machine”). The English speaker (Searle) sitting in the room follows English instructions for manipulating Chinese symbols, whereas a computer “follows” (in some sense) a program written in a computing language. The human produces the appearance of understanding Chinese by following the symbol manipulating instructions, but does not thereby come to understand Chinese. Since a computer just does what the human does – manipulate symbols on the basis of their syntax alone – no computer, merely by following a program, comes to genuinely understand Chinese.

This narrow argument, based closely on the Chinese Room scenario, is specifically directed at a position Searle calls “Strong AI”. Strong AI is the view that suitably programmed computers (or the programs themselves) can understand natural language and actually have other mental capabilities similar to the humans whose behavior they mimic. According to Strong AI, these computers really play chess intelligently, make clever moves, or understand language. By contrast, “weak AI” is the much more modest claim that computers are merely useful in psychology, linguistics, and other areas, in part because they can simulate mental abilities. But weak AI makes no claim that computers actually understand or are intelligent. The Chinese Room argument is not directed at weak AI, nor does it purport to show that no machine can think – Searle says that brains are machines, and brains think. The argument is directed at the view that formal computations on symbols can produce thought.

We might summarize the narrow argument as a reductio ad absurdum against Strong AI as follows. Let L be a natural language, and let us say that a “program for L” is a program for conversing fluently in L. A computing system is any system, human or otherwise, that can run a program.

If Strong AI is true, then there is a program for Chinese, C, such that if any computing system runs C, that system thereby comes to understand Chinese.
I could run C without thereby coming to understand Chinese.
Therefore Strong AI is false.
The first premise elucidates the claim of Strong AI. The second premise is supported by the Chinese Room thought experiment. The conclusion of this narrow argument is that running a program cannot endow the system with language understanding. (There are other ways of understanding the structure of the argument. It may be relevant to understand some of the claims as counterfactual: e.g. “there is a program” in premise 1 as meaning there could be a program, etc. On this construal the argument involves modal logic, the logic of possibility and necessity (see Damper 2006 for the CRA reconstructed as a modal 5 step reductio and Shaffer 2009 in response)).

It is also worth noting that the claim made by Strong AI in the first premise above attributes understanding to “the system”. Exactly what Strong-AI supposes will acquire understanding when the program runs is crucial to the success or failure of the CRA. Schank 1978 has a title that claims their group’s computer, a physical device, understands, but in the body of the paper he claims that the program [“SAM”] is doing the understanding: SAM, Schank says “...understands stories about domains about which it has knowledge” (p. 133). As we will see in the next section (4), these issues about the identity of the understander (the cpu? the program? the system? something else?) quickly came to the fore for critics of the CRA. Searle’s wider argument includes the claim that the thought experiment shows more generally that one cannot get semantics (meaning) from syntax (formal symbol manipulation). That larger claim and related issues are discussed in section 5: The Larger Philosophical Issues.

4. Replies to the Chinese Room Argument
Criticisms of the narrow Chinese Room argument against Strong AI have often followed three main lines, which can be distinguished by how much they concede:

(1) Some critics concede that the man in the room doesn’t understand Chinese, but hold that nevertheless running the program may create comprehension of Chinese by something other than the room operator. These critics object to the inference from the claim that the man in the room does not understand Chinese to the conclusion that no understanding has been created. There might be understanding by a larger, smaller, or different, entity than the man rustling papers in the room. This is the strategy of The Systems Reply and the Virtual Mind Reply. These replies hold that the output of the room might reflect real understanding of Chinese, but the understanding would not be that of the room operator. Thus Searle’s claim that he doesn’t understand Chinese while running the room is conceded, but his claim that there is no understanding of the questions in Chinese, and that computationalism is false, is denied.

(2) Other critics concede Searle’s claim that just running a natural language processing program as described in the CR scenario does not create any understanding, whether by a human or a computer system. But these critics hold that a variation on the computer system could understand. The variant might be a computer embedded in a robotic body, having interaction with the physical world via sensors and motors (“The Robot Reply”), or it might be a system that simulated the detailed operation of an entire human brain, neuron by neuron (“the Brain Simulator Reply”).

(3) Finally, some critics do not concede even the narrow point against AI. These critics hold that the man in the original Chinese Room scenario might understand Chinese, despite Searle’s denials, or that the scenario is impossible. For example, critics have argued that our intuitions in such cases are unreliable. Other critics have held that it all depends on what one means by “understand” – points discussed in the section on The Intuition Reply. Others (e.g. Sprevak 2007) object to the assumption that any system (e.g. Searle in the room) can run any computer program. And finally some have argued that if it is not reasonable to attribute understanding on the basis of the behavior exhibited by the Chinese Room, then it would not be reasonable to attribute understanding to humans on the basis of similar behavioral evidence (Searle calls this last the “Other Minds Reply”). This objection to the CRA is that we should be willing to attribute understanding in the Chinese Room on the basis of the overt behavior, just as we do with other humans (and some animals), and as we would do with extra-terrestrial aliens (or burning bushes or angels) that spoke our language. This position is close to Turing’s own, when he proposed his behavioral test for machine intelligence.

In addition to these responses specifically to the Chinese Room scenario and the narrow argument to be discussed in this section, some critics also independently argue against Searle’s larger claim, and hold that one can get semantics (that is, meaning) from syntactic symbol manipulation, including the sort that takes place inside a digital computer, a question discussed in the section below on Syntax and Semantics.

4.1 The Systems Reply
In the original BBS article, Searle identified and discussed several responses to the argument that he had come across in giving the argument in talks at various places. As a result, these early responses have received the most attention in subsequent discussion. What Searle 1980 calls “perhaps the most common reply” is the Systems Reply.

The Systems Reply (which Searle says was originally associated with Yale, the home of Schank’s AI work) concedes that the man in the room does not understand Chinese. But, the reply continues, the man is but a part, a central processing unit (CPU), in a larger system. The larger system includes the huge database, the memory (scratchpads) containing intermediate states, and the instructions – the complete system that is required for answering the Chinese questions. So the Systems Reply is that while the man running the program does not understand Chinese, the system as a whole does.

Ned Block was one of the first to press the Systems Reply, along with many others including Jack Copeland, Daniel Dennett, Douglas Hofstadter, Jerry Fodor, John Haugeland, Ray Kurzweil and Georges Rey. Rey (1986) says the person in the room is just the CPU of the system. Kurzweil (2002) says that the human being is just an implementer and of no significance (presumably meaning that the properties of the implementer are not necessarily those of the system). Kurzweil hews to the spirit of the Turing Test and holds that if the system displays the apparent capacity to understand Chinese “it would have to, indeed, understand Chinese” – Searle is contradicting himself in saying in effect, “the machine speaks Chinese but doesn’t understand Chinese”.

Margaret Boden (1988) raises levels considerations. “Computational psychology does not credit the brain with seeing bean-sprouts or understanding English: intentional states such as these are properties of people, not of brains” (244) – a person is an agent that is not identical with a brain or a body. “In short, Searle’s description of the robot’s pseudo-brain (that is, of Searle-in-the-robot) as understanding English involves a category-mistake comparable to treating the brain as the bearer, as opposed to the causal basis, of intelligence”. Boden (1988) points out that the room operator is a conscious agent, while the CPU in a computer is not – the Chinese Room scenario asks us to take the perspective of the implementer, and not surprisingly fails to see the larger picture.

Searle’s response to the Systems Reply is simple: in principle, he could internalize the entire system, memorizing all the instructions and the database, and doing all the calculations in his head. He could then leave the room and wander outdoors, perhaps even conversing in Chinese. But he still would have no way to attach “any meaning to the formal symbols”. The man would now be the entire system, yet he still would not understand Chinese. For example, he would not know the meaning of the Chinese word for hamburger. He still cannot get semantics from syntax.

In some ways Searle’s response here anticipates later extended mind views (e.g. Clark and Chalmers 1998): if Otto, who suffers loss of memory, can regain those recall abilities by externalizing some of the information to his notebooks, then Searle arguably can do the reverse: by internalizing the instructions and notebooks he should acquire any abilities had by the extended system. And so Searle in effect concludes that since he doesn’t acquire understanding of Chinese by internalizing the external components of the entire system (e.g. he still doesn’t know what the Chinese word for hamburger means), understanding was never there in the partially externalized system of the original Chinese Room.

In his 2002 paper “The Chinese Room from a Logical Point of View”, Jack Copeland considers Searle’s response to the Systems Reply and argues that a homunculus inside Searle’s head might understand even though the room operator himself does not, just as modules in our brains solve tensor equations that enable us to catch cricket balls. Copeland then turns to consider the Chinese Gym, and again appears to endorse the Systems Reply: “…the individual players [do not] understand Chinese. But there is no entailment from this to the claim that the simulation as a whole does not come to understand Chinese. The fallacy involved in moving from part to whole is even more glaring here than in the original version of the Chinese Room Argument”. Copeland denies that connectionism implies that a room of people can simulate the brain.

Shaffer 2009 examines modal aspects of the logic of the CRA and argues that familiar versions of the System Reply are question-begging. But, Shaffer claims, a modalized version of the System Reply succeeds because there are possible worlds in which understanding is an emergent property of complex syntax manipulation. Nute 2011 is a reply to Shaffer.

Stevan Harnad has defended Searle’s argument against Systems Reply critics in two papers. In his 1989 paper, Harnad writes “Searle formulates the problem as follows: Is the mind a computer program? Or, more specifically, if a computer program simulates or imitates activities of ours that seem to require understanding (such as communicating in language), can the program itself be said to understand in so doing?” (Note the specific claim: the issue is taken to be whether the program itself understands.) Harnad concludes: “On the face of it, [the CR argument] looks valid. It certainly works against the most common rejoinder, the ‘Systems Reply’….” Harnad appears to follow Searle in linking understanding and states of consciousness: Harnad 2012 (Other Internet Resources) argues that Searle shows that the core problem of conscious “feeling” requires sensory connections to the real world. (See sections below “The Robot Reply” and “Intentionality” for discussion.)

Finally some have argued that even if the room operator memorizes the rules and does all the operations inside his head, the room operator does not become the system. Cole (1984) and Block (1998) both argue that the result would not be identity of Searle with the system but much more like a case of multiple personality – distinct persons in a single head. The Chinese responding system would not be Searle, but a sub-part of him. In the CR case, one person (Searle) is an English monoglot and the other is a Chinese monoglot. The English-speaking person’s total unawareness of the meaning of the Chinese responses does not show that they are not understood. This line, of distinct persons, leads to the Virtual Mind Reply.

4.1.1 The Virtual Mind Reply
The Virtual Mind reply concedes, as does the System Reply, that the operator of the Chinese Room does not understand Chinese merely by running the paper machine. However the Virtual Mind reply holds that what is important is whether understanding is created, not whether the Room operator is the agent that understands. Unlike the Systems Reply, the Virtual Mind reply (VMR) holds that a running system may create new, virtual, entities that are distinct from both the system as a whole, as well as from the sub-systems such as the CPU or operator. In particular, a running system might create a distinct agent that understands Chinese. This virtual agent would be distinct from both the room operator and the entire system. The psychological traits, including linguistic abilities, of any mind created by artificial intelligence will depend entirely upon the program and the Chinese database, and will not be identical with the psychological traits and abilities of a CPU or the operator of a paper machine, such as Searle in the Chinese Room scenario. According to the VMR the mistake in the Chinese Room Argument is to make the claim of strong AI to be “the computer understands Chinese” or “the System understands Chinese”. The claim at issue for AI should simply be whether “the running computer creates understanding of Chinese”.

For example, John Haugeland writes (2002) that Searle’s response to the Systems Reply is flawed: “…what he now asks is what it would be like if he, in his own mind, were consciously to implement the underlying formal structures and operations that the theory says are sufficient to implement another mind”. According to Haugeland, his failure to understand Chinese is irrelevant: he is just the implementer. The implemented mind would understand – there is a level-of-description fallacy.

A familiar model of virtual agents are characters in computer or video games, as well as generative AIs such as ChatGPT. Characters in video games have various abilities and personalities, and the characters are not identical with the system hardware or program that creates them. A single running system might control two distinct virtual agents, or physical robots, simultaneously, one of which converses only in Chinese and one of which can converse only in English, and which otherwise manifest very different personalities, memories, and cognitive abilities. For the Systems Reply, the system understands, whereas for the VM reply, the running system creates a new, virtual, mind that is not identical with the system or the physical implementation. Thus the VM reply asks us to distinguish between minds and their realizing systems.

Minsky (1980) and Sloman and Croucher (1980) suggested a Virtual Mind reply when the Chinese Room argument first appeared. In his widely-read 1989 paper “Computation and Consciousness”, Tim Maudlin considers minimal physical systems that might implement a computational system running a program. His discussion revolves around his imaginary Olympia machine, a system of buckets that transfer water, implementing a Turing machine. Maudlin’s main target is the computationalists’ claim that such a machine could have phenomenal consciousness. However in the course of his discussion, Maudlin considers the Chinese Room argument. Maudlin (citing Minsky, and Sloman and Croucher) points out a Virtual Mind reply that the agent that understands could be distinct from the physical system (414). Thus “Searle has done nothing to discount the possibility of simultaneously existing disjoint mentalities” (414–5).

Perlis (1992), Chalmers (1996) and Block (2002) have apparently endorsed versions of a Virtual Mind reply as well, as has Richard Hanley in The Metaphysics of Star Trek (1997). Penrose (2002) is a critic of this strategy, and Stevan Harnad scornfully dismisses such heroic resorts to metaphysics. Harnad defended Searle’s position in a “Virtual Symposium on Virtual Minds” (1992) against Patrick Hayes and Don Perlis. Perlis pressed a virtual minds argument derived, he says, from Maudlin. Chalmers (1996) notes that the room operator is just a causal facilitator, a “demon”, so that his states of consciousness are irrelevant to the properties of the system as a whole. Like Maudlin, Chalmers raises issues of personal identity – we might regard the Chinese Room as “two mental systems realized within the same physical space. The organization that gives rise to the Chinese experiences is quite distinct from the organization that gives rise to the demon’s [= room operator’s] experiences”(326).

Cole (1991, 1994) develops the reply and argues as follows: Searle’s argument requires that the agent of understanding be the computer itself or, in the Chinese Room parallel, the person in the room. However Searle’s failure to understand Chinese in the room does not show that there is no understanding being created. One of the key considerations is that in Searle’s discussion the actual conversation with the Chinese Room is always seriously under specified. Searle was considering Schank’s programs, which can only respond to a few questions about what happened in a restaurant, all in third person. But Searle wishes his conclusions to apply to any AI-produced responses, including those that would pass the toughest unrestricted Turing Test, i.e. they would be just the sort of conversations real people have with each other. If we flesh out the conversation in the original CR scenario to include questions in Chinese such as “How tall are you?”, “Where do you live?”, “What did you have for breakfast?”, “What is your attitude toward Mao?”, and so forth, it immediately becomes clear that the answers in Chinese are not Searle’s answers. Searle is not the author of the answers, and his beliefs and desires, memories and personality traits (apart from his industriousness!) are not reflected in the answers and in general Searle’s traits are causally inert in producing the answers to the Chinese questions. This suggests the following conditional is true: if there is understanding of Chinese created by running the program, the mind understanding the Chinese would not be the computer, whether the computer is human or electronic. The person understanding the Chinese would be a distinct person from the room operator, with beliefs and desires bestowed by the program and its database. Hence Searle’s failure to understand Chinese while operating the room does not show that understanding is not being created.

Cole (1991) offers an additional argument that the mind doing the understanding is neither the mind of the room operator nor the system consisting of the operator and the program: running a suitably structured computer program might produce answers submitted in Chinese and also answers to questions submitted in Korean. Yet the Chinese answers might apparently display completely different knowledge and memories, beliefs and desires than the answers to the Korean questions – along with a denial that the Chinese answerer knows any Korean, and vice versa. Thus the behavioral evidence would be that there were two non-identical minds (one understanding Chinese only, and one understanding Korean only). Since these might have mutually exclusive properties, they cannot be identical, and ipso facto, cannot be identical with the mind of the implementer in the room. Alternatively, we can flesh out Searle’s scenario by supposing those outside the room not only submit questions in Chinese, but also in English. The result would appear to be that there are two individuals in the Room – Searle answering questions about himself and what he believes to be the case, and a Chinese speaker with a different personal history and knowledge of the world. Analogously, a video game might include a (virtual) character with one set of cognitive abilities (smart, understands Chinese) as well as another virtual character with an incompatible set (stupid, English monoglot). These inconsistent cognitive traits cannot be traits of the XBOX system that realizes them. Cole argues that the implication is that minds and persons generally are more abstract than the physical systems that realize them (see Mind and Body in the Larger Philosophical Issues section).

In short, the Virtual Mind argument is that since the evidence that Searle provides that there is no understanding of Chinese was that he wouldn’t understand Chinese in the room, the Chinese Room Argument cannot refute a differently formulated equally strong AI claim, asserting the possibility of using a programmed digital computer to create a distinct mind that understands a natural language. Maudlin (1989) says that Searle has not adequately responded to this criticism.

Others however have replied to the VMR, including Stevan Harnad and mathematical physicist Roger Penrose. Penrose is generally sympathetic to the points Searle raises with the Chinese Room argument, and has argued against the Virtual Mind reply. Penrose does not believe that computational processes can account for consciousness, both on Chinese Room grounds, as well as because of limitations on formal systems revealed by Kurt Gödel’s incompleteness proof. (Penrose has two books on mind and consciousness; Chalmers and others have responded to Penrose’s appeals to Gödel.) In his 2002 article “Consciousness, Computation, and the Chinese Room” that specifically addresses the Chinese Room argument, Penrose argues that the Chinese Gym variation – with a room expanded to the size of India, with Indians doing the processing – shows it is very implausible to hold there is “some kind of disembodied ‘understanding’ associated with the person’s carrying out of that algorithm, and whose presence does not impinge in any way upon his own consciousness” (230–1). Penrose concludes the Chinese Room argument refutes Strong AI. Christian Kaernbach (2005) reports that he subjected the virtual mind theory to an empirical test, with negative results.

4.2 The Robot Reply
The Robot Reply concedes Searle is right about the Chinese Room scenario: it shows that a computer trapped in a computer room cannot understand language, or know what words mean. The Robot reply is responsive to the problem of knowing the meaning of the Chinese word for hamburger – Searle’s example of something the room operator would not know. It seems reasonable to hold that most of us know what a hamburger is because we have seen one, and perhaps even made one, or tasted one, or at least heard people talk about hamburgers and understood what they are by relating them to things we do know by seeing, making, and tasting. Given this is how one might come to know what hamburgers are, the Robot Reply suggests that we put a digital computer in a robot body, with sensors, such as video cameras and microphones, and add effectors, such as wheels to move around with, and arms with which to manipulate things in the world. Such a robot – a computer with a body – might do what a child does, learn by seeing and doing. The Robot Reply holds that such a digital computer in a robot body, freed from the room, could attach meanings to symbols and actually understand natural language. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec and Georges Rey are among those who have endorsed versions of this reply at one time or another. The Robot Reply in effect appeals to “wide content” or “externalist semantics”. This can agree with Searle that syntax and internal connections in isolation from the world are insufficient for semantics, while holding that suitable causal connections with the world can provide content to the internal symbols.

About the time Searle was pressing the CRA, many in philosophy of language and mind were recognizing the importance of causal connections to the world as the source of meaning or reference for words and concepts. Hilary Putnam 1981 argued that a Brain in a Vat, isolated from the world but with neurons connected to a computer that generated a virtual world, might speak or think in a language that sounded like English, but it would not be English – hence a brain in a vat could not wonder if it was a brain in a vat (because of its sensory isolation, its words “brain” and “vat” do not refer to brains or vats). The view that meaning was determined by connections with the world became widespread. Searle however resisted this turn outward and continued to think of meaning as subjective and connected with consciousness.

A related view that minds are best understood as embodied or embedded in the world has gained many supporters since the 1990s, contra Cartesian solipsistic intuitions. Organisms rely on environmental features for the success of their behavior. So whether one takes a mind to be a symbol processing system, with the symbols getting their content from sensory connections with the world, or a non-symbolic system that succeeds by being embedded in a particular environment, the importance of things outside the head have come to the fore. Hence many are sympathetic to some form of the Robot Reply: a computational system might understand, provided it is acting in the world. For example, Carter 2007 in a textbook on philosophy and AI concludes “The lesson to draw from the Chinese Room thought experiment is that embodied experience is necessary for the development of semantics.”

However Searle does not think that the Robot Reply to the Chinese Room argument is any stronger than the Systems Reply. All the sensors can do is provide additional input to the computer – and it will be just syntactic input. We can see this by making a parallel change to the Chinese Room scenario. Suppose the man in the Chinese Room receives, in addition to the Chinese characters slipped under the door, a stream of binary digits that appear, say, on a ticker tape in a corner of the room. The instruction books are augmented to use the numerals from the tape as input, along with the Chinese characters. Unbeknownst to the man in the room, the symbols on the tape are the digitized output of a video camera (and possibly other sensors). Searle argues that additional syntactic inputs will do nothing to allow the man to associate meanings with the Chinese characters. It is just more work for the man in the room.

Jerry Fodor, Hilary Putnam, and David Lewis, were principal architects of the computational theory of mind that Searle’s wider argument attacks. In his original 1980 reply to Searle, Fodor allows Searle is certainly right that “instantiating the same program as the brain does is not, in and of itself, sufficient for having those propositional attitudes, e.g. beliefs, characteristic of the organism that has the brain.” But Fodor holds that Searle is wrong about the robot reply. A computer might have beliefs about, and knowledge of, the world if it has the right causal connections to the world – but those are not ones mediated by a man sitting in the head of the robot. We don’t know what the right causal connections are. Searle commits the fallacy of inferring from “the little man is not the right causal connection” to conclude that no causal linkage would succeed. There is considerable empirical evidence that mental processes involve “manipulation of symbols”; Searle gives us no alternative explanation (this is sometimes called Fodor’s “Only Game in Town” argument for computational approaches). In the 1980s and 1990s Fodor wrote extensively on what the connections must be between a brain state and the world for the state to have intentional (representational) properties, while coming to emphasize that computationalism has limits because the computations are intrinsically local and so cannot account for abductive reasoning, that is inference to the best explanation.

In a later piece, “Yin and Yang in the Chinese Room” (in Rosenthal 1991 pp.524–525), Fodor substantially revises his 1980 view. He distances himself from his earlier version of the robot reply, and holds instead that “instantiation” should be defined in such a way that the symbol must be the proximate cause of the effect – no intervening guys in a room. So Searle in the room is not an instantiation of a Turing Machine, and “Searle’s setup does not instantiate the machine that the brain instantiates.” He concludes: “…Searle’s setup is irrelevant to the claim that strong equivalence to a Chinese speaker’s brain is ipso facto sufficient for speaking Chinese.” Searle says of Fodor’s move, “Of all the zillions of criticisms of the Chinese Room argument, Fodor’s is perhaps the most desperate. He claims that precisely because the man in the Chinese room sets out to implement the steps in the computer program, he is not implementing the steps in the computer program. He offers no argument for this extraordinary claim.” (in Rosenthal 1991, p. 525)

In a 1986 paper, Georges Rey advocated a combination of the system and robot reply, after noting that the original Turing Test is insufficient as a test of intelligence and understanding, and that the isolated system Searle describes in the room is certainly not functionally equivalent to a real Chinese speaker sensing and acting in the world. In a 2002 second look, “Searle’s Misunderstandings of Functionalism and Strong AI”, Rey again defends functionalism against Searle, and in the particular form Rey calls the “computational-representational theory of thought – CRTT”. CRTT is not committed to attributing thought to just any system that passes the Turing Test (like the Chinese Room). Nor is it committed to a conversation manual model of understanding natural language. Rather, CRTT is concerned with intentionality, natural and artificial (the representations in the system are semantically evaluable – they are true or false, hence have aboutness). Searle saddles functionalism with the “blackbox” character of behaviorism, but functionalism cares how things are done. Rey sketches “a modest mind” – a CRTT system that has perception, can make deductive and inductive inferences, makes decisions on basis of goals and representations of how the world is, and can process natural language by converting to and from its native representations. To explain the behavior of such a system we would need to use the same attributions needed to explain the behavior of a normal Chinese speaker.

If we flesh out the Chinese conversation in the context of the Robot Reply, just as with the Virtual Mind Reply, we may again see evidence that the entity that understands is not the operator inside the room. Suppose we ask the robot system using the Chinese translation of “what do you see?”, we might get the answer “My old friend Shakey”, or “I see you!”. Whereas if we phone Searle in the room and ask the same questions in English we might get “These same four walls” or “these damn endless instruction books and notebooks.” Again this is evidence that we have distinct responders here, an English speaker and a Chinese speaker, who see and do quite different things. If the giant robot goes on a rampage and smashes much of Tokyo, and all the while oblivious Searle is just following the program in his notebooks in the room, Searle is not guilty of homicide and mayhem, because he is not the agent committing the acts.

Tim Crane discusses the Chinese Room argument in his 1991 book, The Mechanical Mind. He cites the Churchlands’ 1990 luminous room analogy, but then goes on to argue that in the course of operating the room, Searle would learn the meaning of the Chinese: “…if Searle had not just memorized the rules and the data, but also started acting in the world of Chinese people, then it is plausible that he would before too long come to realize what these symbols mean.”(127). (Rapaport 2006 presses an analogy between Helen Keller and the Chinese Room.) Crane appears to end with a version of the Robot Reply: “Searle’s argument itself begs the question by (in effect) just denying the central thesis of AI – that thinking is formal symbol manipulation. But Searle’s assumption, none the less, seems to me correct … the proper response to Searle’s argument is: sure, Searle-in-the-room, or the room alone, cannot understand Chinese. But if you let the outside world have some impact on the room, meaning or ‘semantics’ might begin to get a foothold. But of course, this concedes that thinking cannot be simply symbol manipulation.” (129) The idea that learning grounds understanding has led to work in developmental robotics (a.k.a. epigenetic robotics). This AI research area seeks to replicate key human learning abilities, such as robots that are shown an object from several angles while being told in natural language the name of the object.

Margaret Boden 1988 also argues that Searle mistakenly supposes programs are pure syntax. But programs bring about the activity of certain machines: “The inherent procedural consequences of any computer program give it a toehold in semantics, where the semantics in question is not denotational, but causal.” (250) Thus a robot might have causal powers that enable it to refer to a hamburger.

Stevan Harnad also finds our sensory and motor capabilities to be important: “Who is to say that the Turing Test, whether conducted in Chinese or in any other language, could be successfully passed without operations that draw on our sensory, motor, and other higher cognitive capacities as well? Where does the capacity to comprehend Chinese begin and the rest of our mental competence leave off?” Harnad believes that symbolic functions must be grounded in “robotic” functions that connect a system with the world. And he thinks this counts against symbolic accounts of mentality, such as Jerry Fodor’s, and, one suspects, the approach of Roger Schank that was Searle’s original target. Harnad 2012 (Other Internet Resources) argues that the CRA shows that even with a robot with symbols grounded in the external world, there is still something missing: feeling, such as the feeling of understanding.

However Ziemke 2016 argues a robotic embodiment with layered systems of bodily regulation may ground emotion and meaning, and Seligman 2019 argues that “perceptually grounded” approaches to natural language processing (NLP) have the “potential to display intentionality, and thus after all to foster a truly meaningful semantics that, in the view of Searle and other skeptics, is intrinsically beyond computers’ capacity.”

4.3 The Brain Simulator Reply
Consider a computer that operates in quite a different manner than an AI program with scripts and operations on sentence-like strings of symbols. The Brain Simulator reply asks us to suppose instead the program parallels the actual sequence of nerve firings that occur in the brain of a native Chinese language speaker when that person understands Chinese – every nerve, every firing. Since the computer then works the very same way as the brain of a native Chinese speaker, processing information in just the same way, it will understand Chinese. Paul and Patricia Churchland have set out a reply along these lines, discussed below.

In response to this, Searle argues that it makes no difference. He suggests a variation on the brain simulator scenario: suppose that in the room the man has a huge set of valves and water pipes, in the same arrangement as the neurons in a native Chinese speaker’s brain. The program now tells the man which valves to open in response to input. Searle claims that it is obvious that there would be no understanding of Chinese. (Note however that the basis for this claim is no longer simply that Searle himself wouldn’t understand Chinese – it seems clear that now he is just facilitating the causal operation of the system and so we rely on our Leibnizian intuition that water-works don’t understand (see also Maudlin 1989).) Searle concludes that a simulation of brain activity is not the real thing.

However, following Pylyshyn 1980, Cole and Foelber 1984, and Chalmers 1996, we might wonder about gradually transitioning cyborg systems. Pylyshyn writes:

If more and more of the cells in your brain were to be replaced by integrated circuit chips, programmed in such a way as to keep the input-output function each unit identical to that of the unit being replaced, you would in all likelihood just keep right on speaking exactly as you are doing now except that you would eventually stop meaning anything by it. What we outside observers might take to be words would become for you just certain noises that circuits caused you to make.
These cyborgization thought experiments can be linked to the Chinese Room. Suppose Otto has a neural disease that causes one of the neurons in his brain to fail, but surgeons install a tiny remotely controlled artificial neuron, a synron, alongside his disabled neuron. The control of Otto’s artificial neuron is by John Searle in the Chinese Room, unbeknownst to both Searle and Otto. Tiny wires connect the artificial neuron to the synapses on the cell-body of his disabled neuron. When his artificial neuron is stimulated by neurons that synapse on his disabled neuron, a light goes on in the Chinese Room. Searle then manipulates some valves and switches in accord with a program. That, via the radio link, causes Otto’s artificial neuron to release neuro-transmitters from its tiny artificial vesicles. If Searle’s programmed activity causes Otto’s artificial neuron to behave just as his disabled natural neuron once did, the behavior of the rest of his nervous system will be unchanged. Alas, Otto’s disease progresses; more neurons are replaced by synrons controlled by Searle. Ex hypothesi the rest of the world will not notice the difference; will Otto? If so, when? And why?

Under the rubric “The Combination Reply”, Searle also considers a system with the features of all three of the preceding: a robot with a digital brain simulating computer in its aluminum cranium, such that the system as a whole behaves indistinguishably from a human. Since the normal input to the brain is from sense organs, it is natural to suppose that most advocates of the Brain Simulator Reply have in mind such a combination of brain simulation, Robot, and Systems or Virtual Mind Reply. Some (e.g. Rey 1986) argue it is reasonable to attribute intentionality to such a system as a whole. Searle agrees that it would indeed be reasonable to attribute understanding to such an android system – but only as long as you don’t know how it works. As soon as you know the truth – it is a computer, uncomprehendingly manipulating symbols on the basis of syntax, not meaning – you would cease to attribute intentionality to it.

(One assumes this would be true even if it were one’s spouse, with whom one had built a life-long relationship, that was revealed to hide a silicon secret. Science fiction stories, including episodes of Rod Serling’s television series The Twilight Zone, have been based on such possibilities (the face of the beloved peels away to reveal the awful android truth); however, Steven Pinker (1997) mentions one episode in which the android’s secret was known from the start, but the protagonist still developed a romantic relationship with the android.)

On its tenth anniversary the Chinese Room argument was featured in the general science periodical Scientific American. Leading the opposition to Searle’s lead article in that issue were philosophers Paul and Patricia Churchland. The Churchlands agree with Searle that the Chinese Room does not understand Chinese, but hold that the argument itself exploits our ignorance of cognitive and semantic phenomena. They raise a parallel case of “The Luminous Room” where someone waves a magnet and argues that the absence of resulting visible light shows that Maxwell’s electromagnetic theory is false. The Churchlands advocate a view of the brain as a connectionist system, a vector transformer, not a system manipulating symbols according to syntax-sensitive rules. The system in the Chinese Room uses the wrong computational strategies. Thus they agree with Searle against traditional AI, but they presumably would endorse what Searle calls “the Brain Simulator Reply”, arguing that, as with the Luminous Room, our intuitions fail us when considering such a complex system, and it is a fallacy to move from part to whole: “… no neuron in my brain understands English, although my whole brain does.”

In his 1991 book, Microcognition. Andy Clark holds that Searle is right that a computer running Schank’s program does not know anything about restaurants, “at least if by ‘know’ we mean anything like ‘understand’”. But Searle thinks that this would apply to any computational model, while Clark, like the Churchlands, holds that Searle is wrong about connectionist models. Clark’s interest is thus in the brain-simulator reply. The brain thinks in virtue of its physical properties. What physical properties of the brain are important? Clark answers that what is important about brains are “variable and flexible substructures” which syntactic, rule-based systems like Schank’s (‘GOFAI’, or Good Old-Fashioned AI) lack. But that doesn’t mean computationalism or functionalism is false. It depends on what level you take the functional units to be. Clark defends “microfunctionalism” – one should look to a fine-grained functional description, e.g. neural net level. Clark cites William Lycan approvingly contra Block’s absent qualia objection – yes, there can be absent qualia, if the functional units are made large. But that does not constitute a refutation of functionalism generally. So Clark’s views are not unlike the Churchlands’, conceding that Searle is right about Schank and symbolic-level processing systems, but holding that he is mistaken about connectionist systems.

Similarly Ray Kurzweil (2002) argues that Searle’s argument could be turned around to show that human brains cannot understand – the brain succeeds by manipulating neurotransmitter concentrations and other mechanisms that are in themselves meaningless. In criticism of Searle’s response to the Brain Simulator Reply, Kurzweil says: “So if we scale up Searle’s Chinese Room to be the rather massive ‘room’ it needs to be, who’s to say that the entire system of a hundred trillion people simulating a Chinese Brain that knows Chinese isn’t conscious? Certainly, it would be correct to say that such a system knows Chinese. And we can’t say that it is not conscious anymore than we can say that about any other process. We can’t know the subjective experience of another entity….”

4.4 The Other Minds Reply
Related to the preceding is The Other Minds Reply: “How do you know that other people understand Chinese or anything else? Only by their behavior. Now the computer can pass the behavioral tests as well as they can (in principle), so if you are going to attribute cognition to other people you must in principle also attribute it to computers.”

Searle’s (1980) reply to this is very short:

The problem in this discussion is not about how I know that other people have cognitive states, but rather what it is that I am attributing to them when I attribute cognitive states to them. The thrust of the argument is that it couldn’t be just computational processes and their output because the computational processes and their output can exist without the cognitive state. It is no answer to this argument to feign anesthesia. In ‘cognitive sciences’ one presupposes the reality and knowability of the mental in the same way that in physical sciences one has to presuppose the reality and knowability of physical objects.
Critics of Searle’s claim here argue that if the evidence we have that humans understand is the same as the evidence we might have that a visiting extra-terrestrial alien understands, which is the same as the evidence that a robot understands, the presuppositions we may make in the case of our own species are not relevant, for presuppositions are sometimes false. For similar reasons, Turing, in proposing the Turing Test, is specifically worried about our presuppositions and chauvinism. If the reasons for the presuppositions regarding humans are pragmatic, in that they enable us to predict the behavior of humans and to interact effectively with them, perhaps the presupposition could apply equally to computers (similar considerations are pressed by Dennett, in his discussions of what he calls the Intentional Stance).

Searle raises the question of just what we are attributing in attributing understanding to other minds, saying that it is more than complex behavioral dispositions. For Searle, understanding appears to involve states of consciousness, as is seen in his 2010 summary of the CRA conclusions. Terry Horgan (2013) endorses this claim: “the real moral of Searle’s Chinese room thought experiment is that genuine original intentionality requires the presence of internal states with intrinsic phenomenal character that is inherently intentional…” But this tying of understanding to phenomenal consciousness raises a host of issues.

We attribute limited understanding of language to toddlers, dogs, and other animals, but it is not clear that we are ipso facto attributing unseen states of subjective consciousness – what do we know of the hidden states of exotic creatures? Ludwig Wittgenstein (the Private Language Argument) and his followers pressed similar points. Altered qualia possibilities, analogous to the inverted spectrum, arise: suppose I ask “what’s the sum of 5 and 7” and you respond “the sum of 5 and 7 is 12”, but as you heard my question you had the conscious experience of hearing and understanding “what is the sum of 10 and 14”, though you were in the computational states appropriate for producing the correct sum and so said “12”. Are there certain conscious states that are “correct” for certain functional states? Wittgenstein’s considerations appear to be that the subjective state is irrelevant, at best epiphenomenal, if a language user displays appropriate linguistic behavior. Afterall, we are taught language on the basis of our overt responses, not our qualia or states of consciousness. The mathematical savant Daniel Tammet reports that when he generates the decimal expansion of pi to thousands of digits he experiences colors that reveal the next digit, but even here it may be that Tennant’s performance is likely not produced by the colors he experiences, but rather by unconscious neural computation that produces both the correct answer and the color he experiences. The possible importance of subjective states is further considered in the section on Intentionality, below.

Since the CRA there has been philosophical interest in another other-minds problem, namely the possibility of zombies – creatures that look like and behave just as normal humans, including linguistic behavior, yet have no subjective consciousness. A difficulty for claiming that subjective states of consciousness are crucial for understanding meaning will arise in these cases of absent qualia: we can’t tell the difference between zombies and non-zombies, and so on Searle’s account we can’t tell the difference between those that really understand English and those that don’t. And if you and I can’t tell the difference between those who understand language and Zombies who behave like they do but don’t really, than neither can any selection factor in the history of human evolution – for predators, mates, fellow tribe members, zombies and true understanders, with the “right” conscious experience, have been indistinguishable. But then there appears to be a distinction without a difference. In any case, Searle’s short reply to the Other Minds Reply may be too short.

Descartes famously argued that speech was sufficient for attributing minds and consciousness to others, and infamously argued that it was necessary. Turing was in effect endorsing Descartes’ sufficiency condition, at least for intelligence, while substituting written for oral linguistic behavior. Since most of us use dialog as a sufficient condition for attributing understanding, Searle’s argument, which holds that speech is a sufficient condition for attributing understanding to humans but not for anything that doesn’t share our biology, an account would appear to be required of what additionally is being attributed, and what can justify the additional attribution. Further, if being con-specific is key on Searle’s account, a natural question arises as to what circumstances would justify us in attributing understanding (or consciousness) to extra-terrestrial aliens who do not share our biology? Offending ET’s by withholding attributions of understanding until after doing a brain scan or post-mortem may be risky.

Hans Moravec, director of the Robotics laboratory at Carnegie Mellon University, and author of Robot: Mere Machine to Transcendent Mind, argues that Searle’s position merely reflects intuitions from traditional philosophy of mind that are out of step with the new cognitive science. Moravec endorses a version of the Other Minds reply. It makes sense to attribute intentionality to machines for the same reasons it makes sense to attribute them to humans; his “interpretative position” is similar to Dennett’s view. Moravec goes on to note that one of the things we attribute to others is the ability to make attributions of intentionality, and then we make such attributions to ourselves. He holds that such self-representation is at the heart of consciousness. These capacities appear to be implementation independent, and hence possible for aliens and suitably programmed computers.

As we have seen, the reason that Searle thinks we can disregard the behavioral evidence in the case of robots and computers is that we know that their processing is syntactic, and this fact trumps all other considerations. Indeed, Searle believes this is the larger point that the Chinese Room merely illustrates. This larger point is addressed in the Syntax and Semantics section below.

4.5 The Intuition Reply
Many responses to the Chinese Room argument have noted that, as with Leibniz’ Mill, the argument appears to be based on intuition: the intuition that a computer (or the man in the room) cannot think or have understanding. For example, Ned Block (1980) in his original BBS commentary says “Searle’s argument depends for its force on intuitions that certain entities do not think.” But, Block argues, (1) intuitions sometimes can and should be trumped and (2) perhaps we need to bring our concept of understanding in line with a reality in which certain computer robots belong to the same natural kind as humans. Similarly Margaret Boden (1988) points out that we can’t trust our untutored intuitions about how mind depends on matter; developments in science may change our intuitions. Indeed, elimination of bias in our intuitions was precisely what motivated Turing (1950) to propose the Turing Test, a test that was blind to the physical character of the system replying to questions. Some of Searle’s critics in effect argue that he has merely pushed the reliance on intuition back, into the room.

For example, one can hold that despite Searle’s intuition that he would not understand Chinese while in the room, perhaps he is mistaken and does, albeit unconsciously. Hauser (2002) accuses Searle of Cartesian bias in his inference from “it seems to me quite obvious that I understand nothing” to the conclusion that I really understand nothing. (From “I can really clearly imagine myself existing without my body”, Descartes unsoundly inferred “I can exist without my body.”) Normally, if one understands English or Chinese, one knows that one does – but not necessarily. The man in the Chinese Room might lack the normal introspective awareness of understanding – but this, while abnormal, does not support the conclusion that he does not understand.

Critics of the CRA note that our intuitions about intelligence, understanding and meaning may all be unreliable. With regard to meaning, Wakefield 2003, following Block 1998, defends what Wakefield calls “the essentialist objection” to the CRA, namely that a computational account of meaning is not analysis of ordinary concepts and their related intuitions. Rather we are building a scientific theory of meaning that may require revising our intuitions. As a theory, it gets its evidence from its explanatory power, not its accord with pre-theoretic intuitions (however Wakefield himself argues that computational accounts of meaning are afflicted by a pernicious indeterminacy (pp. 308ff)).

Other critics focusing on the role of intuitions in the CRA argue that our intuitions regarding both intelligence and understanding may also be unreliable, and perhaps incompatible even with current science. With regard to understanding, Steven Pinker, in How the Mind Works (1997), holds that “… Searle is merely exploring facts about the English word understand…. People are reluctant to use the word unless certain stereotypical conditions apply…” But, Pinker claims, nothing scientifically speaking is at stake. Pinker objects to Searle’s appeal to the “causal powers of the brain” by noting that the apparent locus of the causal powers is the “patterns of interconnectivity that carry out the right information processing”. Pinker ends his discussion by citing a science fiction story in which Aliens, anatomically quite unlike humans, cannot believe that humans can really think once they discover that our heads are filled with meat. The Aliens’ intuitions are unreliable – presumably ours may be so as well.

Clearly the CRA turns on what is required to understand language. Schank 1978 clarifies his claim about what he thinks his programs can do: “By ‘understand’, we mean SAM [one of his programs] can create a linked causal chain of conceptualizations that represent what took place in each story.” This is a nuanced understanding of “understanding”, whereas the Chinese Room thought experiment does not turn on a technical understanding of “understanding”, but rather intuitions about our ordinary competence when we understand a word like “hamburger”. Indeed by 2015 Schank distances himself from weak senses of “understand”, holding that no computer can “understand when you tell it something”, and that IBM’s WATSON “doesn’t know what it is saying”. Schank’s program may get links right, but arguably does not know what the linked entities are. Whether it does or not depends on what concepts are, see section 5.1. Furthermore it is possible that when it comes to attributing understanding of language we have different standards for different things – more relaxed for dogs and toddlers. Some things understand a language “un poco”. Searle (1980)concedes that there are degrees of understanding, but says that all that matters that there are clear cases of no understanding, and AI programs are an example: “The computer understanding is not just (like my understanding of German) partial or incomplete; it is zero.”

Some defenders of AI are also concerned with how our understanding of understanding bears on the Chinese Room argument. In their paper “A Chinese Room that Understands” AI researchers Simon and Eisenstadt (2002) argue that whereas Searle refutes “logical strong AI”, the thesis that a program that passes the Turing Test will necessarily understand, Searle’s argument does not impugn “Empirical Strong AI” – the thesis that it is possible to program a computer that convincingly satisfies ordinary criteria of understanding. They hold however that it is impossible to settle these questions “without employing a definition of the term ‘understand’ that can provide a test for judging whether the hypothesis is true or false”. They cite W.V.O. Quine’s Word and Object as showing that there is always empirical uncertainty in attributing understanding to humans. The Chinese Room is a Clever Hans trick (Clever Hans was a horse who appeared to clomp out the answers to simple arithmetic questions, but it was discovered that Hans could detect unconscious cues from his trainer). Similarly, the man in the room doesn’t understand Chinese, and could be exposed by watching him closely. (Simon and Eisenstadt do not explain just how this would be done, or how it would affect the argument.) Citing the work of Rudolf Carnap, Simon and Eisenstadt argue that to understand is not just to exhibit certain behavior, but to use “intensions” that determine extensions, and that one can see in actual programs that they do use appropriate intensions. They discuss three actual AI programs, and defend various attributions of mentality to them, including understanding, and conclude that computers understand; they learn “intensions by associating words and other linguistic structure with their denotations, as detected through sensory stimuli”. And since we can see exactly how the machines work, “it is, in fact, easier to establish that a machine exhibits understanding that to establish that a human exhibits understanding….” Thus, they conclude, the evidence for empirical strong AI is overwhelming.

Similarly, Daniel Dennett in his original 1980 response to Searle’s argument called it “an intuition pump”, a term he came up with in discussing the CRA with Douglas Hofstader. Sharvy 1983 echoes the complaint. Dennett’s considered view (2013) is that the CRA is “clearly a fallacious and misleading argument ….” (p. 320). Paul Thagard (2013) proposes that for every thought experiment in philosophy there is an equal and opposite thought experiment. Thagard holds that intuitions are unreliable, and the CRA is an example (and that in fact the CRA has now been refuted by the technology of autonomous robotic cars). Dennett has elaborated on concerns about our intuitions regarding intelligence. Dennett 1987 (“Fast Thinking”) expressed concerns about the slow speed at which the Chinese Room would operate, and he has been joined by several other commentators, including Tim Maudlin, David Chalmers, and Steven Pinker. The operator of the Chinese Room may eventually produce appropriate answers to Chinese questions. But slow thinkers are stupid, not intelligent – and in the wild, they may well end up dead. Dennett argues that “speed … is ‘of the essence’ for intelligence. If you can’t figure out the relevant portions of the changing environment fast enough to fend for yourself, you are not practically intelligent, however complex you are” (326). Thus Dennett relativizes intelligence to processing speed relative to current environment.

Tim Maudlin (1989) disagrees. Maudlin considers the time-scale problem pointed to by other writers, and concludes, contra Dennett, that the extreme slowness of a computational system does not violate any necessary conditions on thinking or consciousness. Furthermore, Searle’s main claim is about understanding, not intelligence or being quick-witted. If we were to encounter extra-terrestrials that could process information a thousand times more quickly than we do, it seems that would show nothing about our own slow-poke ability to understand the languages we speak.

Steven Pinker (1997) also holds that Searle relies on untutored intuitions. Pinker endorses the Churchlands’ (1990) counterexample of an analogous thought experiment of waving a magnet and not generating light, noting that this outcome would not disprove Maxwell’s theory that light consists of electromagnetic waves. Pinker holds that the key issue is speed: “The thought experiment slows down the waves to a range to which we humans no longer see them as light. By trusting our intuitions in the thought experiment, we falsely conclude that rapid waves cannot be light either. Similarly, Searle has slowed down the mental computations to a range in which we humans no longer think of it as understanding (since understanding is ordinarily much faster)” (94–95). Howard Gardiner, a supporter of Searle’s conclusions regarding the room, makes a similar point about understanding. Gardiner addresses the Chinese Room argument in his book The Mind’s New Science (1985, 171–177). Gardiner considers all the standard replies to the Chinese Room argument and concludes that Searle is correct about the room: “…the word understand has been unduly stretched in the case of the Chinese room ….” (175).

Thus several in this group of critics argue that speed affects our willingness to attribute intelligence and understanding to a slow system, such as that in the Chinese Room. The result may simply be that our intuitions regarding the Chinese Room are unreliable, and thus the man in the room, in implementing the program, may understand Chinese despite intuitions to the contrary (Maudlin and Pinker). Or it may be that the slowness marks a crucial difference between the simulation in the room and what a fast computer does, such that the man is not intelligent while the computer system is (Dennett).

4.6 Advances in Artificial Intelligence
Even as late as 2001, Robert Damper [2001, Other Internet Resources) dismissed the CRA as useless, and possibly harmful, because “What Searle and others seem ready blithely to assume – the existence of a Chinese ‘understanding’ program able to pass the Turing test … – is so far beyond the current capabilities of AI and computer technology as to amount to science fiction. What could we possibly learn from such a fanciful conception? There is no realistic way of resolving any paradoxes which arise, save appeals to common sense, and we know from the example of quantum mechanics how fallible this is.” And in 2015 Steven Pinker remarked “Human-level AI is still the standard 15-to-25 years away, just as it always has been….”

Searle’s argument was developed in the late 1970s, little more than 20 years after transistorized computers were introduced, as well as the first AI conference (1956). In the many decades since then, there have been enormous advances in areas relevant to the CRA and many of the replies: computing speed and power, robotics, artificial intelligence, neural networks, and to the point, natural language processing.

In late 2022, AI systems based on large language models (LLMs) received wide attention, from academics to their essay-writing students, as well as many other professions in which language proficiency was important. Whereas Shank’s program and database were hand-built, so that, (once debugged!) their highly limited output of a sentence or two about restaurants could rarely, if ever, surprise the programmers, LLM systems crawl the world wide web and can generate paragraph after paragraph that may be all news to their coders.

Does this make any difference to the CRA and its replies? Sabine Hossenfelder (2023) argues that these chatbots understand some of what they say, namely they understand in the same sense that humans understand quantum mechanics. We can understand the equations well enough to make predictions, but we do not have a deep understanding of why the equations are what they are. Jensen Huang (2024 [Other Internet Resources]), CEO of AI chip maker Nvidia, see no such limits: “Generative AI is the most impactful invention of our time, and as with electricity and the internet, it impacts everyone and every industry. … LLMs, learned to understand human language, prior knowledge, and even some common sense from massive amounts of text.”

There is no doubt that AI agents now have a way with words, and can present useful information. If those models can diagnose disease better than diagnosticians, write essays better than essayists, argue law better than lawyers, and answer questions better than teachers, then it appears to be sad sour grapes to say “but they don’t understand anything”. So has AI progress refuted Searle? Asked in 2024 “Do you understand English words”, ChatGPT itself responds “Yes, I understand English words and can process and respond to them. If you have any questions or need help with something specific, just let me know!” It is loquacious if asked what a hamburger is, Searle’s example of something a natural language program cannot understand. It readily explains the difference between a hamburger and the word “hamburger”. But when pressed, and asked HOW it understands English, ChatGPT responds “I understand English semantics through a combination of patterns and context.” When asked directly, “John Searle argues you cannot get semantics from syntax – is he correct?” the response is “Searle’s argument highlights a crucial limitation: syntax alone is insufficient for fully capturing semantics. While syntactic rules provide a framework for constructing sentences, understanding meaning involves additional layers of context, interpretation, and often real-world knowledge.” And to “Doesn’t Searle’s argument apply to ChatGPT?” we get “Yes, John Searle’s argument does apply to systems like ChatGPT. … ChatGPT uses patterns and rules from the vast amounts of text it has been trained on to generate responses. It operates based on statistical correlations between words and phrases, mimicking syntactic structures. … ChatGPT doesn’t truly understand meaning in the human sense. It generates text that appears to be meaningful based on the patterns it has learned, but it lacks genuine comprehension of the content or context beyond what it has been trained to predict.”

So, paradoxically, the system appears to understand that it doesn’t understand. However it fails to notice that these claims are inconsistent with its output made just minutes before. So we are not there yet. A system that answers questions with carefully argued denials that it can understand anything is not going to pass the Turing Test. However this may get fixed in the future, such that LLM systems will avoid what has been called “locutionary suicide” (e.g. “Here’s the answer to your question, but I don’t understand your question”). The consequence may be that if systems that use language just as proficiently as humans in all contexts are argued by a thought experiment to not really understand, then we can’t test for “real” understanding, and it adds nothing to the purposes for which we use language. “Really” understanding becomes epiphenomenal: undetectable, causally inert, and irrelevant.

5. The Larger Philosophical Issues
5.1 Syntax and Semantics
Searle believes the Chinese Room thought experiment supports a larger point, which explains the failure of the Chinese Room to produce understanding. Searle argued that programs implemented by computers are just syntactical. Computer operations are “formal” in that they respond only to the physical form of the strings of symbols, not to the meaning of the symbols. Minds on the other hand have states with meaning, mental contents. We associate meanings with the words or signs in language. We respond to signs because of their meaning, not just their physical appearance. In short, we understand. But, and according to Searle this is the key point, “Syntax is not by itself sufficient for, nor constitutive of, semantics.” So although computers may be able to manipulate syntax to produce appropriate responses to natural language input, they do not understand the sentences they receive or output, for they cannot associate meanings with the words.

Searle (1984) presents a three premise argument that because syntax is not sufficient for semantics, programs cannot produce minds.

Programs are purely formal (syntactic).
Human minds have mental contents (semantics).
Syntax by itself is neither constitutive of, nor sufficient for, semantic content.
Therefore, programs by themselves are not constitutive of nor sufficient for minds.
The Chinese Room thought experiment itself is the support for the third premise. The claim that syntactic manipulation is not sufficient for meaning or thought is a significant issue, with wider implications than AI, or attributions of understanding. Prominent theories of mind hold that human cognition generally is computational. In one form, it is held that thought involves operations on symbols in virtue of their physical properties. On an alternative connectionist account, the computations are on “subsymbolic” states. If Searle is right, not only Strong AI but also these main approaches to understanding human cognition are misguided.

As we have seen, Searle holds that the Chinese Room scenario shows that one cannot get semantics from syntax alone. In a symbolic logic system, a kind of artificial language, rules are given for syntax. A semantics, if any, comes later. The logician first specifies the basic symbol set and some rules for manipulating strings to produce new ones (“well-formed fomulas”). These rules are purely syntactic – they are applied to strings of symbols solely in virtue of their syntax or form. A semantics, if any, for the symbol system must be provided separately. And if one wishes to show that interesting additional relationships hold between the syntactic operations and semantics, such as that the symbol manipulations preserve truth, one must provide somewhat complex meta-proofs to show this. So on the face of it, semantics is quite independent of syntax for artificial languages, and one cannot get semantics from syntax alone. “Formal symbols by themselves can never be enough for mental contents, because the symbols, by definition, have no meaning (or interpretation, or semantics) except insofar as someone outside the system gives it to them” (Searle 1989, 45).

Searle’s identification of meaning with interpretation in this passage is important. Searle’s point is clearly true of the causally inert formal systems of logicians. A semantic interpretation has to be given to those symbols by a logician. When we move from formal systems to computational systems, the situation is more complex. As many of Searle’s critics (e.g. Cole 1984, Dennett 1987, Boden 1988, and Chalmers 1996) have noted, a computer running a program is not the same as “syntax alone”. A computer is an enormously complex electronic causal system (some now have transistor counts that are comparable to the number of neurons in a human brain). State changes in the system are physical. One can interpret the physical states, e.g. voltages, as syntactic 1’s and 0’s, but the intrinsic reality is electronic and syntax is “derived”, a product of someone else’s interpretation. The states are syntactically specified by programmers, but when implemented in a running machine they are electronic states of a complex causal system directly or indirectly embedded in the real world. This is quite different from the abstract formal systems that logicians study. Dennett notes that no “computer program by itself” (Searle’s language) – e.g. a program lying on a shelf – can cause anything, even simple addition, let alone mental states. The program must be running. Chalmers (1996) offers a parody in which it is reasoned that recipes are syntactic, syntax is not sufficient for crumbliness, cakes are crumbly, so implementation of a recipe is not sufficient for making a cake. Implementation makes all the difference; an abstract entity (recipe, program) determines the causal powers of a physical system embedded in the larger causal nexus of the world.

Dennett (1987) sums up the issue: “Searle’s view, then, comes to this: take a material object (any material object) that does not have the power of causing mental phenomena; you cannot turn it in to an object that does have the power of producing mental phenomena simply by programming it – reorganizing the conditional dependencies of transitions between its states.” Dennett’s view is the opposite: programming “is precisely what could give something a mind”. But Dennett claims that in fact it is “empirically unlikely that the right sorts of programs can be run on anything but organic, human brains” (325–6).

A computer does not recognize that its binary data strings have a certain form, and thus that certain syntactic rules may be applied to them, unlike the man inside the Chinese Room. Inside a computer, there is nothing that literally reads input data, or that “knows” what symbols are. Instead, there are millions of transistors that change states. A sequence of voltages causes operations to be performed. We humans may choose to interpret these voltages as binary numerals and the voltage changes as syntactic operations, but a computer does not interpret its operations as syntactic or any other way. So perhaps a computer does not need to make the move from syntax to semantics that Searle objects to; it needs to move from complex causal connections to semantics. Furthermore, perhaps any causal system is describable as performing syntactic operations – if we interpret a light square as logical “0” and a dark square as logical “1”, then a kitchen toaster may be described as a device that rewrites logical “0”s as logical “1”s. But there is no philosophical problem about getting from syntax to breakfast.

In the 1990s, Searle began to use considerations related to these to argue that computational views are not just false, but lack a clear sense. Computation, or syntax, is “observer-relative”, not an intrinsic feature of reality: “…you can assign a computational interpretation to anything” (Searle 2002b, p. 17), even the molecules in the paint on the wall. Since nothing is intrinsically computational, one cannot have a scientific theory that reduces the mental, which is not observer-relative, to computation, which is. “Computation exists only relative to some agent or observer who imposes a computational interpretation on some phenomenon. This is an obvious point. I should have seen it ten years ago, but I did not.” (Searle 2002b, p.17, originally published 1993).

Critics note that walls are not computers; unlike a wall, a computer goes through state-transitions that are counterfactually described by a program (Chalmers 1996, Block 2002, Haugeland 2002). In his 2002 paper, Block addresses the question of whether a wall is a computer (in reply to Searle’s charge that anything that maps onto a formal system is a formal system, whereas minds are quite different). Block denies that whether or not something is a computer depends entirely on our interpretation. Block notes that Searle ignores the counterfactuals that must be true of an implementing system. Haugeland (2002) makes the similar point that an implementation will be a causal process that reliably carries out the operations – and they must be the right causal powers. Block concludes that Searle’s arguments fail, but he concedes that they “do succeed in sharpening our understanding of the nature of intentionality and its relation to computation and representation” (78).

Rey (2002) also addresses Searle’s arguments that syntax and symbols are observer-relative properties, not physical. Searle infers this from the fact that syntactic properties (e.g. being a logical “1”) are not defined in physics; however Rey holds that it does not follow that they are observer-relative. Rey argues that Searle also misunderstands what it is to realize a program. Rey endorses Chalmers’ reply to Putnam: a realization is not just a structural mapping, but involves causation, supporting counterfactuals. “This point is missed so often, it bears repeating: the syntactically specifiable objects over which computations are defined can and standardly do possess a semantics; it’s just that the semantics is not involved in the specification.” States of a person have their semantics in virtue of computational organization and their causal relations to the world. Rey concludes: Searle “simply does not consider the substantial resources of functionalism and Strong AI.” (222) A plausibly detailed story would defuse negative conclusions drawn from the superficial sketch of the system in the Chinese Room.

John Haugeland (2002) argues that there is a sense in which a processor must intrinsically understand the commands in the programs it runs: it executes them in accord with the specifications. “The only way that we can make sense of a computer as executing a program is by understanding its processor as responding to the program prescriptions as meaningful” (385). Thus operation symbols have meaning to a system. Haugeland goes on to draw a distinction between narrow and wide system. He argues that data can have semantics in the wide system that includes representations of external objects produced by transducers. In passing, Haugeland makes the unusual claim, argued for elsewhere, that genuine intelligence and semantics presuppose “the capacity for a kind of commitment in how one lives” which is non-propositional – that is, love (compare Steven Spielberg’s 2001 film Artificial Intelligence: AI).

To Searle’s claim that syntax is observer-relative, that the molecules in a wall might be interpreted as implementing the Wordstar program (an early word processing program) because “there is some pattern in the molecule movements which is isomorphic with the formal structure of Wordstar” (Searle 1990b, p. 27), Haugeland counters that “the very idea of a complex syntactical token … presupposes specified processes of writing and reading….” The tokens must be systematically producible and retrievable. So no random isomorphism or pattern somewhere (e.g. on some wall) is going to count, and hence syntax is not observer-relative.

With regard to the question of whether one can get semantics from syntax, William Rapaport has for many years argued for “syntactic semantics”, a view in which understanding is a special form of syntactic structure in which symbols (such as Chinese words) are linked to concepts, themselves represented syntactically. Others believe we are not there yet. AI futurist (The Age of Spiritual Machines) Ray Kurzweil holds in a 2002 follow-up book that it is red herring to focus on traditional symbol-manipulating computers. Kurzweil agrees with Searle that existent computers do not understand language – as evidenced by the fact that they can’t engage in convincing dialog. But that failure does not bear on the capacity of future computers based on different technology. Kurzweil claims that Searle fails to understand that future machines will use “chaotic emergent methods that are massively parallel”. This claim appears to be similar to that of connectionists, such as Andy Clark, and the position taken by the Churchlands in their 1990 Scientific American article.

Apart from Haugeland’s claim that processors understand program instructions, Searle’s critics can agree that computers no more understand syntax than they understand semantics, although, like all causal engines, a computer has syntactic descriptions. And while it is often useful to programmers to treat the machine as if it performed syntactic operations, it is not always so: sometimes the characters programmers use are just switches that make the machine do something, for example, make a given pixel on the computer display turn red, or make a car transmission shift gears. Thus it is not clear that Searle is correct when he says a digital computer is just “a device which manipulates symbols”. Computers are complex causal engines, and syntactic descriptions are useful in order to structure the causal interconnections in the machine. AI programmers face many tough problems, but one can hold that they do not have to get semantics from syntax. If they are to get semantics, they must get it from causality.

Two main approaches have developed that explain meaning in terms of causal connections. The internalist approaches, such as Schank’s and Rapaport’s conceptual representation approaches, and also Conceptual Role Semantics, hold that a state of a physical system gets its semantics from causal connections to other states of the same system. Thus a state of a computer might represent “kiwi” because it is connected to “bird” and “flightless” nodes, and perhaps also to images of prototypical kiwis. The state that represents the property of being “flightless” might get its content from a Negation-operator modifying a representation of “capable of airborne self-propulsion”, and so forth, to form a vast connected conceptual network, a kind of mental dictionary.

Externalist approaches developed by Dennis Stampe, Fred Dretske, Hilary Putnam, Jerry Fodor, Ruth Millikan, and others, hold that states of a physical system get their content through causal connections to the external reality they represent. Thus, roughly, a system with a KIWI concept is a system that has a state it uses to represent the presence of kiwis in the external environment. This kiwi-representing state will be a state that is appropriately causally connected to the presence of kiwis. Depending on the system, the kiwi representing state could be a state of a brain, or of an electrical device such as a computer, or even of a hydraulic system. The internal representing state can in turn play a causal role in determining the behavior of the system. For example, Rey (1986) endorses an indicator semantics along the lines of the work of Dennis Stampe (1977) and Fodor’s Psychosemantics. These semantic theories that locate content or meaning in appropriate causal relations to the world fit well with the Robot Reply. A computer in a robot body might have just the causal connections that could allow its inner syntactic states to have the semantic property of representing states of things in its environment.

Thus there are at least two families of theories (and marriages of the two, as in Block 1986) about how semantics might depend upon causal connections. Both of these attempt to provide accounts that are implementation neutral: states of suitably organized causal systems can have content, no matter what the systems are made of. On these theories a computer could have states that have meaning. It is not necessary that the computer be aware of its own states and know that they have meaning, nor that any outsider appreciate the meaning of the states. On either of these accounts meaning depends upon the (possibly complex) causal connections, and digital computers are systems designed to have states that have just such complex causal dependencies. It should be noted that Searle does not subscribe to these theories of semantics. Instead, Searle’s discussions of linguistic meaning have often centered on the notion of intentionality.

5.2 Intentionality
Intentionality is the property of being about something, having content. In the 19th Century, psychologist Franz Brentano re-introduced this term from Medieval philosophy and held that intentionality was the “mark of the mental”. Beliefs and desires are intentional states: they have propositional content (a person never just believes or desires, they believe that p, or desire that p, where sentences or clauses that represent propositions substitute for “p”). Searle’s views regarding intentionality are complex; of relevance here is that he makes a distinction between the original or intrinsic intentionality of genuine mental states, and the derived intentionality of language. A written or spoken sentence only has intentionality, namely derived intentionality, insofar as it is interpreted by someone. It appears that on Searle’s view, original intentionality must at least potentially be conscious. Searle then argues that the distinction between original and derived intentionality applies to computers. We can interpret the states of a computer as having content, but the states themselves do not have original intentionality. Many philosophers endorse this intentionality dualism, including Sayre (1986) and even Fodor (2009), despite Fodor’s many differences with Searle.

In a section of her 1988 book, Computer Models of the Mind, Margaret Boden notes that intentionality is not well-understood – reason to not put too much weight on arguments that turn on intentionality. Furthermore, insofar as we understand the brain, we focus on informational functions, not unspecified causal powers of the brain: “…from the psychological point of view, it is not the biochemistry as such which matters but the information-bearing functions grounded in it.” (241) Searle sees intentionality as a causal power of the brain, uniquely produced by biological processes. Dale Jacquette 1989 argues against a reduction of intentionality – intentionality, he says, is an “ineliminable, irreducible primitive concept.” However, most AI sympathizers have seen intentionality, aboutness, as bound up with information, and non-biological states can carry information just as well as can brain states. Hence many responders to Searle have argued that he displays substance chauvinism, in holding that brains understand but systems made of silicon with comparable information processing capabilities cannot, even in principle. Papers on both sides of the issue appeared, such as J. Maloney’s 1987 paper “The Right Stuff”, defending Searle, and R. Sharvy’s 1983 critique, “It Ain’t the Meat, it’s the Motion”. AI proponents such as Kurzweil (1999, see also Richards 2002) have continued to hold that AI systems can potentially have such mental properties as understanding, intelligence, consciousness and intentionality, and will exceed human abilities in these areas.

Other critics of Searle’s position take intentionality more seriously than Boden does, but deny his dualistic distinction between original and derived intentionality. Dennett (1987, e.g.) argues that all intentionality is derived, in that attributions of intentionality – to animals, other people, and even ourselves – are purely instrumental and allow us to predict behavior, but they are not descriptions of intrinsic properties. As we have seen, Dennett is concerned about the slow speed of things in the Chinese Room, but he argues that once a system is working up to speed, it has all that is needed for a mind with derived intentionality – and derived intentionality is the only kind that there is, according to Dennett. A machine can be an intentional system because intentional explanations work in predicting the machine’s behavior. Dennett also suggests that Searle conflates intentionality with awareness of intentionality. In his syntax-semantic arguments, “Searle has apparently confused a claim about the underivability of semantics from syntax with a claim about the underivability of the consciousness of semantics from syntax” (336). The emphasis on consciousness forces us to think about things from a first-person point of view, but Dennett 2017 continues to press the claim that this is a fundamental mistake if we want to understand the mental.

We might also worry that Searle conflates meaning and interpretation, and that Searle’s original or underived intentionality is just second-order intentionality, a representation of what an intentional object represents or means. Dretske and others have seen intentionality as information-based. One state of the world, including a state in a computer, may carry information about other states in the world, and this informational aboutness is a mind-independent feature of states. Hence it is a mistake to hold that conscious attributions of meaning are the source of intentionality.

Others have noted that Searle’s discussion has shown a shift over time from issues of intentionality and understanding to issues of consciousness. Searle links intentionality to awareness of intentionality, in holding that intentional states are at least potentially conscious. In his 1996 book, The Conscious Mind, David Chalmers notes that although Searle originally directs his argument against machine intentionality, it is clear from later writings that the real issue is consciousness, which Searle holds is a necessary condition of intentionality. It is consciousness that is lacking in digital computers. Chalmers uses thought experiments to argue that it is implausible that one system has some basic mental property (such as having qualia) that another system lacks, if it is possible to imagine transforming one system into the other, either gradually (as replacing neurons one at a time by digital circuits), or all at once, switching back and forth between flesh and silicon (see the brief discussion of cyborgization in section 4.3 above).

A second strategy regarding the attribution of intentionality is taken by critics who in effect argue that intentionality is an intrinsic feature of states of physical systems that are causally connected with the world in the right way, independently of interpretation (see the preceding Syntax and Semantics section). For example, a photo of Turing has intentionality: it has content about something, namely Turing. This form of intentionality is independent of interpretation – someone can look at a photo of Turing and think it is a photo of someone else. The same would presumably be the case with a sentence generated by a robot such as “I am now in the clock-room”. That sentence is about a specific robot in virtue of causal connection between the generation of the sentence and the location of the robot. When the robot generates that sentence, it means that the robot is in a room it calls “clock-room”. But someone might interpret and assign the wrong intentionality to it (e.g. they might think it is about some other robot than it actually is about). On this way of thinking about it, intentionality is one thing, and interpretation is something else, namely interpretation is a theory or hypothesis about something’s intentionality. The intentionality of the sentence or photo is its relation to the world; the interpretation of a sentence is second-order intentionality, namely it is about the sentence and its intentionality.

Fodor’s semantic externalism is influenced by Fred Dretske, but they come to different conclusions with regard to the semantics of states of computers. Over a period of years, Dretske developed an historical account of meaning or mental content that would preclude attributing beliefs and understanding to most machines. Dretske (1985) agrees with Searle that adding machines don’t literally add; we do the adding, using the machines. Dretske emphasizes the crucial role of natural selection and learning in producing states that have genuine content. Human built systems will be, at best, like Swampmen (beings that result from a lightning strike in a swamp and by chance happen to be a molecule by molecule copy of some human being, say, you) – they appear to have intentionality or mental states, but do not, because such states require the right history. AI states will generally be counterfeits of real mental states; like counterfeit money, they may appear perfectly identical but lack the right pedigree. But Dretske’s account of belief appears to make it distinct from conscious awareness of the belief or intentional state (if that is taken to require a higher order thought), and so would apparently allow attribution of intentionality to artificial systems that can get the right history by learning.

Howard Gardiner endorses Zenon Pylyshyn’s criticisms of Searle’s view of the relation of brain and intentionality, as supposing that intentionality is somehow a stuff “secreted by the brain”, and Pylyshyn’s own counter-thought experiment in which one’s neurons are replaced one by one with integrated circuit workalikes (see also Cole and Foelber (1984) and Chalmers (1996) for exploration of neuron replacement scenarios). Gardiner holds that Searle owes us a more precise account of intentionality than Searle has given so far, and until then it is an open question whether AI can produce it, or whether it is beyond its scope. Gardiner concludes with the possibility that the dispute between Searle and his critics is not scientific, but (quasi?) religious.

5.3 Mind and Body
Several critics have noted that there are metaphysical issues at stake in the original argument. The Systems Reply draws attention to the metaphysical problem of the relation of mind to body. It does this in holding that understanding is a property of the system as a whole, not the physical implementer. The Virtual Mind Reply holds that minds or persons – the entities that understand and are conscious – are more abstract than any physical system, and that there could be a many-to-one relation between minds and physical systems. (Even if everything is physical, in principle a single body could be shared by multiple minds, and a single mind could have a sequence of bodies over time.) Thus larger issues about personal identity and the relation of mind and body are in play in the debate between Searle and some of his critics.

Searle’s view is that the problem of the relation of mind and body “has a rather simple solution. Here it is: Conscious states are caused by lower level neurobiological processes in the brain and are themselves higher level features of the brain” (Searle 2002b, p. 9). In his early discussion of the CRA, Searle spoke of the causal powers of the brain. Thus his view appears to be that brain states cause consciousness and understanding, and “consciousness is just a feature of the brain” (ibid). However, as we have seen, even if this is true it begs the question of just whose consciousness a brain creates. Roger Sperry’s split-brain experiments suggest that perhaps there can be two centers of consciousness, and so in that sense two minds, implemented by a single brain. While both display at least some language comprehension, only one (typically created by the left hemisphere) controls language production. Thus many current approaches to understanding the relation of brain and consciousness emphasize connectedness and information flow (see e.g. Dehaene 2014).

Consciousness and understanding are features of persons, so it appears that Searle accepts a metaphysics in which I, my conscious self, am identical with my brain – a form of mind-brain identity theory. This very concrete metaphysics is reflected in Searle’s original presentation of the CR argument, in which Strong AI was described by him as the claim that “the appropriately programmed computer really is a mind” (Searle 1980). This is an identity claim, and has odd consequences. If A and B are identical, any property of A is a property of B. Computers are physical objects. Some computers weigh 6 lbs and have stereo speakers. So the claim that Searle called Strong AI would entail that some minds weigh 6 lbs and have stereo speakers. However it seems to be clear that while humans may weigh 150 pounds; human minds do not weigh 150 pounds. This suggests that neither bodies nor machines can literally be minds. Such considerations support the view that minds are more abstract than brains, and if so that at least one version of the claim that Searle calls Strong AI, the version that says that computers literally are minds, is metaphysically untenable on the face of it, apart from any thought-experiments.

If minds are not physical objects this inability of a computer to be a mind does not show that running an AI program cannot produce understanding of natural language, by something other than the computer (See section 4.1 above.)

Functionalism is a theory of the relation of minds to bodies that was developed in the two decades prior to Searle’s CRA. Functionalism is an alternative to the identity theory that is implicit in much of Searle’s discussion, as well as to the dominant behaviorism of the mid-twentieth Century. If functionalism is correct, there appears to be no intrinsic reason why a computer couldn’t have mental states. Hence the CRA’s conclusion that a computer is intrinsically incapable of mental states is an important consideration against functionalism. Julian Baggini (2009, 37) writes that Searle “came up with perhaps the most famous counter-example in history – the Chinese room argument – and in one intellectual punch inflicted so much damage on the then dominant theory of functionalism that many would argue it has never recovered.”

Functionalists hold that a mental state is what a mental state does – the causal (or “functional”) role that the state plays determines what state it is. A functionalist might hold that pain, for example, is a state that is typically caused by damage to the body, is located in a body-image, and is aversive. Functionalists distance themselves both from behaviorists and identity theorists. In contrast with the former, functionalists hold that the internal causal processes are important for the possession of mental states. Thus functionalists may agree with Searle in rejecting the Turing Test as too behavioristic. In contrast with identity theorists (who might e.g. hold “pain is identical with C-fiber firing”), functionalists hold that mental states might be had by a variety of physical systems (or non-physical, as in Cole and Foelber 1984, in which a mind changes from a material to an immaterial implementation, neuron by neuron). Thus while an identity theorist will identify pain with certain neuron firings, a functionalist will identify pain with something more abstract and higher level, a functional role that might be had by many different types of underlying system.

Functionalists accuse identity theorists of substance chauvinism. However, functionalism remains controversial: functionalism is vulnerable to the Chinese Nation type objections discussed above, and functionalists notoriously have trouble explaining qualia, a problem highlighted by the apparent possibility of an inverted spectrum, where qualitatively different states might have the same functional role (e.g. Block 1978, Maudlin 1989, Cole 1990).

Computationalism is the sub-species of functionalism that holds that the important causal role of brain processes is information processing. Milkowski 2017 notes that computational approaches have been fruitful in cognitive science; he surveys objections to computationalism and concludes that the majority target a strawman version. However Jerry Fodor, an early proponent of computational approaches, argues in Fodor 2005 that key mental processes, such as inference to the best explanation, which depend on non-local properties of representations, cannot be explained by computational modules in the brain. If Fodor is right, understanding language and interpretation appear to involve global considerations such as linguistic and non-linguistic context and theory of mind and so might resist computational explanation. If so, we reach Searle’s conclusion on the basis of different considerations.

Searle’s 2010 statement of the conclusion of the CRA has it showing that computational accounts cannot explain consciousness. There has been considerable interest in the decades since 1980 in determining what does explain consciousness, and this has been an extremely active research area across disciplines. One interest has been in the neural correlates of consciousness. This bears directly on Searle’s claim that consciousness is intrinsically biological and not computational or information processing. There is no definitive answer yet, though some recent work on anesthesia suggests that consciousness is lost when cortical (and cortico-thalamic) connections and information flow are disrupted (e.g., Hudetz 2012, a review article).

In general, if the basis of consciousness is confirmed to be at the relatively abstract level of information flow through neural networks, it will be friendly to functionalism, and if it is turns out to be lower and more biological (or sub-neuronal), it will be friendly to Searle’s account.

5.4 Simulation, duplication and evolution
In discussing the CRA, Searle argues that there is an important distinction between simulation and duplication. No one would mistake a computer simulation of the weather for weather, or a computer simulation of digestion for real digestion. Searle concludes that it is just as serious a mistake to confuse a computer simulation of understanding with understanding.

On the face of it, there is generally an important distinction between a simulation and the real thing. But two problems emerge. It is not clear that the distinction can always be made. Hearts are biological if anything is. Are artificial hearts simulations of hearts? Or are they functional duplicates of hearts, hearts made from different materials? Walking is normally a biological phenomenon performed using limbs. Do those with artificial limbs walk? Or do they simulate walking? Do robots walk? If the properties that are needed to be a certain kind of thing are high-level properties, anything sharing those properties will be a thing of that kind, even if it differs in its lower level properties. Chalmers (1996) offers a principle governing when simulation is replication. Chalmers suggests that, contra Searle and Harnad (1989), a simulation of X can be an X, namely when the property of being an X is an organizational invariant, a property that depends only on the functional organization of the underlying system, and not on any other details.

Copeland (2002) argues that the Church-Turing thesis does not entail that the brain (or every machine) can be simulated by a universal Turing machine, for the brain (or other machine) might have primitive operations that are not simple clerical routines that can be carried out by hand. (An example might be that human brains likely display genuine low-level randomness, whereas computers are carefully designed not to do that, and so computers resort to pseudo-random numbers when apparent randomness is needed.) Sprevak 2007 raises a related point. Turing’s 1938 Princeton thesis described such machines (“O-machines”). O-machines are machines that include functions of natural numbers that are not Turing-machine computable. If the brain is such a machine, then, says Sprevak,: “There is no possibility of Searle’s Chinese Room Argument being successfully deployed against the functionalist hypothesis that the brain instantiates an O-machine….” (120).

Copeland discusses the simulation / duplication distinction in connection with the Brain Simulator Reply. He argues that Searle correctly notes that one cannot infer from X simulates Y, and Y has property P, to the conclusion that therefore X has Y’s property P for arbitrary P. But Copeland claims that Searle himself commits the simulation fallacy in extending the CR argument from traditional AI to apply against computationalism. The contrapositive of the inference is logically equivalent – X simulates Y, X does not have P therefore Y does not – where P equals: understands Chinese. The faulty step is: the CR operator S simulates a neural net N, it is not the case that S understands Chinese, therefore it is not the case that N understands Chinese. Copeland also notes results by Siegelmann and Sontag (1994) showing that some connectionist networks cannot be simulated by a universal Turing Machine (in particular, where connection weights are real numbers).

There is another problem with the simulation-duplication distinction, arising from the process of evolution. Searle wishes to see original intentionality and genuine understanding as properties only of certain biological systems, presumably the product of evolution. Computers merely simulate these properties. At the same time, in the Chinese Room scenario, Searle maintains that a system can exhibit behavior just as complex as human behavior, simulating any degree of intelligence and language comprehension that one can imagine, and simulating any ability to deal with the world, yet not understand a thing. He also says that such behaviorally complex systems might be implemented with very ordinary materials, for example with tubes of water and valves.

This creates a biological problem, beyond the Other Minds problem noted by early critics of the CR argument. While we may presuppose that others have minds, evolution makes no such presuppositions. The selection forces that drive biological evolution select on the basis of behavior. Evolution can select for the ability to use information about the environment creatively and intelligently, as long as this is manifest in the behavior of the organism. If there is no overt difference in behavior in any set of circumstances between a system that understands and one that does not, evolution cannot select for genuine understanding. And so it seems that on Searle’s account, minds that genuinely understand meaning have no advantage over creatures that merely process information, using purely computational processes. Thus a position that implies that simulations of understanding can be just as biologically adaptive as the real thing, leaves us with a puzzle about how and why systems with “genuine” understanding could evolve. Original intentionality and genuine understanding become epiphenomenal:

Man to robot companion: “It is sad that you understand nothing”.

Robot companion: “I know, I know. An American philosopher proved ages ago that I never will, so nothing can be done about that. But let’s set that sad thought aside and return to our discussion of the unreliable narrator in Bronte’s works that we were having, and your own trip to the Yorkshire moors. There are some lovely areas there, as I can see using my remote cam. I haven’t read all her novels, but am familiar with …”.

Conclusion
As we have seen, since its appearance in 1980 the Chinese Room argument has sparked discussion across disciplines. Despite the extensive discussion there is still no consensus as to whether the argument is sound. At one end we have Julian Baggini’s (2009) assessment that Searle “came up with perhaps the most famous counter-example in history – the Chinese room argument – and in one intellectual punch inflicted so much damage on the then dominant theory of functionalism that many would argue it has never recovered.” Whereas philosopher Daniel Dennett (2013, p. 320) concludes that the Chinese Room argument is “clearly a fallacious and misleading argument”. Hence there is no consensus as to whether the argument is a proof that limits the aspirations of Artificial Intelligence or computational accounts of mind.

Meanwhile work in artificial intelligence and natural language processing has continued. The CRA led Stevan Harnad and others on a quest for “symbol grounding” in AI. Many in philosophy (Dretske, Fodor, Millikan) worked on naturalistic theories of mental content. Speculation about the nature of consciousness continues in many disciplines. And computers have moved from the lab to the pocket and the wrist.

At the time of Searle’s construction of the argument, personal computers were very limited hobbyist devices. Weizenbaum’s ‘Eliza’ and a few text ‘adventure’ games were played on DEC computers; these included limited parsers. More advanced parsing of language was limited to computer researchers such as Schank. Much changed in the next quarter century; billions now use natural language to interrogate and command virtual agents via computers they carry in their pockets. Has the Chinese Room argument moderated claims by those who produce AI and natural language systems? Some manufacturers linking devices to the “internet of things” make modest claims: appliance manufacturer LG says the second decade of the 21st century brings the “experience of conversing” with major appliances. That may or may not be the same as conversing. Apple is less cautious than LG in describing the capabilities of its “virtual personal assistant” application called ‘Siri’: Apple says of Siri that “It understands what you say. It knows what you mean.” IBM is quick to claim its much larger ‘Watson’ system is superior in language abilities to Siri. In 2011 Watson beat human champions on the television game show ‘Jeopardy’, a feat that relies heavily on language abilities and inference. IBM goes on to claim that what distinguishes Watson is that it “knows what it knows, and knows what it does not know.” This appears to be claiming a form of reflexive self-awareness or consciousness for the Watson computer system. Thus the claims of strong AI now are hardly chastened, and if anything some are stronger and more exuberant. At the same time, as we have seen, many others believe that the Chinese Room Argument showed once and for all that at best computers can simulate human cognition.

Though separated by three centuries, Leibniz and Searle had similar intuitions about the systems they consider in their respective thought experiments, Leibniz’ Mill and the Chinese Room. In both cases they consider a complex system composed of relatively simple operations, and note that it is impossible to see how understanding or consciousness could result. These simple arguments do us the service of highlighting the serious problems we face in understanding meaning and minds. The many issues raised by the Chinese Room argument may not be settled until there is a consensus about the nature of meaning, its relation to syntax, and about the biological basis of consciousness. There continues to be significant disagreement about what processes create meaning, understanding, and consciousness, as well as what can be proven a priori by thought experiments.


------------

VideoThe Chinese Room argument, explained clearly by Searle himself (youtu.be)

submitted 4 years ago by ockidocki

746 commentssharesavehidereportcrosspost
top 500 comments
sorted by: best (suggested)

[–]whentheworldquiets 528 points 4 years ago 

I've heard this described before, and I don't think it refutes 'strong AI', as he puts it, at all. Here's why:

Searle describes himself as analogous to the CPU - which he is in this thought experiment. And he says he doesn't understand Chinese, which he doesn't. But nobody is claiming that the CPU running an AI understands what it is doing, any more than anyone claims the molecules within our synapses know what they're doing.

To put it another way: Searle puts himself in the box and contrasts his understanding of English with his ignorance of Chinese, and on that basis says there is no understanding going on in the box . But that's an insupportable leap. He isn't doing any understanding, but the combination of him, the rulebook, and the symbols are doing the understanding. He has made himself into just one cog in a bigger machine, and the fact a single cog can't encapsulate the entire function of the machine is irrelevant.

permalinkembedsavereport

[–]ReadMoreWriteLess 115 points 4 years ago 

I'm with you. I didn't find this compelling.

For me it's the idea of "meaning". It's hard to parse the concept that I know what a word translates to and how to answer questions with it but not have "meaning" to me.

permalinkembedsaveparentreport

[–]ben10says 18 points 4 years ago* 

The clear gap in his reasoning is the rule book. Him locked in the room can answer English questions because he is the rule book. In Chinese, the rule book knows what the meanings are.

Meaning is nothing more than relationships between different concepts. What I mean when I say that I ‘understand’ what water is is me being able to link wetness, satisfaction of thirst, the ocean and all other relationships to this symbol of ‘water’ but not connecting heat, solidity or opaqueness.

permalinkembedsaveparentreport

[–]xthecharacter 15 points 4 years ago 

Yep, in the example where he's the CPU, if he memorizes the rule book to gain parity with the English example, he at that point has learned Chinese, and if he still needs external sources to answer the questions, then the original details of the argument were not laid out completely.

permalinkembedsaveparentreport

[–]ryanwalraven 7 points 4 years ago* 

This is what immediately struck me. If you performed the experiment as described, reading some sort of English instructions of how to answer the Chinese, obviously eventually you would learn Chinese.

He really sort of blew the explanation imho, and he needed to emphasize that the rulebook isn't some translation dictionary, but something arbitrary. For example, maybe the rulebook maps the Chinese symbols to a drawer in a huge filing cabinet that you retrieve answers from before sending them back out. You will never learn Chinese this way, because you never even have to see the answer. However, "the program" or other parts of the computer clearly have, if they have all the answers to the questions. And folks will say, "Do you really believe the filing cabinet+room is conscious?" But we literally had to insert a conscious being inside to make the whole thought experiment work...

I remember thinking the Chinese Room argument was OK when I first heard of it, but now I'm not sure why. This video had the opposite of the intended effect.

permalinkembedsaveparentreport

[–]Qeztotz 1 point 4 years ago 

The Chinese room argument functions if, as he does, you predicate the thought experiment that the human consciousness is based in its ability to process information. On the surface it seems logical, after all, our ability to quickly think, design, and otherwise act upon the world is unparalleled.

I believe however that human consciousness (if we assume it exists) is not tied solely to the ability to process, but also to the ability to memorise.

Considering the experiment not as a "computer" and its "program" but as a "brain" and its "memory" there is no difference between the human and the entire Chinese room.

Individually neither the book nor the person understands Chinese. But if together they are able to emulate a native speaker, then together they understand Chinese, just like the brain and memory. The outside observers are not inspecting whether the processor understands, they are inspecting whether the room's collective efforts does.

permalinkembedsaveparentreport
load more comments (1 reply)

[–]andrewsad1 1 point 4 years ago* 

Yep, in the example where he's the CPU, if he memorizes the rule book to gain parity with the English example, he at that point has learned Chinese

I disagree with Searle saying that the Chinese Room refutes Strong AI, but this rubs me the wrong way. The rule book doesn't translate what the Chinese characters are, it just says "if the input is 二加三是多少?, send 五. So, if he memorizes the rule book, and someone asks him what 2+3 is in Chinese, he can answer in Chinese, but he wouldn't understand what was being asked or what the answer was.

My problem with the thought experiment is that he's ignoring the fact that a computer can have memory, which the Chinese Room lacks. Any question that requires memory would be unanswerable. If you asked it "what was the last question I asked" twice, it would have to give the same answer both times, while a real artificial intelligence could answer that accurately.

permalinkembedsaveparentreport

[–]KantianNoumenon 19 points 4 years ago 

You have to understand functionalism, which is the view he is arguing against. I explain it a bit in my comment here .

permalinkembedsaveparentreport
load more comments (1 reply)

[–]didymus5 63 points 4 years ago 

Also, such a set of instructions to enable an English speaker to respond to Chinese questions would be extraordinarily complex, and it would be no less complex if it were a computer program.

But strong AI advocates don’t want a computer to be able to answer questions like a human using syntax with a context sensitive program. They want a non contextual general program to emulate semantic meaning. They want a computer to “think“ semantically as well as syntactically. Neural networks are already learning based off of experiences. After a neural network is trained, it could be said to “remember” the “experience” of being trained. I’m not sure what he is getting at by saying he “understands” English or that an English word has “meaning”... is it that he can remember the word being used in various contexts? Cool, why can’t a computer do the same?

permalinkembedsaveparentreport

[–][deleted] 19 points 4 years ago 

I got caught up in the idea of following a rulebook for creating answers in a different language: If all the answers are predescribed, the questions also have to be predescribed. In order to give out an answer in chinese that fools the native speaker asking the question; the rulebooks needs to either a) have all the possible questions (ie imputs of chinese characters) or b) some sort of logical system to creating a set of characters (answer) based on the question. If its b) then the person in the room would, in my opinion, learn to understand the language as time passes, similiar to an AI. I might be wrong, but thats my initial response to the video

permalinkembedsaveparentreport

[–]enternationalist 17 points 4 years ago 

The logical system you're describing, in the real world equivalent, is physics. Say you have a book that perfectly describes the structure and physical behaviour of a human brain. Say someone gives you a question. You calculate the vibrations this sound would make, the vibrations transmitted to the inner ear, the stimulation of nerve cells by cilia, the cascade of neural inputs leading to a verbal output. You hand this output back.

The operator doesn't learn about the language - and more importantly, even if they do, it bears no relation to the operations they are performing. They're just doing physics, and those physics happen to begin and end in Chinese.

What's doing the learning? The program is. If it sufficiently models the mind, our operator will be procedurally creating new parts to the program based on the inputs (e.g. new neurons, connections). To the operator, they are simply adding extra physical elements, and no new understanding of Chinese is created for them. It is the program itself that has encoded the understanding.

permalinkembedsaveparentreport

[–]SL0THM0NST3R 9 points 4 years ago 

My first thought too. Just the act of repetition of "the rules" would teach you "the rules". Ie you are learning Chinese... Until eventually the rule book is no longer needed.

permalinkembedsaveparentreport

[–]LuxDeorum 1 point 4 years ago 

Take a simpler model and I think this becomes less clear. For example, if I wrote a program that plays tic tac toe in exactly the way I would, then converted the program into machine code and taught you exactly how to manipulate machine code the way a computer would, do you think you would inevitably realize you were playing tic tac toe?

permalinkembedsaveparentreport

[–]za1k 1 point 4 years ago 

I dont think "the person eventually infers understanding of chinese from the rules" argument tracks. To begin with, does this mean that the system does not know chinese untill the person has learned it? Further, if you have the same experiment but replace the person in the room inbetween questions then no learning will occur, yet the behaviour of the room system is the same.

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

Words have meaning because the words represent life experiences that are associated with feelings, emotions, ideas, likes and dislikes.

Can an AI ever have feelings of love, feelings of distaste or desire? Can AI appreciate beauty, asunset?

Can an ai feel loneliness or longing? Hate, disgust for another AIs actions?

I think the scientific world has become too stuck on the Turing test

The feeling of satisfaction from completing a work of art or helping a stranger?

Sadness, remorse, grief?

permalinkembedsaveparentreport

[–]aKnightWh0SaysNi 36 points 4 years ago 

If humans can feel it, an AI can theoretically be built to feel it. We aren’t made of magic, it should be possible to construct an AI that registers meaning the same way a human brain does.

permalinkembedsaveparentreport

[–]Cross_22 8 points 4 years ago 

I can build a chip that toggles electricity but I cannot build a chip that is happy. For this reductionist claim to be true we would need to know how consciousness works and if indeed it can be reduced to a logical network, an EM field or something else entirely.

permalinkembedsaveparentreport

[–]Crizznik 42 points 4 years ago 

You can't build a chip that is happy any more than a brain can grow a neuron that is happy. It's the sum of the parts that conjure the pattern of brain activity that we call "happy". Sure, you might not be able to build a chip that is happy, but you might build a series of chips that can form a pattern that can be discerned as "happy", and the necessary chips aside to register that pattern. Now, you're right, we're not there yet, but that's the thing, yet. We may never get there. Even so, it may still be reducible to that point, even if we can't understand it. I say it's better to say "I don't know, let's find out" than to wave off such implications as impossible just cause it makes us feel icky.

permalinkembedsaveparentreport
load more comments (14 replies)

[–]gtmog 1 point 4 years ago 

At some point someone is going to create a neural network chip that contains within it a sufficiently complicated state to be said to feel pain or terror. That chip will be used to tell if it's dark enough in a room to turn the lights on.

Terror? Lights on. Pain? Lights off.

And no one will ever know.

permalinkembedsaveparentreport
load more comments (4 replies)

[–][deleted] 23 points 4 years ago 

Yes, AI will feel those emotions, and furthermore, AI will feel emotions orders of magnitude more complex and profound than we could ever imagine. It takes a special kind of narrow mindedness to think that we, semi intelligent monkeys, have somehow acquired the peak of what can be felt by a sentient being. Furthermore, it is completely possible that our lofty emotions of love, sadness, grief will be viewed by higher level beings as being just as simple and vulgar and horniness and anger.

I can imagine that at some point, higher level AI will be able to create purely synthetic emotions that are completely non biological in origin, completely pure of evolutionary benefit. I can also imagine that these AI will create emotions as art, and share them in the same way we share drawn art.

permalinkembedsaveparentreport
[–][deleted] 4 years ago 

[removed]

[+][deleted] 4 years ago (1 child)

[+][deleted] 4 years ago (5 children)


[–]BernardJOrtcutt[M] 1 point 4 years ago 

Your comment was removed for violating the following rule:

Argue your Position

Opinions are not valuable here, arguments are! Comments that solely express musings, opinions, beliefs, or assertions without argument may be removed.

Repeated or serious violations of the subreddit rules will result in a ban.

This is a shared account that is only used for notifications. Please do not reply, as your message will go unread.

permalinkembedsaveparentreport
load more comments (8 replies)

[–]LuxDeorum 3 points 4 years ago 

could it be that the sensation we have of words having meanings is just the result of a finitely complex mechanical process describing itself

permalinkembedsaveparentreport

[–]randacts13 1 point 4 years ago 

Words have meaning because you can remember them and what they're supposed to mean.

You're conflating the word for an emotion for the actual emotion. Knowing what sad means doesn't make me sad. Likewise I could feel sad without a word for it.

That's beside the point though. You seem to be saying that emotions are something other than physical processes—but of course, that's all they are. Theoretically any physical process can be simulated. The problem is knowing what all the inputs are, and what the expected outputs should be.

I'd also like to know what you think about people who are incapable of experiencing the emotions you describe, either because of the way they were born or due to damage to their hardware . A psychopath knows what remorse means, but cannot feel it. Does that make them not human?

There are many problems with the Chinese Room Argument. The fundamental issue is that, like you said, the Turing Test is not sufficient to describe the human mind. It's only meant to describe our perception of a simulated mind. In fact Turing's question could be reduced to simply "can machines trick humans?"

Searle's argument is based on the false premise that our perception is equal to reality. That a computer that seems to be human is human. Believing a lie doesn't make it true, the light from the sun is not yellow, and objects in the mirror are closer than they appear. Our perception is not reality.

...And that is the real reason we will likely never accomplish Hard AI — or one where the simulation is exactly the same as the mind. It would require us to perceive reality as it truly is. To know the true nature of all the inputs and outputs of the mind. If we can't perceive these things truthfully - we can't simulated them accurately.

permalinkembedsaveparentreport

[–]Dovaldo83 1 point 4 years ago* 

But strong AI advocates don’t want a computer to be able to answer questions like a human using syntax with a context sensitive program. They want a non contextual general program to emulate semantic meaning. They want a computer to “think“ semantically as well as syntactically. Neural networks are already learning based off of experiences. After a neural network is trained, it could be said to “remember” the “experience” of being trained.

I question this notion, at least the type of strong AI that is mentioned in the video. There very well be strong AI advocates who want such, but that is an unnecessary requirement to arrive at strong AI.

The idea is that if a computer passes the Turing test so convincingly that it is indistinguishable from a human mind, then it is the functional equivalent of a human. Just like if I replaced your car's engine with something that responded to gas pedal depression input and output torque exactly like your car's original engine did, we'd call it an engine. It wouldn't matter if we opened it up and found out it was producing the same results in a fundamentally different way. It functions exactly like an engine so it's an engine.

If we knew what mechanically constitutes a full thought and the experience of self, we'd be able to make sentient AI already. Using exactly what goes on in our heads as a standard for true AI is putting the cart before the horse. The Turning Test gives us an appropriate standard to shoot for that doesn't require puzzling our the mechanics that give rise to our consciousness.

It might be that our current concept of thinking, remembering, and experience of being trained isn't necessary to create an AI that fulfills the Turning test, and that would make it no less valid. After all, we can't look in other people's heads and confirm without a doubt that the way they think, remember, and experience is the same as ours, yet we accept their sentience because they function in all the ways we expect sentient beings to function.

permalinkembedsaveparentreport

[–]Jabru08 56 points 4 years ago* 

This is the so called "Systems Reply," and is a position that Searle explicitly argues against in his essay Minds, Brains, and Programs . An excerpt:

My response to the systems theory is quite simple: let the individual internalize all of these elements of the system. He memorizes the rules in the ledger and the data banks of Chinese symbols, and he does all the calculations in his head. The individual then incorporates the entire system. There isn't anything at all to the system that he does not encompass. We can even get rid of the room and suppose he works outdoors. All the same, he understands nothing of the Chinese, and a fortiori neither does the system, because there isn't anything in the system that isn't in him. If he doesn't understand, then there is no way the system could understand because the system is just a part of him.

[...]

Furthermore, the systems reply would appear to lead to consequences that are independently absurd. If we are to conclude that there must be cognition in me on the grounds that I have a certain sort of input and output and a program in between, then it looks like all sorts of noncognitive subsystems are going to turn out to be cognitive. For example, there is a level of description at which my stomach does information processing, and it instantiates any number of computer programs, but I take it we do not want to say that it has any understanding. But if we accept the systems reply, then it is hard to see how we avoid saying that stomach, heart, liver, and so on, are all understanding subsystems, since there is no principled way to distinguish the motivation for saying the Chinese subsystem understands from saying that the stomach understands. It is, by the way, not an answer to this point to say that the Chinese system has information as input and output and the stomach has food and food products as input and output, since from the point of view of the agent, from my point of view, there is no information in either the food or the Chinese -- the Chinese is just so many meaningless squiggles. The information in the Chinese case is solely in the eyes of the programmers and the interpreters, and there is nothing to prevent them from treating the input and output of my digestive organs as information if they so desire.

To anyone reading this, do yourself a favor and read the rest of his response (it starts at page 5).

permalinkembedsaveparentreport

[–]Myto 61 points 4 years ago 

It contains gems such as

Actually I feel somewhat embarrassed to give even this answer to the systems theory because the theory seems to me so implausible to start with. The idea is that while a person doesn't understand Chinese,somehow the conjunction of that person and bits of paper might understand Chinese. It is not easy for me to imagine how someone who was not in the grip of an ideology would find the idea at all plausible.

To me this whole argument seems like just an intuition pump for Searle's incredulity. What if you replace the human and the bits of paper and so on with molecules and neurons, none of those understand any meanings either, yet the whole system (a human) does understand. So what does the argument actually show? Does Searle think neurons are made of magic?

permalinkembedsaveparentreport

[–]yahkopi 18 points 4 years ago 

So what does the argument actually show? Does Searle think neurons are made of magic?

It's just the hard problem in another guise. What Searle seems to be trying to get at with his idea of "meaning" is really just intentionality as a marker of mental states ie of consciousness.

permalinkembedsaveparentreport

[–]thizizdiz 14 points 4 years ago 

No, but his view is not that there is something special about neurons that makes them the only things that can collectively produce consciousness. He thinks, in principle, consciousness should be able to arise out of any sufficiently ordered materials, but the Chinese Room argues against consciousness being able to arise from a machine that operates only via the manipulation of symbols, i.e., a Turing machine. No matter how complex you make it, it will still at the baseline be manipulating symbols, and any reasonable person, to him, would recognize that that is not what we mean by understanding or consciousness.

permalinkembedsaveparentreport

[–][deleted] 4 points 4 years ago 

He thinks, in principle, consciousness should be able to arise out of any sufficiently ordered materials,

I don't think that's true. Because saying that is almost equivalent to saying that consciousness is "hardware" independent (independent of requiring any specific class of hardware): all that matters is the "organization", anything that can be used to bring the right order and organization will do the job. But that's roughly what computational theory of mind leads to, except it's talking about organization in more formal computational terms. It seems that Searle believes that a specific kind of hardware should be necessary to get "meaning" and biological hardwares is one of them.

permalinkembedsaveparentreport

[–]thizizdiz 1 point 4 years ago 

That's a good point. The materials themselves probably matter as well to him, but the point remains there is nothing special to him about brain matter and neurons. In other words, a mind can in principle be made from something other than a brain, but not just anything can make a mind. Something like that.

permalinkembedsaveparentreport

[–]Mediocre_Competition 1 point 4 years ago 

The materials themselves probably matter as well to him, but the point remains there is nothing special to him about brain matter and neurons.

Correct. Searle, like Block, thinks consciousness is a biological phenomenon.

permalinkembedsaveparentreport

[–]Zaptruder 1 point 4 years ago 

It seems that Searle believes that a specific kind of hardware should be necessary to get "meaning" and biological hardwares is one of them.

There's some degree of reasonableness to that statement... but I think it's really just a case of how the hardware enables certain forms of information processing.

Could a traditional von neumann architecture machine do the job? In theory if it was fast enough, it could emulate systems that could figure out meaning - and those emulated systems would understand meaning... but the architecture of the hardware does not lend itself well to deriving meaning as we intuitively understand it.

permalinkembedsaveparentreport
[+][deleted] 4 years ago (5 children)


[–]georgioz 1 point 4 years ago 

Chinese Room argues against consciousness being able to arise from a machine that operates only via the manipulation of symbols, i.e., a Turing machine

You understand that Turing Machine is the most powerful computation model we possess? You can emulate any and all computers by TM. If understanding - e.g if artificial general intelligence is possible on computers it is possible to model it via TM.

permalinkembedsaveparentreport

[–]thizizdiz 1 point 4 years ago 

What about quantum computers?

Also, even if what you say is true, it doesn't negate what I said. He's arguing against TM's being able to be conscious but saying nothing about something beyond a TM. If all computers can ever be are TM's, then it would mean that no computers can ever be conscious, but that is just a consequence of the argument provided that that premise is true. Searle is not taking a stance on whether a TM is all a computer can ever be.

permalinkembedsaveparentreport

[–]georgioz 1 point 4 years ago* 

What about quantum computers?

Even those can be modeled by Turing Machine. TM is incredibly powerful as it has advantage of infinite memory. Also given the nature of TM basically anything that can be described by human - on operation level - e.g. in a book or even something you imagine - can be modeled by TM.

Anyway I think that the main problem of Searle's argument is that the possibility of understanding being an emergent phenomenon. To use an analogy you can ask for instance what is Microsoft Word. In one sense it is a piece of compiled code stored on your hard drive. On the other hand it is piece of emergent phenomena that is what you see on the monitor when executed. It is similar to human. If you are in coma then you are in a frozen state. So the question is - does consciousness exist? On some level it does because potentiality of it is part of your comatic body. But in the real sense it does not exist because the relevant neural pathways do not fire in order for "cogito ergo sum" thought to be executed.

So the argument is that Searle's argument that the syntax semantics cannot be bridged can be explained that the execution of syntax on certain physical hardware creates consciousness and understanding. In the same way people in cryonic sleep do not posses understanding but the understanding can be created by waking them up. Human body and brain is not consciousness. Human brain and body creates consciousness as emergent phenomenon by executing certain syntax we call "living" - the neurons firing, the organs working etc..

And I think he himself gets into the bind. We know for instance that all humans start as single fertilized egg. There is this DNA that stores the "code" for how humans are to be and in presence of good environment this basic code is executed and the cell divides, and then divides again, and then there are specialized cells - and so forth until the living human is born and the program continues with processing of external stimuli until at some point we are conscious. There had to be some point where this program execution and potentiality of the consciousness in form of the program of unthinking cell created consciousness. Or in other word this cell replication and all the rest can definitely be described as an algorithm for Turing Machine. How does Searle explain this phenomenon?

Also back to Chinese room. I think Searle is just hopping the definition of what "understand" means. For instance I can imagine the whole complex of "Human+Box" just as a very good version of google translate with chatbot. So if we define "understand" as being proficient at answering questions based on some arbitrary criteria then the system understands. But Searle then just moves the goalpost and say that to really understand the system needs to be conscious - which should be determined by some further tests. And it is for Searle to say what those tests should be. Obviously Searle thinks other people are conscious. They passed some test of his to convince him that they are not zombies. Once Searle desribes this test of consciousness down in billions of books of billions pages and we can modify the Chinese Room to fit the criteria. So we went back to why I said that that TM is incredibly powerful.

permalinkembedsaveparentreport

[–]thizizdiz 1 point 4 years ago 

There's a lot to unpack here, but I gotta say, I don't find what you say convincing.

TM is incredibly powerful as it has advantage of infinite memory.

You can give a graphing calculator infinite memory. That won't make it any more conscious.

anything that can be described by human - on operation level - e.g. in a book or even something you imagine - can be modeled by TM.

I don't doubt that, but at the base level it will be simulating human behavior, not replicating human cognition. There's a difference.

On the other hand it is piece of emergent phenomena that is what you see on the monitor when executed. It is similar to human.

In the same way people in cryonic sleep do not posses understanding but the understanding can be created by waking them up.

There is this DNA that stores the "code" for how humans are to be and in presence of good environment this basic code is executed

I think you're using these analogies too loosely. Yes we call DNA the code of our cells, but in practice it works quite differently from computer code. People in cryonic sleep do possess the potential for understanding, even if their brains are not working at that moment. A computer does not possess that potential if it's already working from symbol manipulation at the get go. A screen displaying images is not an emergent phenomena, it's just a screen (a computational device) displaying images.

permalinkembedsaveparentreport

[–]georgioz 1 point 4 years ago* 

You can give a graphing calculator infinite memory. That won't make it any more conscious.

It will not. But it will also make it a true Turing Machine - a thing where you can emulate whatever computation you want. Much more powerful thing than your calculator.

I don't doubt that, but at the base level it will be simulating human behavior, not replicating human cognition. There's a difference.

Ok, then explain this difference in words. How do you spot a difference between conscious and nonconscious? Whatever that difference is it can be included into the computation model and you get what you want.

I think you're using these analogies too loosely. Yes we call DNA the code of our cells, but in practice it works quite differently from computer code.

This completely misses the point but also is in fact incorrect. Computing can be carried out on biological substrate .

Anyways again - it is clear that the fertilized egg is not conscious. But it then gains the consciousness somehow - and consistently across billions of occasions all around the world according to some process that can be described in some sort of syntax obviously. So again, at some point of execution of the process of biological growth the human consciousness arises. So we have breached the syntax > semantic barrier. If yes how? If not - why not?

permalinkembedsaveparentreport

[–]thizizdiz 1 point 4 years ago 

Ok, then explain this difference in words. How do you spot a difference between conscious and nonconscious?

If we could simply spot the difference, then we wouldn't need to argue about it. Consciousness is not understood by our best scientists, but we know some key features, like phenomenal experience. You can asset this can be modeled in a computer, but I don't see how you can claim that when we don't even yet grasp how phenomenal experience works.

Anyways again - it is clear that the fertilized egg is not conscious. ...

The zygote is not conscious and yet the infant that is born is conscious. How does this happen? I would imagine the baby develops consciousness once its nervous system is sufficiently functional. Sure, DNA is a kind of code for the production of various cells, but I don't think it's useful to refer to this as a syntax - semantics barrier when talking about consciousness and cognition. We're talking about the consciousness of a developed brain vs. that of a fully realized supercomputer.

permalinkembedsaveparentreport
continue this thread
load more comments (1 reply)

[–]Gned11 8 points 4 years ago 

That quote fascinates me, because the systems reply seems in my gut to be both obvious and plausible... clearly Dennett infected me with "ideology"

permalinkembedsaveparentreport

[–]OldThymeyRadio 9 points 4 years ago 

He sounds like a homophobe who’s tired of having to formulate arguments against the “unnaturalness” of homosexuality because “C’mon, I mean... it’s just gross! Why are we even arguing about this?”

permalinkembedsaveparentreport

[–]Jabru08 11 points 4 years ago* 

(I'm copying and pasting from a paper I wrote on the topic a while ago, with a few edits)

I believe that Searle does a convincing job in his argument, provided that you don't suppose that he is saying more than he actually is. The argument in its original formulation was a thrust against the claims of “strong AI,” which he found to be mistaking simulation for duplication. It challenges the convenient assumption that the semantics of such machines were not important if the syntax could be perfected. It questions the usefulness of the Turing test as a benchmark for development of programs emulating human behavior. What he does not argue is that it would be impossible to ever construct a man made machine that could understand English or Chinese in the same sense that a human person understands them. He explicitly states that such a thing could be possible, provided that the causal powers of the brain could be reproduced. Clearly, he believes that such causal powers do not exist in computers as we know of them today, but that is not to say that this will always be the case. If we could somehow duplicate a human brain, he says, then that would result in an entity with conscious experience, like a human person’s.

If you push the argument far enough, at the end of the day what you will end up with is a restatement of the problem of other minds, but I don’t find that to be a fault in Searle’s argument. He alludes to this in his response to the combination reply. If there existed a robot that walked and talked like a human and contained a computer program that could simulate all of the individual synapses of a human brain, we might wrongly ascribe intentionality to it unless we could know that it was simply running a formal program. He directly addresses this in his response to the other minds reply, and concludes that the correct output, as delivered by a robot, could exist without the accompanying mental state, therefore simply observing that a robot gives a correct output is not sufficient to say that it understands anything. I also don’t find the “systems reply,” that it is not the man who understands Chinese but the entire room , a convincing objection to Searle. If you reduced the entire room into the man’s head, and had him memorize the rules for manipulating Chinese symbols, it appears to me that that sort of understanding is different than the man’s understanding of English.

To answer your question about magic neurons, the answer is basically yes, but I wouldn't phrase it as dismissively. I suppose you could rephrase "magic" as "causality" (whatever that means).

Searle argues that intentionality is a biological phenomenon that is confined to brains, or any machine that has the same causal power as brains. He believes that information processing is a function of brains shaped by millennia of evolution in the same sense that digestion is a function of stomachs, and that neither can be truly reproduced simply by crunching numbers in a computer. Computers, being the syntactic engines that they are, simply do not have the same causal powers of the brain, no matter how complicated the software is which is programmed into them.

permalinkembedsaveparentreport

[–]melty_brains 24 points 4 years ago 

If you push the argument far enough, at the end of the day what you will end up with is a restatement of the problem of other minds

And that's precisely the point, no? At the end of the day, all the Chinese room argument really does is expose Searle's particular bias with respect to the problem of other minds. You don't see it as a fault because you happen to share his bias. The argument lacks persuasive power.

He believes that information processing is a function of brains shaped by millennia of evolution in the same sense that digestion is a function of stomachs

I think one would be hard pressed to argue that information processing is not a function of computers.

In any case, this is the point:

If you reduced the entire room into the man’s head, and had him memorize the rules for manipulating Chinese symbols, it appears to me that that sort of understanding is different than the man’s understanding of English .

Absent prior bias, why should we believe that a set of rules sufficiently complex and comprehensive enough to encompass all "correct" interactions in Chinese is somehow different from understanding the language? Such a set of rules cannot simply be a mapping from query to response - it must also be able to adapt to the context of a conversation / interaction as it evolves. Since a conversation can be arbitrarily long, this ruleset would either (a) have to be arbitrarily large, in which case the premise that it could be fully written down or memorized is implausible, or (b) generative/compact enough to be able to represent and manipulate actual concepts, which I would argue is indistinguishable from understanding.

permalinkembedsaveparentreport

[–]Cerpin-Taxt 4 points 4 years ago* 

why should we believe that a set of rules sufficiently complex and comprehensive enough to encompass all "correct" interactions in Chinese is somehow different from understanding the language?

Knowing which symbols to respond to with which other symbols doesn't mean you know what the symbols stand for.

Replace the Chinese characters with a numeric cipher whose actual meaning remains unknown by the person who is responding to the inputs and it's easier to grasp the problem.

He may well know what he's supposed to write without knowing he was asked for a chocolate cake recipe or that he just responded with one.

As far as he knows it was just the correct string of numbers to respond with.

For a more modern example: Someone asks you a question in Chinese on the internet. You copy paste it into google without translating it and then copy paste the top result back to the chinese person. They are satified with the answer. You have no idea what they asked you. You could do this for any given question asked of you with the same quality of result and still not have and idea of what was being said. Does that mean you understand chinese perfectly because you can answer any question? No of course not. Now what if you memorized every question possible and every google result for it. You still don't know what they mean you just know which symbols go with which.

permalinkembedsaveparentreport

[–]Kovi34 1 point 4 years ago 

if you have a system that's sufficiently complex to answer any question, there's no reason why that shouldn't equal understanding too. You are only one part of the system that answers the question. Much like most parts of your brain cannot parse language at all, but if they were removed you'd probably lose the ability to speak coherently. Just because any one part of the system doesn't understand what it's doing doesn't mean the system as a whole doesn't.

Your understanding of the human language is different parts of your brain decoding the sound and pushing forward memories related to that word to formulate a response. If a computer system can do this, make inferences and truly answer every question coherently, there's no reason why that can't equal understanding. Because either the system is complex enough to encompass all the rules of the language, or it's a table of all possible questions and answers, both of which are enough to understand something.

permalinkembedsaveparentreport

[–]Coomb 1 point 4 years ago 

Because either the system is complex enough to encompass all the rules of the language, or it's a table of all possible questions and answers, both of which are enough to understand something.

You're basically saying this over and over, but the argument you use against Searle ("his argument is basically just a restatement of his personal intuition") is equally valid when applied to you.

To Searle, and to many people, there is a difference between following a rulebook (no matter how detailed that rulebook is) and the operation of a mind. There is a subjective difference from being the "CPU" of the Chinese Room and having a conversation in your native tongue, and that subjective difference is precisely what people mean when they talk about "understanding". When I am processing data in the Chinese Room, I, subjectively, feel different from when I am speaking English. In the Chinese room, I know the rules but when I am passed an input symbol, that input symbol has no referent. Passing me a Chinese sentence is like asking me to respond to the English sentence "Colorless green ideas sleep furiously" - I may know that when I read or hear that Chinese sentence, the rulebook says to respond with some other Chinese sentence, but the words, and as a result, the sentence itself, don't mean anything at all to me.

On the other hand, when you say to me "How is your mother these days?", I know what that sentence means. I know that "how is" is a question about welfare. I know that "your mother" refers to the woman who raised me. I know that "these days" means in the recent past. And when I respond to that question, I'm not blindly following a rulebook. I am actively considering the response. I am connecting the symbols I have been handed with the contents of my mind and producing a response that way - whereas in the Chinese Room, the symbols I have been handed never connect to anything in my mind. Therefore I don't need to have a mind to operate the Chinese Room - so being able to operate a Chinese Room successfully is not evidence that the Chinese Room contains a mind.

permalinkembedsaveparentreport
load more comments (3 replies)

[–]Cerpin-Taxt 1 point 4 years ago* 

The point is you must acknowledge the fact it's possible to respond in a language you do not understand using if/then statements.

It's possible to write the correct thing without knowing what it means, and this is obviously very different to understanding.

That's the entire point of the thought experiment. You're getting too hung up on the technical aspects of how complex the system would have to be. It's a fact that replying correctly and understanding are two different things.

People often know how to use a word correctly by context alone without actually knowing what the word means. You can't say that they understand what the word refers to just because they know when to say it.

permalinkembedsaveparentreport

[–]Kovi34 1 point 4 years ago 

The point is you much acknowledge the fact it's possible to respond in a language you do not understand using if/then statements.

It's possible to respond sure. But to respond according to other information you have, you need to have a ruleset sufficiently complex that it basically equals understanding.

For example, if you're asked whether or not you like something, for you to be able to answer correctly, you need a rule that tells you to grab another piece of information and then pick the response based on that. You cannot make a flowchart, because that flowchart would at some point involve creating new information, ie. getting an answer from elsewhere on whether or not you like the thing. I don't see how this is different from a human understanding a question.

That's the entire point of the thought experiment. You're getting too hung up on the technical aspects of how complex the system would have to be. It's a fact that replying correctly and understanding are two different things.

No, they aren't. Because a certain level of correctness necessitates understanding. For a clearer analogy, being given the correct answer to a division math problem isn't understanding. But being able to create the correct answer to any division math problem necessitates understanding of division. There isn't some deeper meaning to division that you can understand, it's just rules you learn and follow. Language works the same way. If you are able to construct new sentences and give subjective answers to questions then you must understand them by definition.

The issue with the experiment is that the person shuffling the symbols is just one part in the greater machine. The part of your brain that handles language processing or "shuffling symbols" doesn't understand the input or the output, it just grabs the relevant information based on the rules it's fed. The whole is capable of understanding language, one specific part isn't.

People often know how to use a word correctly by context alone without actually knowing what the word means. You can't say that they understand what the word refers to just because they know when to say it.

But they don't know when to say it, they make a guess based on other things they know about the language. If a machine is able to do this, then it must understand the language to make inferences and guess what a word means. No idea how you think this example supports your argument.

permalinkembedsaveparentreport

[–]Cerpin-Taxt 1 point 4 years ago* 

You're conflating being able to give perfectly coherent responses with giving perfectly true responses.

If the chinese box operator was asked what their favourite flavour was it could respond with something that could be true, but isn't necessarily true for the operator, with no new information needed. So yes it could be a flowchart. The goal is sufficiently deceptive mimicry, not actual consciousness.

True subjective information is a new requirement you've imposed on the experiment that isn't a part of the original. Answers to those questions could be pre prepared.

You're begging the question by presuming the system must be conscious because consciousness is required in order to appear conscious.

permalinkembedsaveparentreport
continue this thread

[–][deleted] 1 point 4 years ago 

I don't think the Google example accurately captures the situation. Google can answer a lot of questions, but it is far from conversational. There are plenty of questions you could ask that would it expose it a computer program. In some ways I feel the Chinese room argument reads like this: Assume there is a system that perfectly emulates understanding without understanding. Therefore emulation is not understanding. But can such a system exist?

permalinkembedsaveparentreport

[–]WishOneStitch 10 points 4 years ago 

why should we believe that a set of rules sufficiently complex and comprehensive enough to encompass all "correct" interactions in Chinese is somehow different from understanding the language?

It depends on where you place your "self" in the metaphorical argument Searle made. He says "you" are the CPU, simply processing instructions without being able to understand them; and it is in that lack of understanding that he find fault with the idea of strong AI. From his perspective, the simulation of the thing is not the thing itself , no matter how much it seems to be the thing.

permalinkembedsaveparentreport

[–][deleted] 21 points 4 years ago 

Then Searle's who argument is entirely absurd because then a human being is not a "strong AI" in that a human is a collection of subsystems that have no understanding of each other. A brain has no understanding of its the "wetware" it runs in a chinese persons brain no more than a "person CPU" in the chinese room does. Nobody argues that the hardware is what makes an AI. It ius obviously the software. The set of rules within your brain that govern how you react to external stimuli is who you are.

It honestly boggles my mind that the Chinese Room argument is taken seriously by some people.

permalinkembedsaveparentreport

[–]d20diceman 1 point 4 years ago 

It honestly boggles my mind that the Chinese Room argument is taken seriously by some people.

Yeah, I'm in the same place. I feel like there must be something I'm missing, because I can't imagine how a argument so obviously flawed would ever stick around. Especially when the author is alive - it's not like he'll be unaware of the criticisms.

permalinkembedsaveparentreport

[–]xthecharacter 1 point 4 years ago 

I think the Turing Test fails us here because all of the counterexamples I can think of to "rules following is the same as understanding" involve things other than written conversations.

Lets assume for the sake of argument that the person constructing the rule based responses really does have no idea what the other person is saying or how they are responding. So if the person says "I am going to bring you a piece of chocolate cake" the rule follower has no idea that they're going to be brought anything, that chocolate or cake are involved, or anything else. (This assumes the existence of a set of rules that actually have these properties -- namely, that they don't reveal anything about the message to a person who knows the language the rules are expressed in -- but let's set that aside for now.) Then, the rule follower issues a correct reply, and the first person goes off and brings back a chocolate cake and puts it in front of the rule follower without saying anything. Now if the rule follower were really understanding what was going on, they would expect this, but if they don't and are really just following rules, they would not be expecting it.

Maybe a question like "how do you feel about this" requiring the rule follower to comment on the unspoken details of the situation would extend this to the Turing Test, but I don't think it's something really intended to be covered by it. Either way, it doesn't seem like the Chinese Room Argument was intended to cover it either. In theory, the rule book could be extended to include "responses to situations" and I don't think this really counters your argument "that rule following is the same as understanding." We just need more rules if we want the understanding to come through in non conversational situations.

permalinkembedsaveparentreport

[–]bieker 2 points 4 years ago 

Agreed, I think this is a case of reductio absurdum, he has simplified the thought experiment so much that it makes no sense.

What if we replace the rulebook/database with a "black box". Questions come into his room in English, or Chinese. The English ones he answers, the Chinese ones he sends into the black box and it returns an answer which he in turn returns outside the room.

Can you say for certain that "no understanding" is happening in the box? What if that box contains a Chinese person? What if it contains another English speaker with a rulebook and a database?

Thats the whole point of the Turing test. You can't look behind the curtain because its purpose is to hid the potentially absurdly complicated mechanism that replaces a human mind, which could just be a pile of 100 billion neurons.

permalinkembedsaveparentreport
load more comments (3 replies)

[–]JeanVicquemare 1 point 4 years ago 

Well stated, and I agree. To me, the systems reply is persuasive, and his counterargument is not really an argument at all. Rather, I think we've reached a point where people will just disagree on basic premises. It is interesting to me, though, that Searle can be a devoted materialist while also denying that a system could "know Chinese," since the only difference between such a system and an individual person seems to be that the person has subjective experience of knowing Chinese. Mechanically, what distinction can be drawn?

It seems inconsistent to me that he would dismiss the system as not sufficient to give rise to conscious "knowing," but to a materialist what is the difference between bits of paper and neurons?

Also, I don't appreciate him suggesting that anyone who disagrees with him is under some ideology, while obviously he believes that he has no such ideology.

permalinkembedsaveparentreport

[–]MrYOLOMcSwagMeister 25 points 4 years ago 

His argument against the "Systems Reply" is just a resking of the philosophical zombie 'assumption'. When he says "All the same, he understands nothing of the Chinese, and a fortiori neither does the system, because there isn't anything in the system that isn't in him." he is making the implicit assumption that a system which can hold arbitrarily long conversations in Chinese without 'understanding' it, is possible. This is very similar to the assumption that a system indistinguishable from a human but without consciousness (a philosophical zombie) can exist.

I don't buy either of these assumptions. If a system passes all the tests we can devise to check if it has some property (understanding, consciousness) on what grounds can we then say it is lacking that property?

permalinkembedsaveparentreport

[–]sergeybok 3 points 4 years ago* 

I’m pretty sure searles Chinese room predates chalmers zombies so at least give him some points for originality. Also he isn’t really making an implicit assumption about being able to hold arbitrarily long conversations in Chinese. It’s an explicit assumption (“suppose I had a rule book that ...”).

And its similar to the zombies but that’s mainly because they are both trying to refute the computational theory of mind, and dealing with the hard problem of consciousness. You can throw the Mary argument into the mix as well for being similar, or Block's Blockhead machine. I think it would be extremely hard to prove the two arguments to be equivalent.

permalinkembedsaveparentreport

[–]MrYOLOMcSwagMeister 4 points 4 years ago 

He does predate Chalmer's zombies but Campbell and Kirk made the zombie argument 10 and 6 years before Searle invented the Chinese room!

The way I see it (a bit of layman's perspective admittedly) all these arguments use the assumption that a system can appear to have a property (consciousness/intelligence/understanding) without actually having the property. Granted, I'm definitely very biased because of my background but tend to find Newton's Flaming Laser Sword ( https://en.wikipedia.org/wiki/Mike_Alder#Newton's_flaming_laser_sword ) very useful to answer questions like this for myself and if we can not tell if we are speaking to a Chinese room or someone that knows Chinese, then I would argue we are speaking to a system that understands Chinese.

permalinkembedsaveparentreport

[–]Coomb 1 point 4 years ago 

The way I see it (a bit of layman's perspective admittedly) all these arguments use the assumption that a system can appear to have a property (consciousness/intelligence/understanding) without actually having the property

Of course things can appear to have a property without having it. A trompe l'oeil painting appears to have depth, but doesn't. Dreams appear to be real when you're inside them, but aren't.

if we can not tell if we are speaking to a Chinese room or someone that knows Chinese, then I would argue we are speaking to a system that understands Chinese.

This is just a restatement of the assertion that passing the Turing test is, ipso facto, consciousness.

permalinkembedsaveparentreport

[–]MrYOLOMcSwagMeister 1 point 4 years ago 

Yeah things can pass a single test for having a property without having it, but we're talking about a system that can pass every test we have.

permalinkembedsaveparentreport

[–]Coomb 1 point 4 years ago 

First of all, we're talking about a system that can pass the Turing test, not necessarily every test we have.

second of all, whether something is a mind or not, whether something is conscious or not, surely doesn't depend on whether we can figure out how to test it properly.

permalinkembedsaveparentreport
[+][deleted] 4 years ago* (19 children)


[–]toastjam 12 points 4 years ago 

I find it bizarre that Searle thinks this is a reasonable response. He just supposes a complete impossibility to resolve a flaw in his original problem. Let's move the intelligence inside your head and then we can say it's no longer intelligence! It doesn't make any sense.

Nevermind that no one could ever possibly even come close to memorizing all the infinite question/answer pairs in Chinese to begin with. You couldn't even write it down.

To me, all Searle refutes is that there's anything necessarily special about human intelligence. Your neurons are just following rules too, they don't understand anything by themselves.

And computers may operate symbolically at the lowest level, but that's really selling them short. Logic can get increasingly fuzzy as you go up in abstraction levels. Classifying an image for example will involve millions of non linear combinations of hundreds of thousands of pixels. To call that "symbolic" makes the word lose all meaning.

Basically I think his argument revolves around a bit of sleight of hand. He says, look at this book of chinese rules! Clearly not intelligent, right? But you wouldn't make a Chinese speaking AI with hand coded rules to begin with. It would ingest enormous amounts of real world data to get a fundamental understanding of the world (and probably simulate lots too), and then translation between inner "thoughts" and Chinese output would happen at a higher level.

By the time you're imagining a guy running around looking up codewords in books to output Turing passing Chinese, you've already accepted a flawed premise.

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

We can posit an arbitrarily fast guy with books larger than the observable universe without problem. There is no inherent logical flaw in the room setup, or regarding arbitrarily complex information systems as 'rote symbol manipulation'. The disconnect only comes in with the tacit assumption that consciousness is categorically unlike 'rote symbol manipulation'.

The problem boils down to:

Are there processes that are not information processes.

Is understanding an information process.

The room and putting the rules inside the head is just misdirection and invocation of emotional arguments (doesn't it make you uncomfortable to think about this practically absurd situation? I must be right then!).

permalinkembedsaveparentreport

[–]toastjam 1 point 4 years ago 

Yeah, I think you're right, when you distill it down that's basically the flaw in the argument. Sort of what I was trying to get at by saying he's not proving that humans aren't also Chinese rooms.

Still bothers me that he still seems to have no inkling how AI would actually work and frames it in terms of symbolic techniques that were largely discarded as intractable decades ago.

permalinkembedsaveparentreport

[–][deleted] 1 point 4 years ago 

Still bothers me that he still seems to have no inkling how AI would actually work and frames it in terms of symbolic techniques that were largely discarded as intractable decades ago.

I think this is simultaneously irrelevant to the flaws in the argument, and important to the viewpoint AI critics have.

Whether you're working in prolog, or a series of if statements, or building another abstraction on top with floats based more closely on biology. Other than the fact that FPUs/GPUs are sometimes non deterrministic (which isn't part of the model) a series of tensor operations is still symbol manipulation.

The simulationist/systems viewpoint is founded on ideas like isomorphism and that there is no reason to single out a specific implementation of an information system as special (including implementations which are unintentionally isomorphic), so it shouldn't really matter whether you're in the context of prolog or neural nets or biology simulation or some as yet unknown paradigm.

That said, people thinking in terms of a bunch of if statements or search have an overly simplistic view of what 'a computer program that can trick anyone into thinking it's a human over an arbitrarily long conversation' entails.

permalinkembedsaveparentreport

[–]etzel1200 9 points 4 years ago* 

A person that truly learns all of those rules could be said to effectively learn Chinese and English and translation. If you argue he doesn’t. Then he only doesn’t to the extent that he cannot encompass the system as the system is the operations themselves in the way we are the firing of our neurons vs. the physical neurons themselves.

A more complex version of my stomach with a brain would be capable of cognition. If it could keep itself alive, it’d already be smarter than worm.

permalinkembedsaveparentreport

[–]thizizdiz 2 points 4 years ago 

The problem is what it means to understand a language. Searle would argue that the person with the entire system in them would still only be working on the level of syntax, i.e., they get an input in the form of a bunch of symbols and follow some rules to produce an output of different symbols. They still have no concept of the meaning of the symbols and words they are producing. They have an understanding of the syntax but not a semantic understanding of the output.

permalinkembedsaveparentreport

[–]Kovi34 2 points 4 years ago 

to produce an answer to a question you need to know what the question is. There's no such thing as a rulebook that allows you to produce coherent chinese answers without also teaching you some chinese, because language is more than a set of rules. You can't make a flowchart for a conversation.

permalinkembedsaveparentreport

[–]thizizdiz 1 point 4 years ago 

But we already have forms of weak AI right now that can hold a conversation with you to an extent, and some of them are pretty compelling depending on how complex they are. It is definitely possible to make a sufficiently complex flowchart that a person would not recognize they are talking to a computer.

permalinkembedsaveparentreport

[–]Kovi34 2 points 4 years ago 

it's compelling if you only talk to them for about 5 minutes. It's pretty much just repeating back to you things other people have said to it. that doesn't mean it's capable of conversation.

permalinkembedsaveparentreport

[–]thizizdiz 1 point 4 years ago 

Yes, currently most forms of the technology are only that good. Do you imagine them remaining at that level 50 years from now? 100 years from now? I can imagine them getting much better. And yet they'll still be using the same procedures (what we can essentially reduce to a kind of flowchart) just at a level where they can process through thousands/millions of possible responses in seconds and produce a seemingly creative response.

permalinkembedsaveparentreport

[–]Kovi34 2 points 4 years ago 

that's not true. Today's conversational AIs use machine learning and are essentially black boxes where no one really knows why they produce the output they do. This approach is orders of magnitude more effective at getting the desired output than any kind of flowchart/table lookup stuff.

You can look at chess engines for an example. Traditional chess engines are still pretty close to a "flowchart" and you are right they are much better. But then alphazero came around and it absolutely dominates those engines. It's no longer about making efficient/clever algorithms that find the best move based on what we know, it literally taught itself how to play chess from scratch without ever being shown how to win.

If a conversation level AI ever comes around, it won't be a flowchart, but a black box that literally learned how to speak. I don't see how that's different from a human being learning to speak.

permalinkembedsaveparentreport

[–]thizizdiz 1 point 4 years ago 

I don't know much about machine learning but from what you say, it's an algorithm which allows the computer to improve its own programming via the data it receives over time. But again, at the bottom it is still a machine that manipulates symbols, moving around a bunch of 1's and 0's in extremely complicated ways. You say it will have essentially taught itself to speak, but I say it will just have taught itself how to conform more closely to human conversation patterns. It won't understand the meaning of the words it's saying, and if it tells a joke, it won't understand why it's funny. I will just know that that is a good response to what has been said. Do you think we can build a computer that operates on symbol manipulation (i.e., a Turing machine) and it can somehow, through machine learning algorithms we have programmed into it, learn to how to operate based on a system of genuine meaning and conscious thought?

permalinkembedsaveparentreport
continue this thread

[–]hackinthebochs 11 points 4 years ago 

The paper is worth reading, but Searle's response to the system's reply is insufficient to save the argument as it merely equivocates on what "the system" is. The system is the set of rules and the database of facts, some input/output mechanism, and some central processor to execute the rules against the symbols. The system's reply says that the man blindly executing instructions is not identical to the system that understands Chinese, namely the entire room. Thus properties of the man have no bearing on the properties of the system; that the man does not understand Chinese has no bearing on whether the system as a whole understands Chinese.

But the man is still not "the system" when he leaves the room. The system is some subset of the man, specifically only those brain networks required to carry out the processing of the symbols. It is true that the man doesn't speak Chinese, but since the system processing symbols is still not identical to the man, properties of the man do not bear on properties of the system. So still, the fact that the man blindly executing instructions doesn't speak Chinese does not entail that the system doesn't speak Chinese.

permalinkembedsaveparentreport

[–]Marchesk 8 points 4 years ago 

The real question is whether following rules is the same thing as understanding language.

permalinkembedsaveparentreport

[–][deleted] 1 point 4 years ago 

Yes, and the thought experiment and Searle's reasoning provides nothing on the matter other than a tautology followed by "but I'm arbitrarily disregarding the negation of my thesis, so I'm right".

permalinkembedsaveparentreport
load more comments (1 reply)

[–]THE__PREDDITER 4 points 4 years ago 

I don’t believe that the Turing test is really all that useful, for a number of reasons, but I think that Searle’s response here is pretty weak. For any program (or ledger of rules) complex enough to achieve Turing level responses in Chinese, as he describes here, memorizing that ledger of rules would ABSOLUTELY result in the human understanding Chinese. That’s exactly what knowing a language is. The whole argument falls apart.

permalinkembedsaveparentreport

[–]Muroid 9 points 4 years ago 

Yeah, that’s the point at which I really thought there was some grasping at straws going on.

“Imagine someone memorized all the rules for translating Chinese on the fly so perfectly they could pass for a native speaker, but without understanding Chinese. Then the person is the system but doesn’t speak Chinese, so the system doesn’t speak Chinese.”

Except if you know how to perfectly translate to and from Chinese on the fly, even if it’s literally just an improbable memorization of the correct translation of every possible sentence in every possible context... I don’t see how that is appreciably different from being able to understand and speak Chinese.

The argument winds up boiling down to “Imagine a system that can speak Chinese without being able to speak Chinese. Since it can speak Chinese without being able to speak Chinese, that means speaking Chinese isn’t evidence of being able to speak Chinese.”

permalinkembedsaveparentreport

[–]Anime_Alert 2 points 4 years ago 

There's no translating to and from Chinese. The premise of the Chinese room is that you give a question written in Chinese to the room and you get back an answer written in Chinese. At no point does the person running the Chinese room understand the meaning behind any symbol being manipulated. It would be like going to a Latin mass, following the call and response prayers, and saying, "wow, I know Latin, because I gave the exact right response in Latin at the exact right time". You could replace Chinese with random squiggles or numbers.

Where I disagree with Searle is that there is no system here that understands Chinese. I think that in both the physical room case and the in brain case, the conscious human doesn't know Chinese, but the part of the human plus the book of symbols plus the instructions together create a system that understands Chinese. It doesn't matter if you asked me the conscious human if I understood Chinese, because there's a separation between my conscious brain and the Chinese room that I'm interfacing with. It would be like going up to a sewage plant operator and saying "do you process sewage?" Well... No, they probably just press some buttons and take samples and measurements. But the sewage plant does process sewage. If somehow you could put the whole sewage plant into someone's brain, it would still process sewage even if the conscious part of the person's brain didn't process sewage. It's just convenient that both the Chinese room and the human brain are able to input and output pure information, so the metaphor isn't as clean.

permalinkembedsaveparentreport

[–]jokul 1 point 4 years ago 

You think memorizing the rules of the chinese room gives the subconscious brain knowledge of chinese? The subconscious has access to the same information as the conscious part of the brain, so why would it be able to know chinese but the conscious part would be unable to know chinese?

permalinkembedsaveparentreport

[–]Anime_Alert 2 points 4 years ago 

I don't think it's a question of conscious vs subconscious, because your subconscious brain doesn't know Chinese either. I think that, the way the Chinese room is presented, the knowledge is designed in such a way that it's completely opaque unless you interface it in a very specific way, i.e. by following the instruction book and looking through the symbol table, and that interface doesn't give you access to the knowledge of Chinese. Even if you had the entire Chinese room in your brain, if I asked you in Chinese what your favorite movie was, the answer you give wouldn't be YOUR favorite movie, it would be whatever answer was programmed into the instructions and symbols that you memorized. So I disagree with the argument that, just because the Chinese room is in my brain and I don't know Chinese, the Chinese room must not know Chinese, because I think it's fair to compartmentalize the Chinese room from the rest of my knowledge.

permalinkembedsaveparentreport

[–]jokul 1 point 4 years ago 

If you know everything there is that can be known about the Chinese Room, how can the Chinese Room "know" something that you don't know? Hell, you don't even need to have "memorized" the Chinese Room: you could have been the one to create all the rules yourself and just so happened to have been right.

permalinkembedsaveparentreport

[–]Coomb 2 points 4 years ago 

If you know everything there is that can be known about the Chinese Room, how can the Chinese Room "know" something that you don't know?

In the same way that I know "Критий принадлежал к высшей афинской аристократии и состоял в родстве с Солоном (по классической версии, был его внучатым племянником)." is a valid Russian sentence without having any idea what it means.

permalinkembedsaveparentreport

[–]jokul 1 point 4 years ago 

I dont see what the point of that example is, its effectively a restatement of the chinese room problem. The whole point of the chinese room is to show that understanding syntax is different than understanding semantics, your reply is an argument in favor of the intuition the chinese room is trying to pump.

permalinkembedsaveparentreport
continue this thread

[–][deleted] 3 points 4 years ago 

I disagree.

For argument's sake, lets say you can cram the gigabyte/terabyte of data into your head and do however many gigamips required to implement a strong ai by pure rote symbol manipulation.

At no point will the joke the AI is sharing with the letter writer cause you to chuckle, and if the letter writer teaches the AI all about their sewing machine collection, you still won't know a singer from a janome.

The joke is shared, and the sewing machine knowledge is gained, but that just means there is one of three possibilities:

There's more than one person in your head.
The AI is an elaborate ruse of smoke and mirrors without the mystical elan vital.
Functional human level intelligence is distinct from internal experience and conscioussness and, unless the AI is specifically programmed to lie, it will respond to musings about self awareness and personal experience with confusion.
permalinkembedsaveparentreport

[–]Kovi34 2 points 4 years ago 

I don't follow. How does cramming a black box into your head mean that the black box doesn't understand language?

permalinkembedsaveparentreport

[–][deleted] 2 points 4 years ago 

The black box does understand, but it's not you.

permalinkembedsaveparentreport

[–]Kovi34 2 points 4 years ago 

oh I see what you're getting at. But having a black box like that isn't equivalent to memorizing a set of rules, but having a person who knows that set of rules inserted inside of you, which I feel is an important distinction.

If you have something inside of your brain that can translate english to chinese perfectly as soon as you think you want it translated, then that part of your body understands both english and chinese, but it's entirely separate from your person (even if it's physically inside of your skull)

permalinkembedsaveparentreport
load more comments (6 replies)

[–]whentheworldquiets 1 point 4 years ago* 

If he doesn't understand, then there is no way the system could understand because the system is just a part of him.

Sorry, but he's just making the same error again: he's saying that because there's what we call a mind involved, and THAT mind doesn't understand, then NO understanding is occurring. But there is: the mind that is the emergent property of the simulation he is running in his head DOES understand. That's the system. HIS mind is just standing in for the 'laws of physics' on which that mind operates. Do the laws of physics understand english or chinese? No. Yet they manage to support systems that do.

As for his claim that systems understanding is 'absurd' because it would mean our other organs also 'understand' things - that's begging the question. What's wrong with saying a stomach has a simple kind of 'understanding', or that a seed is packed with the understanding of water and soil and sunshine? I think what's tricking him into believing such ideas are absurd is that stomachs and seeds do not possess the same power of iterative self modification as the brain, a power which allows a near infinite static rule set to be fractally compressed.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

If you're still skeptical, consider the following modified thought experiment:

If the Chinese Room can flawlessly pass the Turing test, it must be able to demonstrate learning and recollection. In particular, should the person talking to the chinese room - let's call him Tolkein - decide to test it by inventing a new language and teaching it to the chinese room, that would have to work.

More importantly, if Tolkein were to ask the room to invent a language and teach it to him , that would also have to work.

In the moments before the room complies, it is the only thing that 'speaks' this new language. If communicating that to Tolkein gives him 'understanding', and him communicating that onward gives others understanding - what else should we call what is in the box just prior but 'understanding'?

permalinkembedsaveparentreport

[–]Coomb 1 point 4 years ago 

If communicating that to Tolkein gives him 'understanding', and him communicating that onward gives others understanding - what else should we call what is in the box just prior but 'understanding'?

Did the apple tree apple Earth system understand Newtonian gravitation because seeing an apple fall inspired Newton to invent it?

If your answer to that question is "yes", then by "understanding" you mean something completely different from the conventional meaning.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

Suppose I proposed that 'understanding' means 'successfully generalising'.

Understanding gravity means knowing that the apple fell because of its mass and not because it was green or because it was Sunday. If you check on Monday and red apples don't fall - whoops; you didn't ACTUALLY understand at all.

In my 'teaching Tolkein' experiment, the box contains a generalisation of the language it is about to teach. It 'knows the rules'. When it has communicated them to Tolkein, he also 'knows the rules'. Why is one successful generalisation understanding, and the other not?

permalinkembedsaveparentreport

[–][deleted] 1 point 4 years ago 

That's not a good response, and is barely even a response.

For the first paragraph, the system is then just a subset of the individual's brain (which is somehow just as competent at reasoning and learning, and making jokes, and writing poetry and doing terrible philosophy as the individual in spite of a) running for a small fraction of the time, and b) being implemented in an abstract and vastly inefficient emulator -- but this is an implementation detail we can allow to pass).

Then in the second paragraph we see the the man behind the curtain:

Furthermore, the systems reply would appear to lead to consequences that are independently absurd.. then it looks like all sorts of noncognitive subsystems are going to turn out to be cognitive.

This is "If brains aren't special and different by virtue of being special and different then they're not special and different.

The correct conclusion is that one of the following is true:

It's impossible to build the room

It's possible to build the room but it's fundamentally different from a brain.

Information systems can understand (either by virtue of panpsychism or some fundamental threshold of some kind such as levels of feedback).

Which is exactly where we started, so the entire argument did nothing.

permalinkembedsaveparentreport

[–]krulp 6 points 4 years ago* 

I actually think it wasn't a bad explanation at all. And I think more examples could have strengthened it. Like if you're in this system and someone asks you a question you find really immoral. In either language, if you follow the system, you will give the same answer. However if it's in syntax you understand, you can also think to yourself, "Wow, that question was fucked up."

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

That doesn't strengthen the argument. You're assuming the room as a whole didn't find the chinese version of the question 'immoral'; that morality can't be captured by the combination of Searle, instruction book and symbols. But that's in direct contradiction to the premise of the thought experiment: that this is a machine that can pass the Turing test.

permalinkembedsaveparentreport

[–]Coomb 1 point 4 years ago 

But that's in direct contradiction to the premise of the thought experiment: that this is a machine that can pass the Turing test.

No it isn't. If I write a bot that responds to the sentence "we should kill all the " with the sentence "wow, that's a fucked up thing to say", there's no requirement that the bot understand morality. But the actual mind that wrote the rulebook would need to understand morality.

permalinkembedsaveparentreport

[–]whentheworldquiets 2 points 4 years ago 

But that machine wouldn't pass the Turing test. Remember: we're not talking about a machine that COULD pass the Turing test if you happen to ask it the right questions, but one that would invariably convince. Suppose I said to the bot "We should kill all the people who would otherwise immediately and unstoppably commit mass unprovoked murder", surely the bot ought to respond differently? In which case, isn't the ability to categorise and discriminate 'understanding'?

permalinkembedsaveparentreport

[–]Coomb 1 point 4 years ago 

But that machine wouldn't pass the Turing test.

Not a machine that literally only responded to one sentence. But a machine with a more extensive corpus could. The Turing test doesn't mean you have to get all the responses exactly "right", whatever that means, as people routinely make mistakes in responding "correctly" to things others say.

Remember: we're not talking about a machine that COULD pass the Turing test if you happen to ask it the right questions, but one that would invariably convince. Suppose I said to the bot "We should kill all the people who would otherwise immediately and unstoppably commit mass unprovoked murder", surely the bot ought to respond differently? In which case, isn't the ability to categorise and discriminate 'understanding'?

Of course it isn't. A sieve or a series of sieves categorizes and discriminates between rocks based on size. Does a sieve understand what it's doing?

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

Does a sieve understand what it's doing?

I've got two answers, and either works.

One answer is to say 'yes' - the most basic kind of understanding: that of the laws of physics. The system knows just enough to hold together and perform what we would call mechanical filtering.

Higher levels of discrimination are arrived at when inputs are passed through abstraction layers on the way to generating a response. Even higher levels are achieved when a system is able to feed back consequences and modify its responses and abstraction layers - and so on. So what we might deem the 'degree' of a system's understanding would be calculated in some fashion based on those measurements. It might be an analogue scale, or digital, or a space rather than a line - the upshot is that understanding is not binary.

The other answer is 'no', where everything above stays the same but we count purely unreflected, unabstracted mechanical response as 'zero' understanding.

permalinkembedsaveparentreport

[–]krulp 1 point 4 years ago 

It's not about the response. Sorry if I wasnt clear there, in this chinese box example, both the unknown and understood syntax deliver the same response, regardless of the input. You are a good human and follow your commands. But I wont have personal opinions about syntax I dont understand. I'm not going to talk to my friends about all the fucked up questions I received that I didn't understand. There maybe a day where we reach sentient computers which can truely understand and contextualize, but computers will pass the Turing Test long before that happens.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

But I wont have personal opinions about syntax I dont understand.

You won't, but the combination of you and the box could. You just wouldn't know it.

That's the fundamental flaw of the thought experiment that I'm trying to highlight: Searle puts himself in the box and says "Well, *I* don't understand, and *I* have no opinions, therefore no understanding is happening and there are no opinions. But he's not the whole of the system.

As I've encouraged others: consider the Chinese Neuron experiment. We run wires into Searle's head and I stand in for one of his neurons, sitting in a dark room pressing a button whenever the conditions are met. I am now part of Searle's brain.

Clearly I'm going to have no opinion about the impulses that pass through my little room. I'm not going to talk to friends about them. Does that mean the system does not hold opinions?

Now replace a half, a quarter, nine tenths of the neurons in Searle's brain with people sat in dark rooms pushing buttons. Replace all of them. At what point in that neuron by neuron process did the system cease to have opinions, cease to be aware? Did it notice?

In other words, are we claiming that a mind capable of true awareness can only emerge/glom onto neurons - that specific combination of protons, neutrons and electrons - and no other? Because if so, it's a huge coincidence that nature evolved brains in the one unique way that would support awareness. A proton or two either way and we would just be two unaware bags of meat having this conversation :)

permalinkembedsaveparentreport

[–]etzel1200 6 points 4 years ago 

This exactly is the refutation. The atoms that comprise me don’t understand English either. However the way the whole system interacts does. In the same way that system essentially ends up understanding English and Chinese in a way he doesn’t.

permalinkembedsaveparentreport
load more comments (12 replies)

[–]fozziethebeat 5 points 4 years ago 

I agree completely.

It's incredibly obvious the CPU within a computer doesn't understanding anything. It's a very straight forward rule following system. The box however does understand Chinese. It's unclear exactly which part of the box understands Chinese, perhaps no single part in isolation does. But as a whole, the box is able to robustly respond to chinese questions like a native speaker.

Asserting that the CPU or any sub component of the box must _understand_ Chinese just avoids the attempt to put a clear definition on understanding.

permalinkembedsaveparentreport

[–]ackermann 1 point 4 years ago 

It's unclear exactly which part of the box understands Chinese

Bingo. This is the interesting part of the Chinese room for me. If you created a conscious simulation of a human brain, with emotions, where does that conscious “mind” reside?

Is it in the program/instructions? But that could just be a shelf of books. A shelf of books being conscious is absurd!

So maybe it’s in the device that “blindly” follows the instructions (it doesn’t speak Chinese). But it has no idea what it’s doing! Particularly in the case of a Turing machine, which is very simple, but theoretically can do anything a modern computer can.

So it must be in the combination of the two? But that raises interesting questions. Does this mind “wake up” when you start the computer? Does speed matter? What if it takes thousands of years to follow a page of instructions in the program?

Or maybe it’s impossible for a simulated brain to be truly “conscious” and really have emotions? Maybe it can only imitate that? Programmed to answer “yes” when asked if it’s awake, but just blindly following that command? (P Zombie)

permalinkembedsaveparentreport

[–]wildbeast99 6 points 4 years ago 

Consciousness is a concept that merely describes a certain set emergent properties, in this sense it isn't "real". I mean the properties of the whole of the brain are no doubt greater then its parts, and thus the mind is a complex adaptive system which shows certain peculiar characteristics, but there no real essential "thing" that corresponds to consciousness— the great mistake of philosophy was to essentialize consciousness as some substance that can be found.

permalinkembedsaveparentreport

[–]Moocry 18 points 4 years ago* 

Not a single thing you said refutes his claims. So many people eager to dismiss him, and yet, I don't think they understand what he's actually saying.

There is no such thing as Strong AI in relation to a human mind, and his proposal is proof of precisely that. You're (most here) avoiding discussing the heart of this issue because there isn't an argument that exists that demonstrates how Strong AI is in any shape or form similar to the biological functions of a human mind. You saying his *knowing* is irrelevant because the machine functions irregardless, isn't an argument for Strong AI being a mind, or mindful of the assets it's arranging in whatever order.

The single defining element of the mind is consciousness, and if you aren't comprehending, or aware of the sentience you're partaking in, it's not a functional mind, or even something someone would use to demonstrate what a mind is.

EDIT2: A clock can correctly tell you the time, does that mean the clock is mindful of what time it is? Absolutely not.

Philosophically, I think you completely missed his point, and even went as far to strengthen (unbeknownst to you) his strongest points: the mind isn't a cog in a greater machine, the mind is the sentience that drives the machine absolutely.

EDIT: Even reading the comments below, it's almost like none of you even had the slightest idea of what he was discussing, and why he was discussing it.

permalinkembedsaveparentreport

[–][deleted] 7 points 4 years ago 

Not a single thing you said refutes his claims.

This criticism applies to your comment.

The systems argument is exactly that the running program is the thing that has subjective experience, not the hardware alone, whether that hardware is virtualised within a different system, and that system is running another consciousness is irrelevant.

You're presupposing that there cannot be more than one consciousness in the room, so of course you conclude that there is not more than one consciousness in the room.

You're also presupposing that understanding happens iff there's internal experience, but that's more of a semantic quibble (an understanding, internal experiencing thing is as good a definition of consciousness as any).

Philosophically, I think you completely missed his point, and even went as far to strengthen (unbeknownst to you) his strongest points: the mind isn't a cog in a greater machine, the mind is the sentience that drives the machine absolutely.

This criticism applies verbatim to your comment.

Additionally I think you're coming from a point of view that completely ignores the idea that information is a real thing, and that (running or non running) programs are a valid ontological category at all.

permalinkembedsaveparentreport
load more comments (9 replies)

[–]bitter_cynical_angry 6 points 4 years ago 

EDIT: Even reading the comments below, it's almost like none of you even had the slightest idea of what he was discussing, and why he was discussing it.

Count me as one of the clueless people then, because if he has a valid argument, I'm certainly not seeing it. I didn't understand from your post what it's supposed to be either though.

permalinkembedsaveparentreport

[–]ackermann 4 points 4 years ago 

I don’t think his explanation is great. Can I try?

Suppose you created a computer simulation of a human brain, with emotions. Where does that conscious “mind” reside?

Is it in the program/instructions? But that could just be a shelf of books. A shelf of books being conscious is absurd!

So maybe it’s in the device that “blindly” follows the instructions (it doesn’t speak Chinese). But it has no idea what it’s doing! It’s just blindly following instructions with no understanding of the big picture! Given different instructions, it might just run Microsoft Word.

So it must live in the combination of the two? But that raises interesting questions. Does this mind “wake up” when you start the computer? Does speed matter? What if it takes thousands of years to follow a page of instructions in the program?

Or maybe it’s impossible for a simulated brain to be truly “conscious” and really have emotions? Maybe it can only imitate that? Programmed to answer “yes” when asked if it’s awake, but just blindly following that command? (P Zombie). Searle, in the video, believes the arguments above “prove” this.

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

A shelf of books being conscious is absurd!

I think this appeal to emotion is also a leg that Searle is standing on, which the people here take issue with.

If you don't accept that sentence, the rest of the argument is a non sequitur :(

permalinkembedsaveparentreport

[–]bitter_cynical_angry 3 points 4 years ago 

A shelf of books being conscious is absurd!

If it's just sitting there, then yes, in the same way that a human brain with absolutely no physical/electrical/chemical interactions happening in it would also not be conscious.

So maybe it’s in the device that “blindly” follows the instructions (it doesn’t speak Chinese).

Just a side note that in the brain, and AFAIK in neural networks generally, there's not really a definite distinction between the instructions and the thing that follows the instructions. It all kind of happens together in every neuron.

So it must live in the combination of the two? But that raises interesting questions.

Indeed it does, but the answers follow readily from the assumptions:

Does this mind “wake up” when you start the computer?

Yes.

Does speed matter? What if it takes thousands of years to follow a page of instructions in the program?

In principle, a brain being run very very slowly would still be conscious, but it wouldn't act the same as a brain being run at regular speed. In particular, its reactions to stimuli would be much slower, and would be much too delayed to be able to, e.g. interact reliably with the physical world. If both the computer and its simulated inputs were slowed down to the same degree though, then there should be no difference in the consciousness, because as far as we know, as long as all the same interactions are happening in the same order, it doesn't matter how long they take.

Or maybe it’s impossible for a simulated brain to be truly “conscious” and really have emotions? Maybe it can only imitate that?

Maybe, but then maybe that's just what your brain is doing. If I asked you if you "really" have emotions or you're just simulating them, you'd probably say you're really having them. But that's also exactly what a p zombie would say, so why should I believe you but not a p zombie?

permalinkembedsaveparentreport

[–]coyotesage 1 point 4 years ago 

Seems perfectly natural that consciousness may "exist" only as a concept, as a byproduct of other interactions. We used to think of gravity as a force, but it's a byproduct of physical objects bending space. Magnetism is just the byproduct of the motions of electric charges. No one can prove that that is all it is, yet, but I feel like it's more reasonable to think of consciousness and qualia as being similar in nature to gravity or magnetism.

permalinkembedsaveparentreport

[–]Moocry 6 points 4 years ago 

A mind is aware of what it is partaking in, Strong AI requires no awareness.

permalinkembedsaveparentreport

[–][deleted] 9 points 4 years ago 

Strong AI requires awareness. The "set of rules for arranging the chinese characters" has awareness baked into it in Searle's example, he merely showed that the hardware executing the software doesn't need to understand the software, which is self evident in programming.

permalinkembedsaveparentreport

[–]enternationalist 4 points 4 years ago 

Precisely.

Say you have a book that perfectly describes the structure and physical behaviour of a human brain. Say someone gives you a question. You calculate the vibrations this sound would make, the vibrations transmitted to the inner ear, the stimulation of nerve cells by cilia, the cascade of neural inputs leading to a verbal output. You hand this output back.

The operator doesn't learn about the language - and more importantly, even if they do, it bears no relation to the operations they are performing. They're just doing physics, and those physics happen to begin and end in Chinese.

What's doing the learning? The program is. If it sufficiently models the mind, our operator will be procedurally creating new parts to the program based on the inputs (e.g. new neurons, connections). To the operator, they are simply adding extra physical elements, and no new understanding of Chinese is created for them. It is the program itself that has encoded the understanding.

permalinkembedsaveparentreport

[–][deleted] 2 points 4 years ago 

At what point did you disprove the hypothesis that all information processing systems have some (usually unmeasurable) degree of consciousness?

Where did you prove that fully passing the turing test (over the course of years of letters including back references and in jokes) requires no awareness?

permalinkembedsaveparentreport
load more comments (18 replies)

[–]PilGrumm 4 points 4 years ago 

pearls before swine... they don't understand because they don't want to believe it

permalinkembedsaveparentreport

[–]Marchesk 2 points 4 years ago 

That's because they are approaching language as if programming a computer, or doing logic. They're not paying attention to how they as human beings actually use language.

permalinkembedsaveparentreport

[–][deleted] 6 points 4 years ago 

I'd argue equally that you're regarding systems in the same innacurate way you regard logic.

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago* 

There are only two parts to this system though. Instruction set and executor, with the executor obviously having zero awareness of self, and the instruction set being static and unchanging.

Are you implying that those two parts are all that are needed to make up a conscious entity that's equivalent to a human brain? I would have to disagree.

Google translate is an actual example of the Chinese room. Words in, instruction set executed, words out. I don't think you're about to tell us that Google translate understands language in the same way that a human translator does though.

Or are you saying that a more complex instruction set is the differentiator here? AI can only ever have those two subsystems, executor and instruction set. A more complex instruction set is the thing that confuses people into thinking it's equivalent to a human mind. It's not though, it's still just those two parts.

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

Google translate is the Chinese room in the same way a flatworm (which we have the complete connectome of and can simulate accurately) is an example of the human brain (or possibly even something simpler, like a pocket watch). Why do people keep disregarding the whole "can trick someone who writes it letters into thinking it's a person" bit (which entails learning new skills, ability to critically analyse literature, asking questions about the letter writer's children, etc. etc. etc. as well as gigabytes or terabytes of state).

The simulationist/systems position is precisely "there is insufficient evidence to discount the possibility that running software can be a mind". Your, Searle's and every other reply I've seen only amounts to one of "nuh huh" or "but rooms being minds makes me uncomfortable" -- often followed up by telling people they don't understand the philosophy. In the case that the instruction set is based on the same principles of the human brain, and it has the same results as the human brain (ie. a human like intelligence that is prone to believing really bad philosophical arguments for some strange reason, and asserts its own internal experience), then discounting that it has internal experience is the absurd position.

Possible follow ups on the "what if the room is a mind" train of thought include variations of panpsychism and there being some special properties of certain information (and possibly feedback loops) that results in self awareness.

There's also the important distinction that this is only one of the logical options. Others include "it's (in practical terms) impossible to build the room", "the room doesn't understand" and "the room may or may not understand depending on how you build it".

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago 

I like how instead of logically replying to the actual content in my comment, you decided to take the opportunity to talk about all the other comments in this post in a "Everyone in here is so dumb except me" manner.

​

Google translate is the Chinese room in the same way a flatworm is an example of the human brain

Yes, that was my entire point. The point of my comparison is that both systems are simply instruction set and executor. Google translate is just a chinese room with a simpler set of instructions. If you switched out the instruction set for something more complex, the room may be able to convince you that it's a person, but does that different instruction set establish a level of consciousness where there previously wasn't any?

Stemming from that, I'll ask the question in my previous comment again. Are instruction set and executor all that's needed to make a conscious entity?

If so, is the presence of consciousness determined by the complexity of the instruction set, or is it guaranteed simply by the existence of both instruction set and executor?

And if consciousness is determined by the complexity of the instruction set, is it a binary on/off of whether the entity is conscious, or is it a sliding scale? Example: A coffee maker has extremely low awareness of self, and an advanced language AI has a high level of self.

permalinkembedsaveparentreport

[–][deleted] 2 points 4 years ago 

Are instruction set and executor all that's needed to make a conscious entity?

Possibly -- which is where you start before the Chinese room.

If so, is the presence of consciousness determined by the complexity of the instruction set, or is it guaranteed simply by the existence of both instruction set and executor?

Possibly, also you have to define consciousness (ie. is it binary, or is it granular). Which again is where we started.

And if consciousness is determined by the complexity of the instruction set, is it a binary on/off of whether the entity is conscious, or is it a sliding scale? Example: A coffee maker has extremely low awareness of self, and an advanced language AI has a high level of self.

That is one of the questions that naturally follows from assuming that the Chinese room can (in principle) be constructed. It is a really interesting question, with lots of actually good literature on the subject. My personal leanings are that there is some kind of atomic level of self reference on which the sliding scale is built which rhymes heavily with Godel's incompleteness theorem, and the answer to the question "why do I only experience one partition of the wavefunction when there is an interaction and not many", but I do not disregard the possibility that all information systems are conscious.

When you say:

with the executor obviously having zero awareness of self

You are immediately dismissing and throwing out all of the interesting parts of the thought experiment. It's not at all obvious, and examining why it isn't obvious and what it being true or false entails is the only valuable thing to come out of it (that and a single throwaway line from the original paper about fire, but I digress).

Additionally this is synonymous with simply stating "Information systems cannot be conscious". So you may as well have not even done the thought experiment to begin with. It is exactly the same circular argument as John Searle makes over and over again in everything of his that I've read (helpfully you can just search for the phrase "pop up" which is a fairly reliable indicator that he's just done it).

Edit: Additionally you have to consider that the executor contains state or information. The physical manifestation of that information is fungible, but its existence can't just be handwaved away.

permalinkembedsaveparentreport

[–]d20diceman 1 point 4 years ago 

I'm not sure that the instruction set could be static, if it's going to make passable conversation, and can answer queries like "No, tell me a different joke, something raunchier than that!" to the satisfaction of the asker. It needs recall and context and other fiddly things.

Having an instruction set which, as part of its execution, reaches into itself and alters the instruction set... That starts to sound not too many steps removed from conciousness.

permalinkembedsaveparentreport

[–]d20diceman 1 point 4 years ago 

Even reading the comments below, it's almost like none of you even had the slightest idea of what he was discussing, and why he was discussing it.

I've felt like this since I was introduced to Searle. It seems like such an obviously terrible argument, I don't understand how it has living proponents. I feel like I must be missing something, but haven't been able to find what it is.

Further investigation of it tends to make it sound like Searle thinks brain matter is magical, and that it wouldn't matter if exactly equivalent processes occurred outside a brain - they still couldn't be conscious, because they aren't made of brain.

If he started saying "But nothing in this room has a Soul" and made that the crux of his argument, I don't think the argument would be any weaker for it.

permalinkembedsaveparentreport
load more comments (13 replies)

[–]jag149 2 points 4 years ago 

Yeah, I don’t have an opinion on whether AI is possible (other than that it’s probably a semantic distinction), and I don’t know enough about Searle to know if he was explaining this argument or supporting it, but it seems to me that the argument requires two alternative fallacies.

First, to your point, there’s an infinite regress inside the box. There’s still a mind doing whatever we think “intelligence” is.

To avoid this, he contrasts processing a database with something called “meaning”. He’s using a transcendental signifier to escape a poststructuralist explanation of AI, and that’s the very premise that poststructuralism destabilizes.

This also invites sentences like “I understand the meaning of the English language”, which are great resume builders, but are philosophically useless.

Interesting thought experiment though.

permalinkembedsaveparentreport

[–]AlphaOhmega 2 points 4 years ago 

This is exactly it. The mind isn't one piece running something, it's a large amount of pieces that communicate to create something more than the individual pieces. The part of my brain that handles beating my heart, doesn't know how to speak words, my amygdala on its own doesn't understand pictures. But if you create an AI that takes the symbols and performs the action, then create another program to recognize the patterns with the actions you start getting a machine that learns what the symbols represent. That is exactly how the brain works, and it's how computers can translate pictures into objects. Our brain isn't magic. It's just extremely complex, but I haven't heard a good argument to claim that we can't recreate it in some form or another. (We actually already do it quite a lot).

permalinkembedsaveparentreport

[–]Tabletop_Sam 3 points 4 years ago 

I feel the same way. He's assuming that the person is what's supposed to be understanding Chinese, but in reality it's the room that understands it. My ear doesn't understand English, but when it goes through the "programming" of my brain, and formulates a "response", whether it be internal or external, it's still going through the same steps as the Chinese Room.

permalinkembedsaveparentreport
load more comments (13 replies)

[–]ShutUpAndSmokeMyWeed 2 points 4 years ago 

I think you hit the nail on the head. His framing of the Chinese room seems rather arbitrary. Why not let him be the rulebook? Or the symbols? Or implant a chip in his brain that tells him how to speak Chinese? Etc. There are endless setups where he plays a larger or smaller role in "understanding", but I would say this entire class of arguments by analogy is pretty weak.

permalinkembedsaveparentreport

[–]LuxDeorum 2 points 4 years ago 

The argument also breaks down in the exact opposite direction I think. He argues that AI are not "minds" because the way he perceives his own mind operating is characteristically different from the way that he understands AI to operate, but this assumes "meaning" in human thinking is an essential characteristic of the thinking, and not of the thinking about thinking.

permalinkembedsaveparentreport

[–]SL0THM0NST3R 1 point 4 years ago 

He also ignores the fact that he would actually be learning Chinese by his actions. Eventually he would no longer need the rule book.

permalinkembedsaveparentreport

[–]Marchesk 1 point 4 years ago 

> He isn't doing any understanding, but the combination of him, the rulebook, and the symbols are doing the understanding.

I don't think the entire system understands anything other than how to follow rules. Understanding language isn't symbol manipulation. Rather, it comes from being embodied, growing, learning, experiencing, sharing with others. The Chinese box isn't doing that. It just has a bunch of provided rules to follow.

You can invoke Wittgenstein that language is use, but use refers to doing things in the world with language, not being a box that translates symbols. Or you can invoke correspondence, but the rules aren't grounded by referring to an external world via the experience of language users, not a lookup table.

However, I don't think Chinese room would be able to perfectly translate English because understanding (semantics) is required for that. And language is fluid and changes. So those rules won't keep working as new words and slang and idiosyncratic uses and new metaphors come into play. Even a group of friends just chatting can come up with their own meanings inside the language game.

permalinkembedsaveparentreport

[–]whentheworldquiets 2 points 4 years ago 

Even a group of friends just chatting can come up with their own meanings inside the language game.

A process that could be captured either by a (much) larger ruleset (listing all possible symbolic combinations and what to do if they are accompanied by all other series of symbolic combinations, encapsulating the consequences of context), or more compactly via self modification (If you see this , write that new rule in the book), which can be compacted further via regular expressions (If you see a sentence of the form ABC and you don't recognise B, write this new rule in the book inserting B as appropriate)

permalinkembedsaveparentreport

[–]SpruceMooseGoose24 1 point 4 years ago 

Minor nitpick with your analogy. But in a way it makes your point stronger, not weaker.

But nobody is claiming that the CPU running an AI understands what it is doing, any more than anyone claims the molecules within our synapses know what they're doing.

For an AI, the equivalent of molecules in our synapsis, isn’t the CPU. It’s the molecules within transistors. Just like with human conscience, you’d be hard pressed to find someone that says the molecules or even transistors ‘understand’ what the AI is thinking.

If your view of human conscience and understanding is based on a material world, where all thoughts are processes based on material relations, then a human brain and a CPU are both machines and equivalent.

However, if your assumptions include that ‘understanding involves more than just material relations and processes’, then you’d be inclined to agree with Searle.

So what are your thoughts on thought itself? Is it a neurological process or is there more to it?

permalinkembedsaveparentreport

[–]whentheworldquiets 2 points 4 years ago 

a human brain and a CPU are both machines and equivalent.

Not quite. In a comparison between AI and the human brain, the CPU is analogous to 'the laws of physics' not the brain itself. The brain is the self modifying simulation the CPU is processing.

permalinkembedsaveparentreport

[–]SpruceMooseGoose24 1 point 4 years ago 

I took CPUs to mean central processing units, not contemporary CPUs that are sold by Intel or AMD, etc.

I’m terms of CPUs, you seem to be thinking of application specific ICs like the current generation of CPUs and GPUs on the market. Programmable gate arrays are becoming more common by the day and are much better suited to AI applications.

By a ‘self modifying’ criteria, these programmable gate arrays are vastly more self modifying than human brains are. So much so in fact that human brains are rigid comparatively.

Please let me know if I’ve misunderstood your comment in any way.

permalinkembedsaveparentreport

[–]whentheworldquiets 2 points 4 years ago 

No, not at all :) I was referring (as I believe Searle was) to classical fixed function CPUs, which would need to simulate the system's self modifying properties in software.

permalinkembedsaveparentreport

[–]134_and_counting 1 point 4 years ago 

What if the difference he implies between programmed responses and actual understanding comes down to degrees of complexity and determinism?

You said that “the combination of him, the rulebook, and the symbols are doing the understanding” but let’s say the question is “how are you feeling today?”

The rulebook’s algorithms would direct him to give a set of characters back given some set of conditions (did he have lunch today, is it cold in the room, etc.) If the same set of conditions is repeated at some point in the future the Chinese answer would be the same. But the English answer might not be, it would depend of the emergent quality we call “mood” whose origin and complexity exist at the quantum level. So under the exact same set of circumstances, the Chinese answer would be deterministic and the English answer (based on actual conscious understanding) wouldn’t be.

The Chinese rule book would have to be infinitely complex, to contain infinite flowcharts directing him to take into account every possible permutation of things that happened to him in his lifetime that may possibly affect his current answer in order to really approximate understanding.

permalinkembedsaveparentreport

[–]whentheworldquiets 2 points 4 years ago 

The Chinese rule book would have to be infinitely complex,

Or self modifying. As with (eg) the Mandlebrot Set, finite rules for self modification can give rise to endlessly unpredictable behaviour. I say 'unpredictable' not because it's non deterministic, but because there are no shortcuts to finding out what 'would happen if...'

permalinkembedsaveparentreport

[–]obenj 1 point 4 years ago 

I think you’re misunderstanding the idea behind the Chinese room. Keep in mind I haven’t seen this video but I did learn about this in a cognitive science course. Encapsulating the entire function of translation is not the same as consciously understanding it. My professor showed us this in anticipation of a debate regarding AI ethics, and I interpreted it as a message against anthropomorphizing the mechanics of AI. I know this seems counterintuitive because a human is in the room, but it still is a good example of how our external view of an entity or object can be deceptive about internal mechanism. I know you say that nobody is claiming that AI understands the processes but historically this wasn’t always the case, and soon enough once AI is convincing enough to pass for real humans, there are going to be debates about this.

permalinkembedsaveparentreport

[–]whentheworldquiets 3 points 4 years ago* 

Not sure what you mean by 'function of translation'.

Part of the premise of the Chinese Room experiment is that it flawlessly passes the Turing test. That means I can tell it my name, and later it will recall it. I can teach it a new language. I can ask it to teach ME a new language that it has invented. Anything a person can do, the Chinese Room is presupposed to be capable of simulating.

The Chinese Room experiment tries to trick you into thinking it's a disproof of Strong AI by cheating in two ways. First, as I mentioned in my earlier response, it puts a 'known mind' in the room and demonstrates that the 'known mind' does not understand what is happening. This is misdirection; a philosophical parlour trick. It does not exclude the possibility that the 'known mind' now forms merely a part of a 'new mind' that does understand what is going on.

The second trick is to present the mechanism of thought in a manner that is technically Turing equivalent, but intuitively unfamiliar. I'd be willing to put money on you not having imagined this Chinese Room could teach a Chinese person a whole new made up language, for instance. This is because the setup of the Chinese Room includes a 'rule book' - something fixed - whereas our experience of the mind is of something mutable, able to learn and change. If we alter the experiment such that the mechanism Searle is working comprises data stores and rules that self assemble based on input and boot strapping rule generation patterns, it becomes much harder to intuitively assert that NOTHING is doing any understanding in there. The fact such a system is (non intuitively) Turing equivalent to a sufficiently comprehensive rule book is something the thought experiment relies upon to trick you.

permalinkembedsaveparentreport

[–]obenj 1 point 4 years ago 

You have a very thorough response so forgive me if I miss anything as I am running on low sleep at the moment.

First I want to mention that the Turing test is not a reliable or consistent method in determining intelligence or human like capabilities. An AI passing the Turing Test does not prove that you can teach it a new language, vice versa, or that it can do anything else a human can do. The Turing test can also not be “flawlessly” passed per say, because there are no exact parameters for passing except for tricking a judge into believing the AI is human. The parameters only specify that the machine is 100% convincingly a Chinese speaking human, no more. There are no specific questions the judge is required to ask, so passing is often as simple as the AI pretending to be stupid. The AI does not have to sound smart or answer questions correctly, only pass as a human.

While I agree that the Chinese Room is not 100% disproof of strong AI because it fails to exclude the possibility of the “known mind” being part of a larger conscious entity, it really is quite convincing. Logically I can’t disprove the consciousness of the Chinese room as a whole because consciousness is not externally measurable, but to say that it’s even remotely likely to be conscious seems dishonest. At the very least I think it is a fantastic argument against functionalism. Regardless of whether or not you think the Chinese room as a whole plausibly has consciousness, the differences in internal mechanism between an actual Chinese speaker and the Chinese room are drastic enough that defining mental states by their functional role seems absurd.

I’m not quite sure I understand your last paragraph, so correct me if I’m wrong here. I believe you’re saying that the computer in the Chinese Room follows a simplistic rule book/coding, and if it used a more complex code that was more dynamic it would be harder to disprove consciousness. If this is what you’re saying, I disagree on the grounds that consciousness is not defined or even characterized by the ability to learn or change. Yes they are part of our experience but that does not mean they define our consciousness. Regardless, rule generation patterns or self assembling code have to start with a ‘rule book’ detailing the rules and instructions of self assembly, and therefore are equally as deterministic as the original Chinese room code.

permalinkembedsaveparentreport

[–]whentheworldquiets 2 points 4 years ago 

Look at it this way: if I were faced with the task of giving a black box a Turing test, I would try to teach it a language or get it to teach me one :)

Better still, look at it this way: if all the Chinese Room experiment is doing is saying that it's possible to trick someone into believing they're having a conversation without the system doing the talking being aware... so what? That's still not a disproof of Strong AI. At the very best it counters any attempted PROOF of strong AI - but it does so by effectively asserting solipsism: any mind that is not wholly my own may not be a real mind.

> I disagree on the grounds that consciousness is not defined or even characterized by the ability to learn or change.

There we will have to disagree, I fear. How can one be aware of something without being different to when one was not aware of it?

> Regardless, rule generation patterns or self assembling code have to start with a ‘rule book’ detailing the rules and instructions of self assembly, and therefore are equally as deterministic as the original Chinese room code.

Determinism and awareness are not mutually exclusive.

In the end the Chinese Room argument boils down to organic prejudice and solipsism: "I know I am a mind, and I am willing to credit very similar looking arrangements of matter with having a mind and awareness, but I will deny that same generosity to other arrangements of matter despite them behaving just like me and despite me not knowing for sure that other brains are really self aware either."

permalinkembedsaveparentreport

[–]obenj 1 point 4 years ago 

The Chinese room AI doesn’t have to answer any of your questions correctly, so even if you do ask it to teach you a language or try to teach it a language it may be fruitless. If you ask it to teach you a made up language it may just tell you that’s too hard, but it it’s in perfect Chinese and you think it’s a human then its successfully passed the test.

In regards to how we define consciousness, would you say a human with severe amnesia is not a conscious being? If the amnesia was so severe that they no longer could create any memories whatsoever. They would be aware only of the exact present moment. I understand this is a huge what if, and I agree that memory and change is beyond common in conscious individuals, however I don’t see it as a defining or required feature of consciousness.

I think you have a great point in your second paragraph about how the Chinese room at best counters any proof of strong AI without disproving it’s existence. I think this is where we mostly agree. The Chinese room definitely does not disprove strong AI, but there are very few things about the nature of this world that we can 100% prove or disprove. I understand that it’s fully possible that inorganic entities can and may have consciousness, but personally I find it very unlikely. I know this is organic prejudice, however I find organic prejudice justifiable. Consciousness is a man made term, determined by organic creatures to explain how they experience the world. Our consciousness and realities are heavily influenced by our senses, which are uniquely organic. Even if there is some form of “consciousness” in inorganic entities, I think it would be so different to how we experience it that we couldn’t even call it consciousness.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

I wouldn't get too hung up on the particulars of the Turing test or the implementation of the Chinese room. It's intended to convince the reader that even a perfect simulacrum of a human being, able to pass any external test, could do so without being what we call "aware"

Probably the simplest argument I can muster against that claim is this:

If it is possible for a perfect simulacrum of a human being to exist without awareness, then it is remarkable to the point of incredulity that evolution would have converged upon the one configuration of matter that can both grow deterministically from a single cell to behave like a human AND enables the experience of being one. Just think if our neurons had a slightly different chemical composition, or fired via a different electrochemical reaction, we would just be two insensate bags of meat yammering at each other, rather than two self aware minds.

Clearly that line of thinking is absurd. It is ridiculous to suppose that the precise electrochemical implementation of our neural network determines the presence or absence of awareness, not least because unless it by a staggering coincidence also happened to be the most efficient, natural selection would have long since replaced any conscious humans with the aforementioned bags of meat.

That being the case, I'm obliged to conclude that I cannot deny the Chinese room its own version of awareness. The fact its Searle component does not partake of it is as irrelevant as the fact he does not partake in MY consciousness.

permalinkembedsaveparentreport

[–]rhods1 1 point 4 years ago 

Your presupposing that the brain is some kind of computer.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

No, I'm simply demonstrating that the chinese room experiment does not rule out Strong AI. I'm not trying to prove the Strong AI conjecture.

permalinkembedsaveparentreport

[–]rhods1 1 point 4 years ago 

So your argument is that it doesn’t rule out strong AI because the computational theory of the mind is true?

permalinkembedsaveparentreport

[–]MyPersonalAccounts 1 point 4 years ago 

this is the real answer. Don't know why it dropped from first place when it has 100 more upvotes.

permalinkembedsaveparentreport

[–]seldomtimely 1 point 4 years ago 

That's not a good response, nor a refutation, of Searle's argument. The system's response is beset by the same problem: the system is making correct syntactical manipulations without a clue as to the semantics/ semantic contents of the langauge. Now, the only way the argument can fall is if you extend that same thesis to the human wielder of language, rendering semantic content into some sort of epiphenomenon. But that's not a plausible thesis when considering linguistic meaning in natural languages. If interested in the topic look up the grounding problem

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago* 

> the system is making correct syntactical manipulations without a clue as to the semantics/ semantic contents of the langauge.

Prove it :)

Searle's argument is misdirection because he says "Well, *I'm* in the room and I *know* I don't know what's going on. Therefore the system doesn't know." But that's tantamount to solipsism: he could make the exact same claim about ANY mind that isn't wholly his. The moment you grant the existence of other awarenesses you abdicate the right to legislate what being a mind is 'like'.

It's also a binary claim. There's no room for 'semi aware'. According to him, a mind either is aware of semantic content or it is not. Binary. So let's use that as a lever to tear the argument down.

Suppose we stick some wires in Searle's head and I stand in for one of his neurons. All I do is sit in a dark room and mimic the behaviour of that neuron based on its inputs and outputs.

Am I justified in saying "Well, I'm part of the mechanism and *I* don't know what's going on, therefore the system as a whole doesn't know"? Clearly not.

What if we replace more neurons with stand in people? What if we keep going until there isn't a single neuron left, just people in dark rooms pressing a button when the right lights come on. At what point in that neuron by neuron process does the system execute a binary flip from being semantically aware, to not?

permalinkembedsaveparentreport
[+][deleted] 4 years ago (2 children)


[–][deleted] 1 point 4 years ago 

The way Searle talks about this is never how I've understood the Chinese Room Argument. I've never thought of it as a phenomenological argument but rather an epistemological one. It makes sense as a counterfactual that refutes not strong AI but the possibility of knowing whether or not you've constructed strong AI. There are no data observable from the outside which provide sufficient evidence of consciousness.

This is the same lesson I draw from zombie arguments. Chalmers' claim about conceivability entailing possibility and the implications viz a viz dualism are not convincing to me. However, zombies as a counterfactual do refute the idea that we can use external evidence to prove anything about phenomenal experience.

Given the ethical implications of constructing conscious AI this is a very important observation.

permalinkembedsaveparentreport

[–]rockoil 1 point 4 years ago 

I came to make the same argument. The cells in your brain also don’t understand English, Chinese or anyhthing. They don’t even understand movement. I do (not the Chinese).

As long as the computer, as a whole, can understand and interact with others, indistinguishable from a human, I’d say a mind has been created.

permalinkembedsaveparentreport

[–][deleted] 1 point 4 years ago 

I saw it similarly to you but slightly different. In his analogy he's the hardware, not the software. He started by saying, the program is the mind. Then he says, I'm in this room with a rulebook which is the program. The rulebook is the mind, not him. It's a muddy refutation since he in the example does have a mind being a person, but he is analogous to something that doesn't, ie a computer.

permalinkembedsaveparentreport

[–]PsychicPsauce 1 point 4 years ago 

I have to defend Searle because I think what he's trying to say is that no matter how good a computer gets at translating Chinese words into English, that computer is never going to "understand" Chinese (or English for that matter). It's always going to remain an inanimate object. And the concept of "strong" AI, according to wikipedia:

Artificial general intelligence ( AGI ) is the hypothetical [1] intelligence of a machine that has the capacity to understand or learn any intellectual task that a human being can.

I actually think that it should end at "understand." Searle says he "understands" English, but I would say that he "understands" the language no better than a machine learning algorithm for chess "understands" chess. They both learned from trial and error, with feedback on positive and negative outcomes and rules for improving their efficiency.

We have to ask ourselves if our brains are doing more than what a computer does. If our bodies are simply taking the inputs of our environment, processing them, and outputting the most efficient actions, then eventually, we could make a computer that could be indistinguishable from a human being, which is what some people think is the definition of strong AI. However, we should ask ourselves why this would be interesting.

I think that if you want to have something that you can call "strong" AI, you would need a program that is able to literally improve its own code while it's running. This is what our brains do every day. I don't think that is going to happen on PCs any time soon.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

I have to defend Searle because I think what he's trying to say is that no matter how good a computer gets at translating Chinese words into English, that computer is never going to "understand" Chinese

That's not the thrust of his argument. It's nothing to do with translation between languages, it's the ability to respond in a manner indistinguishable from a human being - to converse in one language. At first it seems reasonable to suppose that obviously a stupid bot parroting canned responses isn't self aware, but that's not giving enough credit to the person interrogating the bot. For example, I would try to teach the bot a new language. If I succeed, then I know whatever is in the box now 'understands' my language even if only in the context of its prior knowledge and experience . But what is OUR understanding but an accumulated set of rules within the context of prior experience?

I think that if you want to have something that you can call "strong" AI, you would need a program that is able to literally improve its own code while it's running. This is what our brains do every day. I don't think that is going to happen on PCs any time soon.

That's actually pretty old news :)

permalinkembedsaveparentreport

[–]PsychicPsauce 1 point 4 years ago* 

Just re watched it and I actually agree with your original argument. We don't go about thinking by telling our neurons and atoms what to do.

However, his point is that no matter how good a computer gets at simulating human behavior, it will never be more than an inanimate object running a pre determined set of instructions.

Edit: Watched whole video

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

it will never be more than an inanimate object running a pre determined set of instructions.

But not necessarily consciously predetermined by the programmer. Nobody can anticipate in advance what a machine learning algorithm will do under all possible future inputs. So a great deal of the 'programming' of such a machine comes from the inputs - the algorithm's experiences - not the programmer. Which all of a sudden doesn't sound so different from our experiences as human beings, does it?

permalinkembedsaveparentreport

[–]PsychicPsauce 1 point 4 years ago 

While it's true that nobody can anticipate what a machine learning algorithm will do under all possible future inputs, I think the crux of the issue is that a machine will never be "conscious" in the way that a human being is.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

I agree consciousness is the crux of the issue, but I disagree with the claim that a machine can or will never achieve it.

Why? Because our brain is a machine. It is made of protons and neutrons and electrons - the exact same stuff a computer and its software are made of. So what that claim is really saying is that only one particular configuration of protons and neutrons and electrons is special enough for consciousness to emerge/attend.

To which I would say (as I have to others here): how incredible it is, then, that evolution chanced upon - and did not optimise away - the one and only way for protons neutrons and electrons to collectively manifest consciousness. Just think: if it had picked silicon instead of carbon, we would be two mindless unaware bags of meat playing out this conversation based on the mechanical interactions of our synapses, rather than conscious beings.

Is our organic prejudice starting to sound a bit implausible yet?

If we were aliens with different brain chemistry, landing on the earth and studying humans, would we be justified (once we'd found the right end to probe) in saying, "Oh, look - these brains are carbon based. So there can't be any real awareness here, no real consciousness. They're just complex simulacra."

To me the question is not "can machines ever be conscious". Rather I would ask: "Given the fact that the only known complex self reflecting abstraction engines in the universe - arrived at by the unintentional grinding of evolution - are also conscious, how can we possibly doubt that it is the property of "being a complex self reflecting abstraction engine" that confers consciousness, and not the choice of building material?"

permalinkembedsaveparentreport

[–]PsychicPsauce 1 point 4 years ago 

I think the main problem is in perspective. Humans, and all animals are born, live, and die. A computer is built, and could theoretically run forever. The human experience starts with childhood, goes through adulthood, and ends in old age. A computer gets booted up, and operates on whatever inputs humans put into it.

My question is: if we asked a computer AI to describe its experience to us, what would it tell us? Could it tell us how it feels to boot up, or run a particular program? I think that in order to call it a strong AI, it would have to be able to think (and feel), not just convince outside observers that it is thinking. Therefore, I would say that a computer will never truly be any more conscious than a hammer, no matter how good that hammer is at simulating thought.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

Could it tell us how it feels to boot up, or run a particular program?

For the former, I would say it would depend on whether 'waking up' was instantaneous for it. If it were a gradual process during which memories of its thought processes could be retained, yes.

For the latter, we're slipping back into the same mistake Searle is making, asking how it feels to run a particular program as though it is the CPU we are inspecting for awareness. If you asked an AI to run a program it would be like asking a person to run a program using pencil and paper.

permalinkembedsaveparentreport

[–]reverendsteveii 1 point 4 years ago 

When I was taking Philosophy at school our discussion of the chinese room problem focused on synergy, the way the combination of the room, the person and the lookup book can have something that none of them have any part of individually. I wish I'd had your gift for rhetoric, it would have made the discussion a lot more fruitful to be able to ask which particular neuron has some inkling of how to speak chinese and how many instances of that neuron I needed to understand it.

permalinkembedsaveparentreport

[–]Zanakii 1 point 4 years ago 

I didn't see it like that. I'm seeing his argument as more along the lines of the system not having meaning behind seeing each symbol, which a computer likely wouldn't. A computer sees the word dog, it can now give you words that associate with dog like fetch, bark, animal but it doesn't have any meaning behind it, it isn't understanding that a dog is a furry lovable companion or have past knowledge of dogs etc. It's just pumping out what it was programmed to know.

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

Like Searle, you're looking for understanding in the wrong place, and declaring it doesn't exist when you don't find it.

Of course a computer, or a hard drive, or a CPU doesn't 'understand' what a dog is. The 'understanding' is in the connections between 'dog' and 'furry' 'lovable' and 'companion', and between those concepts and yet other concepts within the system. The richer and more interconnected that system of data, and the better its inputs can abstract and generalise input, the greater the level of understanding it represents.

Some people at this point say "Yeah, but it's not real understanding; it's not real awareness. And my reply is: well isn't it fortunate that evolution just happened upon the one way of building an expert system - just the right arrangement of protons, neutrons and electrons - to make our understanding and awareness real. A few molecules one way or the other and we would be two mindless bags of meat playing out this conversation via our unaware neural nets.

Does that sound likely?

Whatever is going on between minds/awareness/experience and complex self reflecting self modifying systems, I think we can safely discount the hypothesis that the details of the hardware are significant.

permalinkembedsaveparentreport

[–]Keruli 1 point 4 years ago 

You employ/pre suppose a different concept of understanding to Searle's.

From what I can tell from the very informal statement of his argument: That there is no understanding happening in the combination of him + rulebook + alphabet is effectively a premise of his argument - a premise whose agreement depends on your idea of understanding/meaning/consciousness.

So the argument pre supposes a certain concept of understanding and thus only works within that framework. I don't think this is a flaw in the argument, I think he didn't intend the argument to be stronger than that. And if you see understanding differently, then of course the argument doesn't work...

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

If "no understanding is happening" is a premise, then the exercise is meaningless because whatever happens it won't be counted as 'understanding'.

In that case his philosophical position collapses to solipsism: he cannot in good faith attribute awareness and consciousness to any mind except his own, because they are all chinese rooms.

Remember: it is his contention that a purely mechanical simulation of a mind is not a mind - well, all he can see by inspecting other peoples' heads is a mechanical simulation of a mind. He cannot tell if the magical X factor of awareness he possesses is present.

permalinkembedsaveparentreport

[–]Keruli 1 point 4 years ago 

no, you've generalized the premise ad absurdum, or to the point where the whole argument becomes trivial - generalized to have a different content and logical form than what I'm saying. The premise is more specific, something like: no understanding is happening in a situation/event that consists of a machine that simply follows a rule book and uses a given supply of symbols to process inputs into outputs.

Regarding solipsism, i disagree also. The basic answer is something like: when we see another human we associate them with ourselves and ascribe similar properties/states/contents/ontology to them. And you seem to be confusing a) all other entities (including those we call people or agents or whatever) MAY be chinese rooms with b) they all ARE (must be) chinese rooms...

permalinkembedsaveparentreport

[–]whentheworldquiets 1 point 4 years ago 

The premise is more specific, something like: no understanding is happening in a situation/event that consists of a machine that simply follows a rule book and uses a given supply of symbols to process inputs into outputs.

Okay - to help calibrate what I say next, what would you say about the following:

Suppose we put Searle in a room and handed him a series of pictures. All of the pictures are of TV static, and we have randomly assigned each picture to one of two categories, A or B. Searle has at his disposal a catalogue of all the pictures along with which category they are in. His job is to tell us which category each picture we give him belongs to.

Would you agree that no 'understanding' arises as a result of this experiment?

Assuming you do, why? I would say it is because it is impossible for Searle to apprehend any pattern. He cannot generalise, and nor does the system as a whole. Even if he eventually internalises the rule book and sorts the pictures from memory, he would not be able to categorise any new pictures we handed him.

Now suppose we change the experiment so that the pictures are of two different objects viewed from various angles. Searle has never seen either of these objects before. We still give Searle the catalogue, but in a very short space of time he stops using it.

In this case, would you agree that understanding has occurred? I would say yes, because Searle has apprehended a pattern. He has generalised a principle. Not a lot of understanding has occurred - he doesn't know what the objects are for, but still: a little bit of understanding has occurred based on input and feedback.

So in both cases there's a mind and a rule book. The rule book is effectively the same in both cases, but in only one do we credit the mind - something we know to be capable of understanding - with doing so.

Assuming you're in agreement so far, on what basis can we say that an artificial neural network that has learned to distinguish between pictures of two unfamiliar objects has not gained an equivalent degree of understanding? The chemical composition of the neurons? I think that's a non starter.

Back to the Chinese Room, and hopefully you can see where I'm heading with this. In Searle's thought experiment - which seeks to disprove Strong AI, the existence of understanding in any artificial mind - he deliberately chooses to implement that artificial mind in rule book form. He stacks the deck.

Suppose instead Searle was manually performing the calculations that lead to an artifical neural network 'understanding' these two categories of item. He would be just as unaware of the effect of his actions as he was in the Chinese room, yet he would be part of something that DID understand.

permalinkembedsaveparentreport

[–]Keruli 1 point 4 years ago* 

thank you for the interesting additional variations. I'm not entirely in agreement with your 2nd one - various tricky questions arise with the notion of the 2 unknown objects and with the interpretation of the hypothetical Searle's pattern recognition suggesting actions. I think that doesn't matter though, as I agree with the main thrust of your argumentation. I suspect the real problem here is that we don't actually have clarity/agreement on what the conclusion/thesis of the chinese room is. I'm re watching the video and it's quite hard to make out exactly what he's claiming, maybe i will read up on it somewhere.

I would have no problem understanding his thesis if he simply left it at t1: (It is not the case that: (If a simulation passes the turing test, then understanding/mind is occuring/present)) which is how he starts, but then he adds alternative/additional quasi definitions of 'strong AI' and it gets too tricky to understand exactly what he's saying right now.

To him the term 'strong AI' seems to, at least in the first instances in the introduction of the video, to refer to a statement about the relation between turing tests and understanding/mind. This seems somewhat confusing, as 'strong AI' may also be taken to refer to a type of artificial intelligence..... maybe some problems are occuring with this ambiguity.

Now, taken in the sense of the statement t1, i agree with it - and t1 doesn't claim that neural networks or whatever can't produce a mind, just that passing the turing test doesn't imply a mind.

....OK now i've listeend to the 2nd part of his quasi definition 3 times, and I suspect that he actually used some very ambiguous and misleading formulations in his introductory definitions, and that that's where the problem lies as opposed to in the actual argument (although one could take the introduction to itself be a premise, i suppose... Yes, maybe another problem with this kind of thought experiment is that it is misleading in that it suggests the premise set is contained in the thought experiment, distracting from the fact that some key premise may be contained in the preamble - A kind of misleading division of the premise set occurs.). later in the introduction he says

''[...] strong AI - and that view says: the mind is to the brain as the program is to the hardware. So if we've got the right program, then it doesn't matter what the hardware is, because we've created a mind.''

Now firstly, it seems quite questionable what the logical relation is between this statement and his previous quasi definition of 'strong AI' 15 seconds earlier where he more directly connecting the concept to the turing test. Secondly, it seems to me that there is a key ambiguity in the 'right' in 'right program': Does that mean right in the sense of right for passing the turing test or right for producing a mind? These two are conceptually distinct - co extensionality would need to be argued for...

anyway, this seems like a mess, i'm not sure where this gets us now : D

permalinkembedsaveparentreport

[–]random_access_cache 1 point 4 years ago 

Thank you, I watched this and although I find this to be a very interesting and well thought argument, but I couldn't help but feel that there is a very easy counter argument I can't quite make up. This could be it. We can't really refute Strong AI 100%, I would argue his explanation reduces the probability of Strong AI but doesn't diminish it (if at all). Then again, I still find it to be an interesting view worth exploring.

permalinkembedsaveparentreport
load more comments (8 replies)
[+][deleted] 4 years ago (148 children)


[–]rmeddy 25 points 4 years ago 

I always think of this comic when talking about The Chinese Room.

To me, it's pretty easy to keep kicking that can down the road.

permalinkembedsavereport
load more comments (1 reply)

[–]thesnuggler83 47 points 4 years ago 

Make a better test than Turing’s.

permalinkembedsavereport

[–]dekeche 40 points 4 years ago 

I'd agree with that. The argument seems to be less a refutation of "strong A.I." and more of a refutation of our ability to tell if responses are generated from understanding, or pre programmed rules.

permalinkembedsaveparentreport

[–]KantianNoumenon 18 points 4 years ago 

It's a response to "functionalism" which is the view that mental states are "functional states", meaning that they are just input/output functions. This view was popular in philosophy of mind around the time Searle wrote his paper.

If functionalism is true, then a perfect digital simulation of a mind would literally *be* a mind, because it would perfectly replicate the functional relationships of the mental states.

Searle thinks that this is not the case. He thinks that minds are properties of physical brains. You could have a perfect simulation of the "functions" of a mind without it actually being a mind (with meaning and conscious experience).

permalinkembedsaveparentreport

[–]AccurateOne5 9 points 4 years ago* 

It’s not clear how he’s drawing that distinction though. He tries to rely on intuition to draw a distinction between the program in the book and the human by saying that the program in the book is in some sense “simple”, by virtue of it being on a book. That is however a restriction that he imposed.

What if as part of the instructions in the book, you had to store information somewhere else and retrieve it later?

To answer questions like “What day is it?” will obviously require inputs beyond what are available to a human sitting in a box with a book. A Chinese person in a box will also not be able to answer such a question.

Essentially, it’s not clear how he drew a distinction between the human brain and the thought experiment. Furthermore, the reason the argument “seems to make sense” is because he needlessly handicapped the AI by making it simpler than it would be.

EDIT: he also argues that since the English person doesn’t understand Chinese the whole “box” doesn’t understand Chinese. Replace the book with an actual Chinese person: the English person still doesn’t understand Chinese, does the system still not understand Chinese?

permalinkembedsaveparentreport

[–]thesnuggler83 3 points 4 years ago 

Searle’s whole argument collapses on itself when scrutinized, unless it’s more complicated than he can explain in 3 minutes.

permalinkembedsaveparentreport

[–]KantianNoumenon 1 point 4 years ago 

He's trying to establish that you could have a functional simulation of mental states that doesn't actually have mental states. If that is true, then functionalism is false.

I think you're getting a bit caught up on the particulars of the example and not on the structure of the argument and what he is actually arguing against. I recommend reading the SEP article about the paper here and the article about functionalism here .

I think that a lot of people misunderstand what the argument is actually about, and think that he's arguing for dualism or something similar. Searle thinks that consciousness is a biological phenomenon (the way that digestion is a biological phenomenon). He thinks that a simulation of consciousness would not actually be real consciousness (like how in a computer simulation of water, nothing actually gets wet).

permalinkembedsaveparentreport

[–]MechanisticMind 1 point 4 years ago 

Searle thinks that consciousness is a biological phenomenon

I mean (almost)everyone thinks it is a biological phenomenon...in us.

That doesn't meant it can't be done non biologically, and I'm pretty sure even Searle thinks that might be possible. So this statement doesn't mean much.

I think that a lot of people misunderstand what the argument is actually about, and think that he's arguing for dualism or something similar.

That may not be his intention but the following...

He thinks that a simulation of consciousness would not actually be real consciousness

Really doesn't sound that different from the p zombie scenario (especially to a functionalist). Two entities that behave the exact same way but apparently only one is 'conscious'

Sure they are made of different stuff (unlike p zombies) but Searle doesn't have a good argument for why that matters, and I don't think he could do that without dipping into dualism.

If we design robots that simulate our minds quite well and can reproduce(by building new units) and leave them on a planet for a few centuries, perhaps they will come up with their own philosophy of mind.

And in this philosophy only digital silicone circuits can be conscious, and biological or other means of simulating it can never truly understand like digital silicone does.

Searle can say they are wrong all he likes if he meets them, but what argument can he possible make to these robots that the robots couldn't just reverse? (and vice versa)

(like how in a computer simulation of water, nothing actually gets wet).

Things in the simulation get wet (if it is good)

But in the same way that simulated water can't wet things in the real world, non simulated water can't wet thing in the simulated world.

The argument works both ways, this doesn't show the 'realness' of one over the other, but that they occupy different worlds.

permalinkembedsaveparentreport

[–]KantianNoumenon 1 point 4 years ago 

That doesn't meant it can't be done non biologically, and I'm pretty sure even Searle thinks that might be possible. So this statement doesn't mean much.

My understanding is that Searle thinks that an artificial mind would have to be physically analogous to a human mind, meaning that the robot would have to have some kind of artificial brain.

Really doesn't sound that different from the p zombie scenario (especially to a functionalist). Two entities that behave the exact same way but apparently only one is 'conscious'

Sure they are made of different stuff (unlike p zombies) but Searle doesn't have a good argument for why that matters, and I don't think he could do that without dipping into dualism.

I don't think that the p zombie is analogous to the Chinese room in this case, because the p zombie has a brain that has all the physical characteristics of a normal human brain. Searle would say that such a p zombie is impossible because mind is a physical property of human brains.

Things in the simulation get wet (if it is good)

But in the same way that simulated water can't wet things in the real world, non simulated water can't wet thing in the simulated world.

The argument works both ways, this doesn't show the 'realness' of one over the other, but that they occupy different worlds.

These are really strong statements (strong in terms of having far reaching entailment) and I think you would have to do a lot of metaphysics to actually support these statements.

permalinkembedsaveparentreport

[–]MechanisticMind 1 point 4 years ago 

I don't think that the p zombie is analogous to the Chinese room in this case

My point was that they are essentially the same argument to a functionalist, who is the target of the argument. What something is made out of doesn't matter to a functionalist, so one would think Searle would try and give a reason to care about what something is made of.

Searle would say that such a p zombie is impossible because mind is a physical property of human brains.

I know what Searle would say in most cases, but as per my reversal argument, the robot planet philosophers would simply state that mind is a physical property of digital silicon circuits arranged correctly. What arguments would Searle make against that besides his usual scoffing at views different from his own?

The concept of a computer simulating everything a brain does to the extent where it is indistinguishable from a human but somehow misses a "physical property" of the brain doesn't make any sense.

If it misses a physical property of the brain, then it couldn't possibly simulate the brain in the first place (which is a different claim, but not one Searle is making here) because physical properties change the behaviour of a system. And if you say this property has no causal role, then wouldn't it be a non physical property? basically a form of dualism(epiphenonimanism or at least panpsychism) which you claim Searle is not arguing for?

I never see biological naturalists come head to head with this major issue.

permalinkembedsaveparentreport

[–]KantianNoumenon 1 point 4 years ago 

I know what Searle would say in most cases, but as per my reversal argument, the robot planet philosophers would simply state that mind is a physical property of digital silicon circuits arranged correctly.

I think that this reversal case is only analogous if we assume that the robot philosophers actually have consciousness. If that is true, then the example begs the question. If it is false, then you have non conscious beings arguing that they have consciousness, and I'm not sure what that tells us.

Anyways, I don't really know why you're arguing with me. My original intent for commenting in this thread was to make some clarifications for people who had basic misunderstandings about Searle's argument. I'm not actually here to endorse Searle's views.

I think that a lot of people in this subreddit don't actually know anything about philosophy and tend to pull comments out of their ass. On the flip side, it seems like you've engaged with the material before.

My suggestion would be to try to make smaller scope assertions and avoid strong metaphysical claims. I'm not an expert in the P zombie argument, but I remember that it is complex and relies on some obscure modal logic. So, I would recommend avoiding comparisons to that argument unless you're prepared to provide a fleshed out analysis of how its structure is analogous to this case. In philosophy, we generally want to limit the scope of our discussions and avoid introducing new considerations.

permalinkembedsaveparentreport

[–]MechanisticMind 1 point 4 years ago 

I think that this reversal case is only analogous if we assume that the robot philosophers actually have consciousness.

Note that the robots can also make this case about us. People like Searle say only one side is correct, they need to provide a good reason or explain what they mean more clearly imo.

If it is false, then you have non conscious beings arguing that they have consciousness, and I'm not sure what that tells us.

Not just that, but non conscious beings that act exactly the same as conscious beings arguing that they have consciousness.

It seems to tell us that an entities claims (like Searles) that only beings like themselves could be conscious are to be dismissed as they don't necessarily correlate with them being conscious.

Anyways, I don't really know why you're arguing with me.

Yeah I didn't mean to come off this way, but I can't exactly argue with Searle :P

And as you had some familiarity I thought you might know of where a biological naturalist clarifies the issue I brought up before of the "physical property" and what it means.

There don't seem to be many biological naturalists around, Searle/Massimo/Block are the ones I know and they either don't bring it up or evade from what I've seen.

But as you don't seem to endorse the view I guess this convo has reached its end :)

permalinkembedsaveparentreport

[–]rockmodenick 1 point 4 years ago 

You seen pretty well versed in this, so other than applying it to AI, isn't this essentially an argument that a simulation of a mind would be a p zombie? I've always felt the Chinese room was another take on or maybe expansion of that philosophical concept. There might be differences in unaware of.

permalinkembedsaveparentreport

[–]KantianNoumenon 1 point 4 years ago 

The p zombie argument is an argument for dualism. Searle does not agree with dualism, he thinks that consciousness is a physical and biological phenomenon.

P zombies are supposed to have normal human bodies and physical brains. Therefore, a simulation of a mind would not be a p-zombie.

permalinkembedsaveparentreport

[–]rockmodenick 1 point 4 years ago 

I track most of that, hence applying it to AI being different. I think I'm not asking this correctly, would it be correct to think of the mind of a Chinese room type non conscious AI as having the same characteristics as the mind of a p zombie? Or to put it another way, that a Chinese room AI could correctly simulate the mind of a p zombie?

permalinkembedsaveparentreport

[–]KantianNoumenon 1 point 4 years ago 

They both don't have a mind. So I guess they're analogous in the sense that 0 == 0.

permalinkembedsaveparentreport

[–]ice109 4 points 4 years ago 

You'd never be able to write such a program because the number of questions is infinite but the number of responses is finite (because the program is finite). Note I'm not not talking about recognizing a recursively enumerable language. Searle explicitly said database and those are finite (and if not then you're talking about a model of computation that's beyond what we have now and for the foreseeable future).

Alternatively I would argue that given enough time he would actually "understand" because he's not a fixed ROM computer; he would learn to recognize patterns and be able to abstractly reason about the symbols (much like one would infer the rules of arithmetic given enough arithmetic examples). Would he know what the Chinese words "pictured in the world" (ala Wittgenstein)? Obviously not but does that matter? Holy grail of AI is symbolic reasoning.

permalinkembedsaveparentreport

[–]lurkingowl 1 point 4 years ago 

Part of what I think people miss when talking about the Turing test is that it basically includes a lot of tests better than itself. :) If you come up with some approach that you think isn't covered, then just have the tester use that approach to distinguish the two, and voila!

AI can't currently win the Turing test against a random person off the street, so we don't think of the Turing test as being against a trained professional AI Researcher/Cognitive Scientist. But beating an expert is the real Turing test, and an expert would use any other option for testing available (assuming it fits the format.)

permalinkembedsaveparentreport
load more comments (1 reply)

[–]HomicidalHotdog 33 points 4 years ago 

Can someone help me with this? Because this does seem like an effective argument against the sufficiency of the Turing test, but not against strong AI itself. By which I mean: we do not have a sufficient understanding of consciouness to be certain it is not just as he describes receive stimulus, compare to rules, output response - but with much, much more complicated rulesets that must be compared against.

So yes, the chinese room refutes the idea that the a Turing complete computer understands chinese (or whatever input), it fails to demonstrate that from the outside (us as observers of the room) we can be certain that the box in questions is not conscious. I have a feeling that I just am taking this thought experiment outside its usefulness. Can anyone point me in the direction of the next step?

permalinkembedsavereport

[–][deleted] 4 points 4 years ago 

So yes, the chinese room refutes the idea that the a Turing complete computer understands chinese (or whatever input),

Only for a very specific box to draw around the computer. It does not refute that the program understands.

Let's say we have an implementation of the chinese room that is just a choose your own adventure. Quattrovigintillions upon quintillions of 'if you see character x, go to page y'.

The page number necessarily contains at least as much information as a human consciousness. For every letter it is responding to for every favourite colour you claimed in the last letter for every phone number you could have possibly given it for every day you told it was your birthday there is a table of gotos covering every possible phone number you could be about to give it.

Not only that, but those gotos describe an information processing system at least as powerful as the human consciousness, or the turing test will eventually fail.

.The only thing the chinese room proves is that the hardware (even if virtualised) is not the whole of the thing that is conscious, which is so obvious that saying it is completely pointless.

permalinkembedsaveparentreport

[–]HomicidalHotdog 1 point 4 years ago 

Thank you, this helped

permalinkembedsaveparentreport

[–]Jabru08 9 points 4 years ago 

I wrote a long essay on this problem exactly in college, and from my understanding you've hit the nail on the head. If you push his argument to its logical extreme, you simply end up with a re statement of the problem of other minds that happens to criticize the usefulness of the Turing test in the process.

permalinkembedsaveparentreport

[–][deleted] 2 points 4 years ago 

I think Searle fails to even do that much. An bilingual native english speaker merely has a memorized set of rules for converting english words to chinese words. Does the english speaker understand chinese?

A native chinese speaker merely has a set of instructions for connecting things and ideas in their head to chinese symbols. Does this person truly understand chinese? Per Searle's example, no. His whole argument is absurd.

permalinkembedsaveparentreport

[–]ElDescalzo 1 point 4 years ago 

My experiences learning a second language, teaching English as a second language and (oddly) watching autistic children make me think you must be wrong and Searle must be right.

Searle understands the English. The questions and answers have meaning to him. The Chinese does not have any meaning to him. One of the autistic children I am thinking of is very much aware of the rules and of the meaning of the concepts she wishes to convey. She is often frustrated by her inability to follow the rules to convey the meaning.

permalinkembedsaveparentreport

[–]HomicidalHotdog 1 point 4 years ago 

I guess what I'm trying to get at is this: We don't what consciousness is, so it may very well be that we are executing rules we aren't aware of that tell us what the meaning is. Thus, we can understand when a pun is delightfully terrible because we understand what the language's true meaning is and what the clever turn of phrase's meaning. I am not going through and following language rules, it's intuitive. But that intuition is built on my learning the rules, not innate understanding.

permalinkembedsaveparentreport

[–]zero_z77 1 point 4 years ago 

What the chinese room basically proves is that deception has less to do with the abilities of the deciever and more to do with the perception of the person being decieved.

For example, if the symbols fed in to the room are in "gibberish" but still use chinese characters, the man in the room may "think" he's learned chinese but has in fact been decieved into learning gibberish. The turing test is predicated on the idea that deception requires cleverness, the chinese room disproves that premise.

permalinkembedsaveparentreport
load more comments (2 replies)

[–]Shitymcshitpost 8 points 4 years ago 

This guy is using logical fallacies of someone who's religious.

permalinkembedsavereport

[–]sck8000 8 points 4 years ago* 

Due to limitations of human observation, is it not true that a sufficiently complex AI actually being sentient and one merely appearing to be sentient are functionally indistinguishable to us? The limitations of the human experience prove this to be true, as it is the case for how we consider other human minds.

In an almost Truman Show esque analogy: Imagine that everyone in your life, except yourself, is an actor with a script. This script tells them what to do, what to say, how to portray every detail of their interactions with you in an almost infinite number of situations. In effect, artificially reproducing the experience of your whole life down to the tiniest of details.

How could you distinguish those people from your own consciousness, determine that they are genuinely sentient as you are, rather than following a script? They are essentially all "Chinese Rooms" themselves. Descartes famously created the maxim "I think, therefore I am" as a demonstration that only his own consiousness was provable. The same could be said here.

Break down the neurology of the human mind down to a granular enough scale, and you have basic inputs and outputs, simulatable processes on a sufficiently complex machine. Give someone the tools, materials, and enough time, and if you gave them such a model of a person's human brain, they could recreate it exactly. How is that any different to an AI?

The "context" that Searle refers to is just as syntactical as the rest of the operations a machine might simulate. We cannot prove that our own meanings and experiences are not equally logical, let alone those of an AI. He may state that he has greater context and meaning attached to his logic than that of a machine, but it could just as easily be simulated within his own neurones - a "program" running on his own organic brain.

permalinkembedsavereport
load more comments (4 replies)

[–]Ragnarotico 7 points 4 years ago 

"If you can't tell the difference, does it matter?"

permalinkembedsavereport

[–][deleted] 1 point 4 years ago* 

See my issue with this is simple: acting. Do my DnD characters, if sufficient effort is put into them, become sentient? Would it then be manslaughter for me to stop playing DnD? Do I use bogus philosophical arguments to justify an unhealthy obsession? We may never know.

permalinkembedsaveparentreport

[–]bliceroquququq 25 points 4 years ago 

I enjoyed watching this but have always found the Chinese Room argument to be somewhat facile. It’s true that “the man inside the room” doesn’t “understand Chinese”, but the system as a whole quite clearly understands Chinese extraordinarily well.

To me, It’s like suggesting that since an individual cluster of neurons in your brain “doesn’t understand English”, then you as a person don’t understand English, or lack consciousness, or what have you. It’s not a compelling argument to me.

permalinkembedsavereport

[–]MmePeignoir 5 points 4 years ago 

but the system as a whole quite clearly understands Chinese extraordinarily well.

It boils down to what you mean by “understand”. You clearly are framing “understanding” in functionalist terms - if you can perform functions related to the language, if you can use the language well then you “understand” it. Searle is using a different definition, with “understanding” similar to “comprehension” - there’s a component of subjective experience in it, and it seems absurd that the man and the room as a whole can have the subjective experience of “understanding”.

permalinkembedsaveparentreport

[–]cowtung 9 points 4 years ago 

When I'm coding up something complicated, very often the solution to how I should do something just "comes" to me. It wells up from within and presents itself as a kind of image in my mind. My conscious mind doesn't understand where the solution came from. It might as well be a Chinese Box in there. The human perception of "understanding" is just a feeling we attach to the solutions our inner Chinese Boxes deliver to the thin layer of consciousness claiming ownership over the whole. It isn't so much that the Chinese Box as a system understands Chinese. It's that human consciousness doesn't understand Chinese any more than the Chinese Box does. We could take a neural net, give it some sensory inputs, and train it to claim ownership over the results of the Chinese Box, and it might end up believing it "understands" Chinese.

permalinkembedsaveparentreport

[–][deleted] 8 points 4 years ago 

Searle is using a different definition, with “understanding” similar to “comprehension” - there’s a component of subjective experience in it, and it seems absurd that the man and the room as a whole can have the subjective experience of “understanding”.

This definition presupposes that consciousness is not emergent and is binary rather than granular. Of course if you presuppose that consciousness cannot emerge from something simpler and that more complex consciousness cannot be created by combining elements that are simple enough to comprehend, then you'll conclude that consciousness cannot emerge from a system.

It's completely circular.

permalinkembedsaveparentreport

[–]MmePeignoir 2 points 4 years ago 

This definition presupposes that consciousness is not emergent and is binary rather than granular.

Binary and granular are not mutually exclusive. Either you have consciousness or you don’t. Sure, some things might be more conscious than others, but that doesn’t mean you can’t ask a yes no question. Unless you want to say everything is at least a little bit conscious and nothing is not conscious at all, and there, we’re back to panpsychism.

Saying that consciousness is “emergent” is meaningless. Traffic is an emergent property of cars. Fluid dynamics are emergent from liquid particles. But if we understand everything about each individual car and its movements, we will understand traffic completely. If we understand everything about each individual liquid molecule, we will be able to understand the fluid completely. There is nothing left to explain.

This is not the case for consciousness. We may be able to understand everything there is to understand about physics and particles and neurons and their workings, and be able to perfectly explain the functions and behaviors of the brain, yet still fail to explain why we have genuine consciousness instead of being p zombies. There’s an explanatory gap there. This is the hard problem of consciousness.

I’m not saying that consciousness cannot be studied scientifically, but purely physical rules about particles and fields and so on cannot adequately describe consciousness. We need a new set of rules to do that.

permalinkembedsaveparentreport

[–][deleted] 6 points 4 years ago 

If we understand everything about each individual liquid molecule, we will be able to understand the fluid completely.

I'd disagree at this point. Molecules are not fluids, it seems more trivial because wee actually understand it, but the same distinction between the molecules and the fluid or the cars and the traffic applies, a single car doesn't contain an atom of a traffic jam.

It could be that an iota of sense of self is a property of everything, and only becomes apparent in aggregate (ie. pansychism).

It could be that consciousness is a property of information, and not stuff, and the model describing the neurons and their workings will be exactly as conscious as the brain(s) it describes -- ie. that consciousness is a necessary property of certain types of collections of rules.

(You could also combine the above two)

These are both fully compatable with a materialist worldview, and amenable to scientific methods, and far more compelling than vague handwaving about science being insufficient.

permalinkembedsaveparentreport

[–]Dziedotdzimu 1 point 4 years ago 

That's a well reasoned response but if you want emergentism you'll need psycho physical laws to account for mental causation. Jaegwon Kim has a good section on mental causation and psychophysical laws in his book "Philosophy of Mind".

Given the causal closure of the physical world, and that minds can have causal efficacy, minds do what they do by virtue of their physical properties and the laws of physics and you don't need to invoke irreducible laws of mentality which enter into existence with the proper level of complexity or information feedback.

Panpsychism/Neutral monism actually criticizes the notion of "cold/innert" matter and makes the claim that the power to be perceived subjectively perceived is part of the ontology of matter itself. Not of an an emergent essence or non physical one. Check out Donald Davidson theory of mental events for and how it might relate to the supervenience argument. oops my bad. I misread your point

permalinkembedsaveparentreport

[–]Crizznik 6 points 4 years ago 

What's absurd to me is the idea that you can have a "rule book" that can intelligibly incorporate all possible responses to any possible question that doesn't just teach the "CPU" the language necessarily. To me, being able to respond in such a way is indistinguishable from understanding the language. Also, even if this were a good argument, it would be an argument against the Turing test being a suitable test for intelligence, not against the existence of strong AI.

permalinkembedsaveparentreport

[–]bliceroquququq 1 point 4 years ago 

But your brain is quite literally composed of billions of “Chinese Rooms”: individuals neurons getting signals from other individual neurons, each one firing or not firing in a purely mechanistic way based on factors like timing, input frequency, action potential, availability of neurotransmitters, etc.

None of these “Chinese rooms” in your brain have any “understanding” of what they’re doing, but their collective actions make up all the “subjective experience” and “understanding” in your head.

permalinkembedsaveparentreport

[–]ryanwalraven 1 point 4 years ago 

If I understand the word "chair" because it conjures images or memories in my mind, is that really different than the computer looking up the word chair, whose encyclopedia / rules entry contains photos, and then printing out the Chinese characters that were asked for? What can the "understander" do that the computer can't? As others have brought up, machine learning can even teach a computer to recognize photos of chairs. Sure, it will get confused sometimes, but humans can also argue over whether things like bean bags or couches or stools are technically chairs, or where burritos are sandwiches or whatever.

permalinkembedsaveparentreport
load more comments (2 replies)

[–]CommissarTopol 6 points 4 years ago 

The illusion of mind emerges from the operation of the rule book.

In the case of the human mind, the rule book has been created by a long process of evolution. Humans that had a defective rule book didn't reproduce that rule book further. And humans that had mutually compatible rule books that also promoted survival, could propagate those rule books.

The illusion of the Chinese Room emerges from philosophers over estimating their role in the operation of the Chinese Room.

permalinkembedsavereport

[–]Gullyvuhr 4 points 4 years ago 

This is an older argument that predates some of the newer applications of machine learning algorithms -- but, in all, I would challenge the idea of "meaning" that he says is unique to the human mind. Meaning, at it's core, is just a value assessment, and the value assessment is either unnecessary for the sorting task (looking for similarity in the symbol) or given to the application by the programmer (put A here, and B over there). Applications tend to have a specific task to accomplish, and if meaning isn't needed for the task why would it be there? I think this represents something the mind does that applications are not needed to do in their role as a tool -- but not needed =! never will.

I'd also say ML, when you start talking about prediction/prescription throws this into disarray -- and let's take epidemiology as our example. When we're talking about transmission vectors, or early detection of high risk cancer, or any use case where you're looking at mountains of data and the application is parsing the data, defining the dimensions, weighting them, reducing them, and weighting them again (even through something like MLP) then it is coming up with a mathematical value assessment which I'd say is "meaning" in the specific context of the question being asked/answered.

permalinkembedsavereport

[–]lurkingowl 1 point 4 years ago 

That's true, but if the argument were valid, it should apply equally well to current ML. The fact that it doesn't really work there means it was invalid all along.

I do think that a lot of the semantic/syntactic distinction is based on good old fashion AI approaches/programming. It breaks down pretty hard under more modern ML approaches, where the models are literally learned from interacting with the world.

permalinkembedsaveparentreport

[–]metabeliever 28 points 4 years ago 

For what its worth I've never even understood how this is supposed to make sense. Its like he's saying that because the cells in my brain don't understand English, then I don't understand English.

This argument splits people. To some it is obviously right, others, obviously wrong. Daniel Dennett calls this argument, and ones like it intuition pumps .

permalinkembedsavereport

[–][deleted] 8 points 4 years ago 

95% of philosophy is intuition pumps, especially when philosophers try to confront topics not in their field.

permalinkembedsaveparentreport

[–]Crizznik 3 points 4 years ago 

That is what Dennett said. That intuition pumps are by themselves not bad, they are useful to communicate complicated philosophy, it's just prone to being abused. And is not often abused intentionally.

permalinkembedsaveparentreport

[–]MmePeignoir 2 points 4 years ago 

Its like he's saying that because the cells in my brain don't understand English, then I don't understand English.

That’s not what he’s saying.

To Searle, being conscious is a prerequisite of being able to understand something. Sure, you’re conscious - we don’t know exactly why - but it seems absurd to assert that the man and the room as a whole is somehow conscious , i.e. has subjective experience, unless you subscribe to some form of panpsychism.

For what it’s worth, you’re right that it’s not going to be too convincing to people with strong physicalist/functionalist intuitions. I find those people similarly absurd - Dennett’s Consciousness Explained really reads more like Consciousness Explained Away , with him denying the obvious existence of subjective experience as distinct from physical phenomenon, but that is that.

permalinkembedsaveparentreport

[–][deleted] 12 points 4 years ago 

but it seems absurd to assert that the man and the room as a whole is somehow conscious

What? Why? The only absurdity is randomly ruling it out for no reason.

permalinkembedsaveparentreport

[–][deleted] 2 points 4 years ago 

Something something burden of proof

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

Except the thesis is "Strong AI is categorically impossible" not "I can create a mind".

permalinkembedsaveparentreport

[–][deleted] 1 point 4 years ago 

True, easy to forget

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

Additionally, 'machines/information systems can have minds' is -- from some points of view -- arguably the null hypothesis rather than any positive claim.

If you accept that minds exist (which hopefully everyone engaging in this context does at least for the current purpose), then asserting that biological human (or mamallian/vertibrate/vertibrate+cephalopod depending on POV) brains are special and different and have properties that no information process or machine can have is the positive claim.

When every other phenomenon we've encountered has varying degrees of evidence (and no solid evidence against) that it can in principle be modeled arbitrarily accurately by an information process, then Occam's razor doesn't outright say you should start from a position of consciousness being an information system, but does walk up to winking and jabbing you in the ribs repeatedly.

permalinkembedsaveparentreport

[–]MmePeignoir 1 point 4 years ago 

Yes, indeed, the room as a whole could be conscious - but then, as I said, you’d need to subscribe to some sort of panpsychism. If a room can be conscious than all sorts of things can be.

To be conscious is to have subjective experience, to “have something it is like to be you” - for instance, we can imagine that there is something it is like to be a bat, but not so easily for a rock. What would it be like to be this room man system? What kind of subjective experience would it have?

permalinkembedsaveparentreport

[–][deleted] 4 points 4 years ago 

Yes, indeed, the room as a whole could be conscious - but then, as I said, you’d need to subscribe to some sort of panpsychism. If a room can be conscious than all sorts of things can be.

So...? This isn't a disproof. It's putting the cart before the horse. "If brains are not special and unique the brains are not special and unique" isn't an insight.

To be conscious is to have subjective experience, to “have something it is like to be you” - for instance, we can imagine that there is something it is like to be a bat, but not so easily for a rock.

Physics has shown over an over again that not being able to imagine something doesn't make it false.

What would it be like to be this room man system? What kind of subjective experience would it have?

Don't know, maybe it's completely alien, maybe it's a lot like being human I have a bunch of thoughts, and there are a bunch of books. It's beyond the scope of 'is it categorically impossible' though.

permalinkembedsaveparentreport

[–]stevenjd 3 points 4 years ago 

If a room can be conscious than all sorts of things can be.

Um, yes? Isn't that the point of Strong AI?

Sentience is software. The material it is made of is, in a sense, irrelevant.

Of course in practice the material we are made of does have an impact. We're not brains in vats, we are biological beings. Our experiences are constrained by our biology and evolution. Our moment to moment thoughts are probably 98% unconscious and only 2% under conscious control, and our thinking is heavily influenced by our internal chemical environment: hormones, state of health, drugs and alcohol. We can't even imagine what it is like to be a dog , not very well, and we've spent thousands of years breeding them to be as close to us as we can make them.

Our failure to be able to understand what it would be like to be an artificial AI, or a alien, is hardly surprising. If you are religious, you probably accept that you will never understand what it would be like to be god.

But that failure of our imagination does not rule out the existence of non human intelligence. On this very planet, there are dozens or hundreds of species that display greater or lesser intelligence, including simple forms of language, emotion, learning, etc. That alone proves that human intelligence is a matter of degree, not kind.

we can imagine that there is something it is like to be a bat, but not so easily for a rock

There is little evidence for bats being sentient, or at least not very sentient. Douglas Hofstadter's "I Am A Strange Loop" talks about a hierarchy of sentiences, bats are not likely to have much, if any, sense of self. But rocks? Being a rock is not like being anything.

permalinkembedsaveparentreport

[–]Crizznik 6 points 4 years ago 

It's not absurd, nor is it panpsychism. It's materialism at it's core, that there is nothing special about the human brain, consciousness, or subjective experience. All you need is to have the ingredients, and you're conscious, and the ingredients aren't all that special, just very complex.

permalinkembedsaveparentreport
load more comments (15 replies)

[–]stevenjd 1 point 4 years ago 

Its like he's saying that because the cells in my brain don't understand English, then I don't understand English.

That’s not what he’s saying.

That is precisely what he is saying, except "Chinese" not English.

He literally says that because he (the component of the system which does nothing but look up symbols in a huge book of rules) doesn't understand Chinese, the system as a whole doesn't understand it. Did you watch the video clip?

At 2:30 he lays out his definitions:

"The rule book they call the computer program and the boxes of symbols that I use they call the computer database. Me, they call the computer, or if you like, the central processing unit."

The CPU (the man in the Chinese Room) is just one component out of a greater system, i.e. in u/metabeliever 's analogy a mere cell in a brain.

At 3:05 he comes to his conclusion:

"My answers are indistinguishable from a native Chinese speaker. Okay, now here is the point: no matter how good the program, no matter how effective I am in carrying out the program, and no matter how my behaviour simulates that of a Chinese speaker, I don't understand a word of Chinese. And if I [i.e. if the CPU or cell] don't understand Chinese on the basis of implementing the program, neither does any other digital computer on that basis."

Searle then goes on to claim that the difference between his answering questions in English versus Chinese is that he understand English but he doesn't understand Chinese.

Sure. And neither does the CPU in Searle's brain (the neurons and other cells) understand English either, since his CPU is nothing but a lump of meat with the consistency of cold oatmeal.

With the wrong database (say, a decade of Russian but no English) Searle wouldn't be able to answer questions in English either.

Without the right software (as in children who are raised by wild animals, or in conditions of awful neglect where they are locked up from infancy and not exposed to human contact and language) he wouldn't be able to answer questions in English, or any other language.

Without the right software and database, Searle's lump of meat would not be very impressive.

Searle's argument fails because it applies equally to the human brain as to a digital computer. In Searle's argument, he is the CPU, and the CPU alone doesn't understand Chinese. True.

But Searle's lump of meat alone doesn't understand English either. It needs software and a database. It needs to learn English, just as the Chinese Room as a whole needed to learn Chinese.

If you think Searle's argument rules out strong AI in metal and silicon, then it also rules out strong AI in meat.

Suppose we asked Searle, in English, "Do you understand this sentence?" and he replied "Yes." But why did he answer that? Searle would say that he said "Yes" because he actually does understand it, but that's exactly what he would say if he was the man in the Chinese room . Why should we believe him speaking in English but not the room as a whole speaking in Chinese? Maybe Searle is mistaken about understanding English. Or lying. How would you know the difference?

Think about how complicated and intricate the rules of the Chinese room must be. When asked, in Chinese of course, "Do you understand this sentence?" the rules must involve not just a dumb look up table that says "Yes", it must involve a whole lot of extra processing: check the questioner's tone of voice for sarcasm, decide whether it is appropriate to make a joking response "No, I don't speak Chinese" in perfect Chinese, etc. We know that the program plus database is that sophisticated, because Searle's answer is indistinguishable from a native Chinese speaker.

Searle in this process is the least important part of the Chinese room. He could be replaced by a robot, or a trained monkey, or a pigeon. All the intelligence is in the program! His part in it all is just to be a dumb mechanical machine that looks up rules and follows them. Of course the CPU doesn't understand anything, just like his meat, his neurons and cells in the brain, likewise respond mechanically to electrical and chemical signals and understand nothing. If this neuron fires, fire that neuron in return.

In his story, Searle (the CPU) doesn't understand Chinese. And his neurons (likewise the CPU in this analogy) don't understand English either.

But just as the whole system of neurons + software + learning (memory) understands English, so the whole system of the room, the rules, and the CPU understands Chinese.

permalinkembedsaveparentreport

[–]metabeliever 1 point 4 years ago 

See, you're the kind of person I can agree to disagree with. My intuition is so unilateral that I can't see from your perspective. Which just has to be a limitation on my part.

Plus what I always found frustrating about these arguments in class was that I was dead sure I was right. 100% certain that my opponent was wrong. And that the whole thing didn't matter at all. Which is a lot of strong feelings to hold at once.

permalinkembedsaveparentreport

[–]ryanwalraven 1 point 4 years ago* 

but it seems absurd to assert that the man and the room as a whole is somehow conscious, i.e. has subjective experience, unless you subscribe to some form of panpsychism.

This seems like kind of a weak argument, imho, when it requires a conscious person to write a massive and complex rule book to trick the consciousness test. That's where it falls apart for me -- it's not possible for the person in the room to pass the test and "fool" the test committee without some other humans doing a ton of work beforehand. It's essentially like those other humans are there to answer the questions because they've done such a good job writing the rules, or the program.

Certainly, the rule book is not conscious, but then we have to insert a human to manage the rules (on both ends) to make it work. Like... you say it's absurd that the man and the room is somehow conscious... but the man IS literally conscious and has subjective experience.

permalinkembedsaveparentreport
load more comments (14 replies)

[–]macemillion 1 point 4 years ago 

I don't see how anyone could see this as OBVIOUSLY correct or incorrect, we have no way of knowing with certainty. I do seriously doubt this is remotely accurate but I don't have a way of knowing that. Why are even supposedly great philosophers so determined to come to an arbitrary conclusion, when it's perfectly acceptable to not know something?

permalinkembedsaveparentreport

[–]metabeliever 2 points 4 years ago 

Sorry, I mean "seems obviously". The chinese room has always, immediately Seemed obviously wrong headed to me. Which is worth almost exactly nothing.

​

Oh, man, don't ask that too loudly, these guys tenure is on the line.

permalinkembedsaveparentreport

[–]lurkingowl 16 points 4 years ago* 

I don't think there's any discussion around this that's likely to change people's opinions on the core question. But can we discuss the fact that the argument is a giant strawman, attacking a position no one actually holds?

The "Systems Reply" is the position he needs to argue against, and trying to shuffle it into a footnote has always seemed very disingenuous to me (in addition to his "I'd just memorize it" "argument" being a completely unconvincing response, but let's set that aside.) The whole thing feels like a giant misdirection.

permalinkembedsavereport

[–][deleted] 7 points 4 years ago 

It was a novel thought experiment and it is still useful, but the systems reply has refuted his argument.

The basic error is that the argument performs a slight of hand to reduce an actual human to a CPU and then claims a CPU is not a human.

The argument is that, because a human can perform simple tasks that a CPU can do, the CPU is not AI. Does not really make sense.

A single neuron is also not AI.

The thought experiment is useful in that is illustrates how intelligence is an emergent property of a complex system. None of the individual components are intelligent by themselves.

permalinkembedsaveparentreport

[–]lurkingowl 3 points 4 years ago 

My problem is, he mentioned "the systems reply" in his original paper . He knew that's what he actually had to argue against, but set up this strawman argument (the single neuron analog as you say) to declare victory before even talking about anything resembling the idea he claimed to refute.

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

I hate to be the one to tell you, but this happens frequently in Academia.

Academics know the weakness in their argument, but they still champion their argument and it is up to others to react.

And in a way this is good. It is better to have an academia where people are willing to argue different positions than to only have a consensus culture.

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago 

the argument is a giant strawman, attacking a position no one actually holds

There are many people who believe that complex AI is the same as Strong AI. I'm not sure how you came to the conclusion that there isn't a single person who thinks that.

He didn't start with his argument to "declare victory", his whole argument is a counter point. The systems reply is a counterpoint to his counterpoint. Are you upset that he laid things out in a logical order?

You would've rather he argued the counterpoint to his counterpoint without first establishing his original counterpoint? Lol what?

permalinkembedsaveparentreport

[–]lurkingowl 3 points 4 years ago 

There are many people who believe that complex AI is the same as Strong AI. I'm not sure how you came to the conclusion that there isn't a single person who thinks that.

I believe that. That's not what the pre systems "reply" thought experiment argues against. In the main thought experiment he argues that some small subsystem ("him"/ the paper pushing piece) doesn't understand Chinese. No one believes that, or believed at the time that ignoring all of the actual software, data being tracked, data accumulated in the program, etc, would still somehow count as understanding Chinese.

The "systems reply" is the belief he needs to argue against. Trying to shuffle it into a response to his argument is misdirection from the fact that the main argument is against a strawman.

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago 

No one believes that, or believed at the time that ignoring all of the actual software, data being tracked, data accumulated in the program, etc, would still somehow count as understanding Chinese.

You're misunderstanding the analogy then. The instruction set that the worker is following in the box is the actual software. The data being tracked and data accumulated (same thing, different words) are also part of the software, and therefore part of the instruction set.

I'm not sure why you thought the instruction set and software were two separate things, but they are most certainly not. That's what software is. It's a big set of instructions for the cpu. The CPU is the worker in the analogy.

​

The "systems reply" is the belief he needs to argue against. Trying to shuffle it into a response to his argument is misdirection from the fact that the main argument is against a strawman.

It's literally called the systems REPLY because it's a direct response to the Chinese box argument. He's not reshuffling it into a response, it is the response .

Again you're acting like he shouldn't have laid out the basis of his argument before responding to counter points. That's not how this works.

permalinkembedsaveparentreport

[–]lurkingowl 2 points 4 years ago 

It's literally called the systems REPLY because it's a direct response to the Chinese box argument.

And I'm saying the core argument (without the systems reply and Searles' counter reply) is a strawman. No one thinks the CPU/Worker "understands Chinese", so the baseline argument is a strawman.

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago 

Even if it were a strawman (it's not, but I'm addressing this in the other reply. Not sure why you needed to split the threads on this), you still would lay out your base argument before addressing responses. That's just how a debate works. You don't just jump straight into debating the other side without first laying out your side of things.

You may not like it, but that's how it be.

permalinkembedsaveparentreport

[–]lurkingowl 1 point 4 years ago 

(I was trying to split the computer architecture stuff from the strawman or not discussion, but it didn't come across well. :))

Let's say Searle had stopped his original paper without mentioning the systems reply, and without addressing the idea that the understanding is tied up in the software and the non Searle parts of the system. Do you think that would have been a strawman argument?

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago 

No, I don't think it's a strawman. The beginning of the video states that he's arguing against idea that a computer that's functionally identical to a human mind is the same as a human mind. That the two are both "minds". Or more specifically, that the chinese box actually understands chinese.

The first thing the system reply does is point out the flaw in the analogy, that the cpu doesn't understand, but that says nothing about the entire box.

It then says that the box, as a sum of its parts, does understand. That right there prevents Searle's argument from being a strawman. It would only be a strawman if Searle was arguing against something that nobody actually believed, but the system reply double's down on that belief. The entire basis of the system reply is what Searle was originally arguing against.

permalinkembedsaveparentreport

[–]lurkingowl 1 point 4 years ago 

The CPU is the worker in the analogy.

And the (non systems reply) argument is only focused on whether the worker/CPU understands Chinese. Beyond explaining how the software could be in a big book, he completely ignores it for the argument.

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago 

No it's not. It's focused on the computer itself, not just the cpu. He says it in the first 40 seconds of the video. I suggest rewatching that bit.

He's arguing against the theory that a computer smart enough to simulate the human mind can be considered the same as an actual conscious mind.

The chinese box (everything encapsulated within it) doesn't understand chinese either, it just pumps out accurate translations. It's not the same as a conscious human understanding chinese and translating. If it were, we should probably be giving google translate a few days off every now and then. You know, cause of labor laws and all.

permalinkembedsaveparentreport

[–]lurkingowl 3 points 4 years ago 

No it's not. It's focused on the computer itself, not just the cpu.

The argument is against the idea of the whole computer, but the actual argument is that Searle himself, executing the instructions and passing notes to himself, doesn't understand Chinese. In his own analogy, that's an argument that the CPU doesn't understand Chinese, not that the whole computer doesn't.

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago* 

Upon rewatching the video in it's entirety, I will admit that putting himself as the CPU instead of the whole machine is bit of a mistep in the analogy. I think this stems from lack of technical background on his part.

However, his overall point is definitely that the computer itself doesn't understand chinese, and he states it pretty clearly multiple times outside of the analogy. I'm a programmer, so I'm kind of use to translating what people say into what they actually mean when it comes to technology, so I may have done that automatically here.

But again, his point is definitely aimed at the computer, not just the cpu.

Replace Searle saying "but I don't understand chinese at all" with "but the box doesn't understand chinese at all", and I think you'll get a better sense of what he's getting at.

Edit: So to bring it all back. Faulty analogy. Not a strawman, though.

permalinkembedsaveparentreport

[–]theologickal 1 point 4 years ago* 

The systems reply doesn't do anything, alot of the problem here is the average person dosent understand what a computer is.

For example, take a child from birth and instead of putting them through school or something, teach them instructions for manipulating symbols their entire life. This child does not understand Chinese unless they are actually exposed to concepts and semantics. The very fact I can make up a word "Lokerton" that clearly obeys the language syntax, and has its own intelligible place in the set of words in the sentence should be proof enough of this. It is a string of symbols empty of content, yet I can construct sentences with it. Similarly, a child can learn to manipulate symbols and signs like a game, without knowing what concepts they are referring to.

Another problem that you have to deal with is the fact that understanding is now observer dependent. Take the universe and construct a turing machine which takes in the state of the universe, then prints out some possible state of the universe at some T later. The physical description of the universe is the language here, but we do not then say that the Universe has understanding of what will happen next simply because I interpret the universe as this TM actualized in the real world.

permalinkembedsaveparentreport

[–]lurkingowl 1 point 4 years ago 

(This is a direct discussion of the argument/systems reply itself, which wasn't the point of my comment, but I'll happily respond here directly.)

The syntax/semantics distinction is problematic because Searle has so deeply coupled his definition/requirements of semantics with consciousness and intentionality. It was more useful against older AI paradigms, but falls very short of modern AI techniques. And because his arguments claimed to cover all symbol manipulation, they must have been wrong initially as well.

To give you two counter examples to think about:
Imagine a robot with a powerful set of machine learning algorithms built in. It develops sensory models of what it sees, feels, and hears in the world. It learns what objects are by picking them up and playing with them. It learns the definition of words by interacting with humans and getting corrected. It comes into the room you're in and says "There's an apple in the next room." It seems perverse to say that this statement doesn't have semantic content. Searle spins a web of "derived intentionality" to basically say "lalala, I can't hear you" to the idea that this robot's statements don't have true semantic content.

Second example: Imagine a hundred of these robots set loose on a planet in another galaxy with no humans. They communicate entirely amongst themselves, and develop a language together. They convey information to each other about things humans have literally never seen. They develop concepts that humans have never thought of. It seems even more perverse to say that their statements have no semantic content. They're clearly referring to things in their environment.

Whether the definition of understanding is observer dependent seems pretty unrelated, and is a broader question. I would say that understanding requires an internal model of the world, sensory connections between the internal model of the world and an external model, and at least some ability to simulate the external world (in faster than external world speed) with a fidelity that lets the understanding system predict the sensory inputs from the future state of the world with some accuracy. The double mapping of sensory states from the internal world to the external world is key here. So we wouldn't say the entire universe has understanding. But we could draw clear lines around a system and its sensory inputs from outside the system, to determine if it's got an internal model that's predicting external stimuli at faster than real time. If it is, that seems like a good first cut at saying the system is understanding the outside world.

permalinkembedsaveparentreport

[–]theologickal 1 point 4 years ago 

The syntax/semantics distinction is problematic because Searle has so deeply coupled his definition/requirements of semantics with consciousness and intentionality. It was more useful against older AI paradigms, but falls very short of modern AI techniques. And because his arguments claimed to cover all symbol manipulation, they must have been wrong initially as well.

this does not mean a system which performs an algorithm is has intetionality. AI methods have changed but they are still algorithms, computation has never not been algorithms, so the turing machine model does cover all symbol manipulation.

" Imagine a robot with a powerful set of machine learning algorithms built in. It develops sensory models of what it sees, feels, and hears in the world. It learns what objects are by picking them up and playing with them. It learns the definition of words by interacting with humans and getting corrected. It comes into the room you're in and says "There's an apple in the next room." It seems perverse to say that this statement doesn't have semantic content. Searle spins a web of "derived intentionality" to basically say "lalala, I can't hear you" to the idea that this robot's statements don't have true semantic content. "

It says theres an apple in the room because you specifically set it up to do that. Youre not understanding what I'm saying: the semantic content is coming from YOUR mind. The interpreter of the computer program is the one that makes the semantic jump.

Think about why it is possible to make computers in Magic the gathering, the physical world, and minecraft and so on. It is because these systems are collections of rules on objects, which when a mind understands how these rules work, they can use symbols or signs or objects as "stand ins" for concepts that are not physical. By understanding these concepts rigorously enough, aswell as understanding the rules you are playing with, you can solve a problem by collecting concepts together and relating them together such that the relationship between them in thought matches how they are related in the rules you are working with. This is the essence of what a computer actually is.

When you realize that a computation is just a Turing machine actualized in some way, whatever mind like reality emerges from a computation is incredibly dubious.

Another argument:

What if there are two Chinese people. But only one of them has the key to decode the binary string the computer outputs as a mapping to the Chinese language. Here, every computation is exactly the same, but one person gets ones and zeros that they cant understand, and one person known how to interpret what the machine is saying. If the machine has understanding, then it either knows Chinese or not. Yet It can't speak with the first person who is speaking chinese.

permalinkembedsaveparentreport

[–]lurkingowl 1 point 4 years ago 

It says theres an apple in the room because you specifically set it up to do that.

No, I set it up to learn. It learned what an apple is, and learned how to use language, and then told me about the apple. The "robots on another world" example is an extension of that idea. No person is telling these robots what their language means. (Separating my replies some since these threads are getting big.)

permalinkembedsaveparentreport

[–]lurkingowl 1 point 4 years ago 

this does not mean a system which performs an algorithm is has intentionality.

It doesn't automatically have intentionality, no. But it can gain intentionality if it's causally connected to the world through sensory inputs, learning, and interactions. An algorithm interacting with the world and learning from it gains semantic content, and can do so without any human mediation.

permalinkembedsaveparentreport

[–]advice_scaminal 4 points 4 years ago 

Searle reminds me of Lee Sedol before he played AlphaGo.

permalinkembedsavereport

[–]DankBlunderwood 3 points 4 years ago 

He has been taught English just as the computer has been taught Chinese. His lack of knowledge of the biochemical mechanics of neural pathway creation and language acquisition in humans does not change the fact that what he perceives as "meaning" is not meaningfully distinct from a strong AI's ability to acquire Chinese. They differ only in method, not result.

permalinkembedsavereport

[–]ChaChaChaChassy 3 points 4 years ago* 

...as usual a lot of philosophy is wasted time due to an insufficient understanding of science.

If the Chinese Room maps every input to 1 and only 1 output it won't even APPEAR to know Chinese... it won't pass the Turing test, and is indeed a "dumb" system. So we must assume that the Chinese Room can carry on a conversation in Chinese where the same input can lead to differing outputs based on historical context, and this mandates not only a rule book but a database of past experiences (we would call them memories). In this case, whether he knows Chinese or not is irrelevant because the mechanism as a whole that he is a part of DOES. No one is saying every transistor/neuron has to understand the language that it's helping to speak...

permalinkembedsaveparentreport

[–]bitter_cynical_angry 31 points 4 years ago* 

I've sometimes wondered if the summaries I've read of the Chinese Room argument are really accurate, but here is the nonsense straight from the horse's mouth. The real thing is indeed just as silly as the summaries make it sound.

I've never understood why such an obviously flawed argument that rests on such clearly misinterpreted principles has become so persuasive and influential. It's like the philosophical zombie argument: ridiculous, and yet extremely attractive to people who don't like the idea that there's nothing inherently special about the brain that can't be done by the right arrangement of physical objects and interactions. That arguments like these are taken seriously and debated at great length decrease my respect somewhat for the field of philosophy.

Now that I have your attention, I'd be happy to go into much greater detail on why the Chinese Room is wrong, but you can find a quick takedown in Wikipedia under the Systems Reply section. I assume the other criticisms are equally destructive, but one suffices.

I will briefly add that one obvious flaw in the argument that is nevertheless often ignored is that the rule book and database are presented by Searle as unchanging static look up tables. But if the Chinese Room is able to reply in a way indistinguishable from a human , then it must be able to change its rule book and its database in response to the inputs. The answer to "What time is it" changes every second or minute. The answer to "What did I just ask you?" changes with every new question. The answer to "Do you prefer chocolate or vanilla" is something determined by not just one rule and one set of data but probably hundreds of rules and thousands of pieces of data that are constantly being modified in response to inputs. A Chinese Room that couldn't do that would almost immediately fail to impersonate even a very dumb human being. The human in the room is by far the least interesting and least important aspect of the argument. The mind that understands Chinese is obviously this amazing ever changing rule set and database.

Edit: To clarify, I'm not intending to attack the OP. To the contrary, I upvoted it and I'm thankful for the video because it's great fodder for my own philosophical arguments.

Edit 2: Autocorrect typo.

permalinkembedsavereport

[–][deleted] 8 points 4 years ago 

Sealre is basically arguing "MAN IN ROOM WITH A STRONG AI RULESET IS NOT STRONG AI". Like, yeah buddy, nobody is arguing that the hardware is what makes humans or AI interesting, it is the software. I'm in the same boat as you, a childish fallacy being taken seriously casts a huge stain on philosophy as a serious field. Searle simply doesn't understand what he is talking about. I honestly wonder if he hasn't realized he's wrong by now. It would be really troubling if he hasn't.

permalinkembedsaveparentreport

[–]bitter_cynical_angry 7 points 4 years ago 

I think, just from a basic human nature standpoint, that it's actually impossible for him to admit he's wrong, even if he ever actually comes to believe that himself, which I doubt he will. To paraphrase Planck: Philosophy, like science, advances one funeral at a time. It's not the way it ought to be, but it's the way it is.

permalinkembedsaveparentreport

[–]Digit117 1 point 4 years ago 

Ha that's hilarious, I can't believe I've never heard of that phrase until now.

permalinkembedsaveparentreport

[–]stevenjd 2 points 4 years ago 

A very good analysis, but I don't think your argument about the rule book follows. Searle does allow that the rule book is as sufficiently complex as needed. It's not necessarily just a dumb static lookup table where you look up keywords and then give a response. There could even be further inputs to the system. We should give Searle the benefit to assume that he would allow extra inputs, and memory, otherwise the Chinese Room couldn't answer questions like "What time is it?", "Is it dark outside right now?", or "What was your answer to the question I asked you a moment ago?"

Since Searle says that the Room is indistinguishable to a sentient Chinese speaker, who presumably is able to answer such questions, we have to allow the Room to do the same.

But even granting Searle that benefit, it's a lousy argument that doesn't deserve to be taken seriously, let alone as a refutation of Strong AI.

permalinkembedsaveparentreport

[–]bitter_cynical_angry 2 points 4 years ago 

It looks like both you and I have now said elsewhere in these comments that if the rulebook and dataset are allowed to be self modifying and complex enough to answer indistinguishably from a human, then Searle has actually shown that either the Chinese Room does understand Chinese, or that he himself doesn't understand English.

permalinkembedsaveparentreport

[–]taboo__time 3 points 4 years ago 

When I first heard the idea I thought it was a very clever, smart way of exploring the idea of intelligence, understanding and AI. Not an actual refutation.

Then when I heard him more I realised he actually thought he's resolved the question. My opinion of him went down a lot.

permalinkembedsaveparentreport

[–]bleglorpaglorp 1 point 4 years ago 

The rule book can be static but in order to function as described, would have to essentially simulate the entire situation and coincidentally line up exactly with the real world inputs given. At that point a deterministic world is assumed so that whole free will can of worms is opened as well.

I think its directly evident theres something magical about our existence, its consciousness itself. Pzombies and chinese room and the like are all attempts at grounding consciousness to the ego, insisting that the same magic that is experience itself must also be part of personality as we experience it. But its not, its a fundemental property of reality, of existence.

permalinkembedsaveparentreport

[–]bitter_cynical_angry 5 points 4 years ago 

The rule book can be static but in order to function as described, would have to essentially simulate the entire situation and coincidentally line up exactly with the real world inputs given.

I don't think even that suffices. If the rule book and data are static, then they would have to be nearly infinite in size (or maybe actually infinite, I'd have to think about that a bit more, but certainly astronomically huge in any case). Keep in mind that if it's perfectly static, there could be no memory of previous questions and answers that the person the Room is talking to has actually asked, so if they say, "What did I just ask you?", the Room has no way to give the answer unless it stored the previous question. And that's just the tip of the iceberg of the dynamic changes that would be required to answer indistinguishably from a human.

...But its not, its a fundemental property of reality, of existence.

Like panpsychism ? That's kind of an interesting idea but I've never been quite sure how it could be demonstrated, since it seems like universe that is panpsychic and one that is purely materialist would be indistinguishable, and introducing another layer of explanation is unnecessary in that case.

permalinkembedsaveparentreport

[–]bleglorpaglorp 2 points 4 years ago 

I guess, I've come lately to not viewing it as panpsychism but rather just the obvious only choice and same way other things exist. Consciousness exists everywhere but so does water, but we only consider something "wet" when enough water is present to be noticable. The universe is not pan wateristic, or I guess it is, but the language seems odd. It has the potential for water to be present at any point in space in time.

permalinkembedsaveparentreport
load more comments (4 replies)
load more comments (6 replies)

[–][deleted] 1 point 4 years ago 

It doesn't have to be dynamic, it could just have a terabyte of pages (ie. the page number is a trillion bytes).

permalinkembedsaveparentreport
load more comments (2 replies)
load more comments (15 replies)

[–]TheHuaiRen 6 points 4 years ago* 

This won’t age well

Edit: /r/SubSimulatorGPT2

permalinkembedsavereport

[–]lafras-h 8 points 4 years ago* 

What Searl misses is that people outside the room are themselves in their own Chinese rooms(skulls). Their minds are doing the same computation from symbols in the world to paper as the AI does in the room from paper to paper. Instead of proving strong AI false, he proves consciousness false.

permalinkembedsavereport

[–]Revolvlover 3 points 4 years ago 

It's cool that Reddit comments can revive so much of the aftermath of the Chinese Room "argument" as if it weren't stale. I guess it's a tribute to Searle that it still gets people worked up, and that's enough. But it's long been beaten to death. Searle's take has virtually no adherents, then or now. (Less, now that he's in trouble.) It's just idiomatic Searle, that he can't himself quite explain.

"Original intentionality" - a cipher for what Searle doesn't understand about the CR problem he invented. As others have pointed out here, the intentionality of the room operator's oracle, the manual - is pretty damned hard to envision. Not least because it's supposed to encompass (let's say Mandarin) Chinese, a natural language that is not as cohesive as English. But let's say there is a canonically intelligible spoken Mandarin - it still presents special complications in the written form. One is tempted to think that Searle chose Chinese as the problematic on purpose.

Considering all that, there are obvious reasons why CR is still interesting. Firstly: most of the standard responses are intuitively obvious. Secondly: the standard responses still fail to address the thing Searle cares about. Thirdly: CR is a clever turn on the Turing test. It's an inversion. He thinks that no oracle of language understanding/knowledge is sufficient for a blind homunculus to be the one that understands.

permalinkembedsavereport

[–]Vampyricon 3 points 4 years ago 

But it's long been beaten to death. Searle's take has virtually no adherents, then or now.

Thank god. If there were a significant proportion of Chinese Roomers, philosophy would be in dire straits indeed.

permalinkembedsaveparentreport

[–]Chaincat22 3 points 4 years ago 

That's an interesting argument, but wouldn't you eventually learn some chinese? Writing or speaking it so long, would you not, eventually, learn some chinese just naturally? As babies, we don't really know meaning. We don't really know how to speak the language our parents speak, but we eventually start making the same sounds they make. And in turn we start to learn what those sounds mean. Ironically we learn what those sounds are as other sounds. What's preventing a machine from learning the same way we do? Would an AI truly not start to understand what it's doing? And if so, what's stopping it, and what might we have to do different to let it? Consciousness sprung from evolutionary chaos out of nowhere, surely we can recreate it, we're just doing something wrong.

permalinkembedsavereport

[–]pgrizzay 3 points 4 years ago 

We need a deepfake version of this

permalinkembedsavereport

[–]gsbiz 3 points 4 years ago 

It's a flawed premise. Following his theory if you setup a Chinese room inside a Chinese room you would still be unable to distinguish between a computer and a human. In the analogy a human doing the job of translating English because it has understanding could simply be another Chinese room with a vastly more complex instruction book that conveys meaning of combined symbols, which is all the human brain device does.

permalinkembedsavereport

[–]ockidocki[S] 15 points 4 years ago 

Searle presents here his celebrated Chinese Room argument. The delivery is entertaining and and a joy to watch, in my opinion. I hope you enjoy it too.

permalinkembedsavereport

[–][deleted] 7 points 4 years ago 

Seems to me like if the cards, book, person, and all the other props were all made out of circuits that talked to each other, that'd be an instance of strong AI. Searle's incredulity at the systems reply only has intuitive oomph because all the props in his experiment are different objects. If you turned them all into neurons that talked to each other that'd just be a brain.

permalinkembedsavereport

[–]al-Assas 6 points 4 years ago 

I don't get it.

The person inside the Chinese room is not the Chinese room. He's only a part of the mechanism. The Chinese room as a whole understands Chinese.

permalinkembedsavereport

[–]MidnightGolan 2 points 4 years ago 

His argument is that the Chinese room doesn't understand Chinese, at all, it just gives the perception of it.

permalinkembedsaveparentreport

[–]JDude13 2 points 4 years ago 

A distinction without a difference

permalinkembedsaveparentreport

[–]BubbaFettish 1 point 4 years ago 

The difference is the person in the scenario isn’t a whole mind, but more like part of a mind, like a single neuron.

Similarly each single neuron follows a set of rules (if the input goes in gets above a threshold it activates and sends signals out along it’s output, or whatever).

In the same way we can’t say the man in the room knows Chinese. We don’t say a single neuron in your head knows English. We do say the whole collection of brain cells in your head, your brain / your mind, knows English.

We can’t say the man in the room knows Chinese, but we can say everything in the Chinese room taken together knows Chinese.

permalinkembedsaveparentreport

[–][deleted] 3 points 4 years ago 

The argument is weak, unfortunately.

permalinkembedsavereport

[–]Irratix 2 points 4 years ago 

This is a far better explanation than I ever got in high school, but I maintain the same criticism of it I think. I do have trouble putting to words why I think this but it seems to me that Searle believes that computers only follow very predictable and intended command structures, such as "if A then do B". I think most AI researchers would find that somewhat reductive.

Most AI structures are designed with the idea in mind that us programmers are incapable of making well functioning algorithms to solve certain problems, and as such they are designed to learn how to solve these problems without humans knowing precisely what these structures are doing. Consider neural network structures. We can train them to solve certain problems, but at the end of it we have no idea what the neural network looks like or what each neuron is doing. It's just not following some kind of rulebook, as Searle describes.

I do believe I agree that a program passing the Turing test is insufficient reason to believe it is Strong AI, but I think I maintain the position that Searle's argument is insufficient to demonstrate this given our current understanding of AI structures.

permalinkembedsavereport

[–]HarbingerDe 2 points 4 years ago 

The argument, in my opinion, doesn't sufficiently demonstrate that what's happening in all of our heads isn't in effect just an algorithm that "matches symbols" and "follows rules".

Obviously that "algorithm" would be unimaginably complex and perhaps even impossible to ever understand or replicate, but I don't think this argument is very convincing.

permalinkembedsaveparentreport

[–]thelemursgonewild 2 points 4 years ago 

Omg luckily I found this post! I have to do an assignment in My studies of psychology about this. I have read the whole Argumentation over and over again but still can not really formulate an answer to those to questions: Do you think the distinction between real thinking and mere simulation of thought makes sense? Can you think of an example from psychology that explains why the distinction does/does not make sense. I have especially problems coming up with a good example. Help is greatly appreciated :)

permalinkembedsavereport

[–]ChaChaChaChassy 1 point 4 years ago* 

Here is the problem with this argument:

If the Chinese Room maps every input to 1 and only 1 output it won't even APPEAR to know Chinese... it won't pass the Turing test, and is indeed a "dumb" system. So we must assume that the Chinese Room can carry on a conversation in Chinese where the same input can lead to differing outputs based on historical context*. In this case, whether he knows Chinese or not is irrelevant because the mechanism as a whole that he is a part of DOES. No one is saying every transistor/neuron has to understand the language it's helping to speak...

*To understand what I mean by historical context consider if a person asked you the exact same question twice. In the first case, of the "dumb" system, the Chinese Room would answer the same way twice... but if the Chinese room actually understands Chinese it likely will not... Example:

"How are you today?"

"Fine, how are you?"

"How are you today?"

"Fine how are you?"

Versus:

"How are you today?"

"Fine, how are you?"

"How are you today?"

"You just asked me that, are you feeling okay?"

If the Chinese Room answers as in the second example then the Chinese Room is actually communicating and in that case it not only understands Chinese but it understands everything else necessary to use language... which leads us right back to the same question about whether there is a meaningful distinction between the appearance of understanding and actual understanding (which is, ultimately, about P Zombies...).

What he's doing by talking about a "rule book" is obfuscating the actual complexity of this... To actually be able to converse as a believable human would not be possible with a "rule book", it would be a "rule library" larger than every library on Earth combined with a historical record of every past interaction... at which point you've reached the complexity of the human brain. In fact this library is so large we have given up instantiating it to create AI and have realized that we must instead write software that can learn independently the same way actual humans do. The library must construct itself from experience... aka it must "learn". At that point what is the difference between that "rule library", historical record, and the mechanism that utilizes them and an actual brain?

permalinkembedsaveparentreport

[–]NoPunkProphet 2 points 4 years ago 

That distinction seems hugely arbitrary. I don't need to explain how I know something in order to know and practice it.

permalinkembedsavereport

[–]madpropz 2 points 4 years ago 

What if the meaning/semantics are just another set of more intricate rules inside the mind?

permalinkembedsavereport

[–]Keruli 1 point 4 years ago 

Then there are, by definition, no semantic entities, no meanings... His argument pre supposes that we have semantics and that semantics are distinct from syntax. This is a fairly popular view. If you disagree, then that's fine, but that doesn't make his argument wrong, just irrelevant/meaningless in your world/theory.

permalinkembedsaveparentreport

[–]Vampyricon 2 points 4 years ago 

This doesn't seem in any way analogous to how AI works. He seems to think that AI has a list of all possible questions and all responses hardcoded in as a giant lookup table.

And as many others mentioned, his argument proves too much: The individual neurons don't have understanding, and only fire at set frequencies when stimulated, so by Searle's logic we don't understand anything either.

permalinkembedsavereport
load more comments (1 reply)

[–]macemillion 2 points 4 years ago 

I don't understand this analogy at all, it seems to me like he is comparing apples and oranges. He even said the person in the chinese room is like the CPU, yet he's comparing that to the human mind? Shouldn't he be comparing that to the human brain? Our brain does have some basic instructions written into it but most of what we know we learn, essentially storing that information in a database and retrieving it later. How is AI any different?

permalinkembedsavereport

[–]lucidfer 2 points 4 years ago 

You can't reduce a system to a singular component and expect it to be a functioning model of the entire system.

My optic nerve doesn't understand the raw signal impulses that are being transmitted from photo chemical reactions in my eyes to the neurons of my brain, but that doesn't mean I'm not a fully functioning mind.

permalinkembedsavereport

[–]EVEOpalDragon 1 point 4 years ago 

I was thinking along this line while he was trying to explain his point. I don’t know that he has enough of a grasp on what the brain does/ what computer programs do . If he considers the “mind” a magical black box then his argument makes a kind of sense but it falls apart when you assume that he is in the black box following the rules “cpu” would not have to understand the meanings of the symbols/shapes in order to follow the rule book and output the response just as individual neurons do not need to know what the letter N looks like. The pattern recognition can be solved by groups of neurons using electrical connections and chemical relationships rather than transistor switches.

permalinkembedsaveparentreport

[–]ydob_suomynona 2 points 4 years ago 

Well eventually you'd learn the Chinese. But that's not the point since the answers you give come from the rulebook anyway (i.e. someone else's mind).

Pretty sure the syntax computers use does have meaning to them, that's quite literally part of the definition of syntax. Even things that a computer receives and recognizes as not syntax have meaning to it. As long as it's an input it should have meaning. It's just cause and effect. The only "input" that would not have meaning is the destruction which leads to the non existence of it.

I don't really understand how this argument is supposed to hold up and what's so special about "human" meaning.

permalinkembedsavereport

[–]senshi_do 2 points 4 years ago 

I am unsure many people really understand why they do most things in the first place. They might think they do, but I reckon that biology and chemistry have a much bigger role to play than people realise. Those are our rule books, we're just not always aware of them.

Not a great argument in my opinion.

permalinkembedsavereport

[–]Treczoks 4 points 4 years ago 

From the text under the video:

It simply proves that a computer cannot be thought of as a mind.

Nope. It simply proves that he does not understand what those "computer thingies" are or a "computer program" does.

His "Chinese boxes" example is wrong on so many counts, it actually hurts .

Yes, if you get the rules and fetch boxes, you are a nice boy. But it does not make you smart. It just makes you follow the rules, which is exactly what a computer does. But the smart part that is about "understanding Chinese" in this example is not the person in the room with the boxes. The smart part are the instructions that are given to him from the outside.

TL;DR: Even philosophers can totally misunderstand things.

permalinkembedsavereport

[–]JDude13 4 points 4 years ago 

I see it like this: you are not a Chinese speaker, but the system containing you, the rule book, and the symbols is a Chinese speaker. The room itself is the speaker.

This argument seems like claiming that I am not an English speaker because none of my neurons individually know how to speak English.

permalinkembedsavereport

[–]ChaChaChaChassy 1 point 4 years ago 

This argument seems like claiming that I am not an English speaker because none of my neurons individually know how to speak English.

Exactly. The whole is greater than the sum of its parts.

permalinkembedsaveparentreport

[–]ObsceneBird 4 points 4 years ago 

I'd never heard him speak before, what a wild California meets Wisconsin accent. Great video! I disagree with Searle about many things but I think his fundamental position on intentionality and semantic meaning is spot on here. Most of the replies from AI advocates are very unconvincing to me.

permalinkembedsavereport

[–]brine909 8 points 4 years ago 

the way I see it, a conscious being must be composed of things that aren't conscious. The atoms that make your neurons aren't conscious, the neurons themselves aren't conscious and most functions of the brain function outside of consciousness.

now looking into the Chinese room argument we can say that the rule book is the program and the person is the cpu. no one part of it is self aware but together they create a system that seems to be conscious.

It can be argued that even tho each individual part isn't conscious and doesn't know what it's doing but the system itself is conscious, similar to how each individual neuron or small group of neurons isn't conscious, but the whole brain itself is conscious.

permalinkembedsaveparentreport

[–]RocketPoweredPope 1 point 4 years ago 

now looking into the Chinese room argument we can say that the rule book is the program and the person is the cpu. no one part of it is self aware but together they create a system that seems to be conscious.

Where does the consciousness arise then? Because if those two parts (instruction set and executor) are all it takes, then we could use that logic to say that every computer is conscious.

No matter the complexity, an instruction set is still just an instruction set. Making it more complex isn't the same as adding a third element to the (instruction set and executor) system.

So I guess the question, is do organic beings have a third element that makes us conscious, something that we just haven't discovered yet? OR, is the only thing stopping my Alexa from being considered "conscious" a better set of programmed instructions?

permalinkembedsaveparentreport

[–]KingJeff314 1 point 4 years ago 

How do you define consciousness?

permalinkembedsaveparentreport

[–]brine909 1 point 4 years ago 

I wouldn't say every computer is conscious, but that every computer has the potential to become conscious given the right software. Think about the things computers already can accomplish with just instructions, executions, read and write.

Imagine a minecraft world, there is no one part of the computer that is that world, sure you have the file but without the processor, display and graphics card the world doesn't come to life and is just a meaningless batch of code. And even thoe there is no one part in the computer that is the world, the world still exists and you can interact with it and make changes too it. No one part of the computer is this space you are interacting with, you need all of the components to work together to create this space you are playing around in.

My point is that the physical components and action of a computer is different then the simulation it is running, they are two separate entities. Even thoe none of the components of the computer are sentient just like how none of the neurons in your brain are sentient doesn't mean that it can't simulate conciousness and if consciousness is just a complex network in your brain then I dont see why simulated consciousness is any different to biological consciousness other then the fact that it uses a different medium

permalinkembedsaveparentreport

[–]dxin 3 points 4 years ago 

This is like saying computers are dumb as sand.

In reality, especially modern computer systems, are built on layers upon layers of abstractions. The lower layer doesn't know the meaningfulness of their work. This is nothing new. E.g. your web browsers knows you are browsing web pages but doesn't understand a word of such page. The operating system doesn't know you are browsing but knows you are using it to display something and communicating with the network. The CPU is just running instructions. And the microcode and execution units doesn't even understand the instructions.

CPU itself is not AI. AI is a system processing power, software, and more. CPU itself is deterministic but AI doesn't have to be. This fact doesn't conflict with the fact human mind can be simulated using computational power.

Fire doesn't know how to cook doesn't mean you can not cook with fire, simple like that. In fact, you can use fire to generate electricity to power a automated machine to cook just as fine.

permalinkembedsavereport

[–]Crizznik 3 points 4 years ago 

I love how the bottom replies of this nature get downvoted while the one further up are upvoted. I'm wondering if the people who really understand philosophy are making it down here and disliking these refutations because they are unintelligent, or if it's the dumbasses wanting to dunk on materialists obsessively downvoting everything they disagree with.

permalinkembedsaveparentreport

[–][deleted] 2 points 4 years ago* 

Searle is saying that if he performed the role of the computer that is believed to understand Chinese, he still wouldn't understand Chinese, and that proves that the computer wouldn't understand Chinese either.

There are two problems with that:

In his example, he only manipulates symbols to generate answers from the questions. That's not good enough to pass the Turing test - you also need to keep the state of the simulated mind in the database and update it after every sentence. Otherwise, the output of the system will be the same for every same input. To use an example - without periodically updating the state, you could get a conversation "How are you?" "Fine, thanks!" "How are you?" "Fine, thanks!" "You just answered like you didn't remember what I asked four seconds ago, are you ok?" "What do you mean?" That wouldn't pass the Turing test. You have to change the thought experiment to include not only symbols and the book, but also the state of the system that's being changed after every step.

He's using a different definition of "computer" than the computational theory of mind (CTM). His definition is the hardware that physically performs the computation. CTM's definition of computer is the formal system itself. The difference is that while the "state of Searle" is Searle's mind, the "state of the computer" is the state of the simulated mind (which is the state of mind I mentioned in point (1)). By inspecting his own state of mind, Searle correctly concludes that he doesn't understand Chinese, but that's not where he should be looking - he should be looking into the simulated mind's state.

So first you need to change the thought experiment to include the state of the simulated mind, and then you'll discover that the experiment is an equivocation fallacy between Searle's and CMT's definition of computer.

permalinkembedsavereport

[–]plonk_house 2 points 4 years ago 

I think the issue he is trying to bring attention to is that Strong AI would be determined by external observation (e.g. passing a Turing test) while the concept of humanized intelligence has an internalized asset that a computer could not possess: genuine understanding rather than rote processes. That lack of understanding by AI has pros and cons.

However, It can certainly be argued that genuine human understanding is little more than rote process that has been attached to physical and emotional feedback. That said, the whole concept of determining “real” AI versus well programmed output is really getting into a limitation on meaningful measurement since all usable observation of AI would be external.

And that brings us back to the over simplified test that most of us would use for usable AI : if it walks like a duck and sounds like a duck, I’m going to say it’s a duck without having to dissect it.

permalinkembedsavereport

[–]LemmingRus 1 point 4 years ago 

Yea, but no. You see making a ‘perfect replication a brain’ would understand this nuisances... but instead, he backpedals from the original premise to set that example by putting limitations on it and thusly fails to make his point.

permalinkembedsavereport

[–]etherified 1 point 4 years ago 

One aspect that isn't included in the Chinese room experiment, but I feel should be, is that for living minds, the "rulebook" is constantly being updated with new information based on conditions and events in the outside world.

And there would be some overarching "program" (still in the Chinese room) that would determine how the new information relates to the existing information in the rulebook, how it should be stored, and if previous rules should be updated or deleted based on it.

If we include this, which I believe we should, it may help overcome some people's aversion to seeing AI as a "mind".

permalinkembedsavereport

[–]Critique_of_Ideology 1 point 4 years ago 

. The Chinese Room Argument

permalinkembedsavereport

[–]ajan1019 1 point 4 years ago 

By seeing questions and answers again and again in different variations, he might able to understand the Chinese language fully. That is what AI is all about. If we can find an AI which is capable of doing that it is strong AI.

Let's say you trained AI on past data(different sets of answers and questions), then it will find the rule book(simple mathematical formula). Then, using the rule book it can easily give answers to future questions.

Sometimes human behaves like that. I didn't born with all the information. For example, I don't know how to name animals. I learned using data and created a model in my brain and I am using that.

permalinkembedsavereport

[–]Aerodrive160 1 point 4 years ago 

I didn’t kill your dog.

permalinkembedsavereport

[–]nunnatron 1 point 4 years ago 

This doesnt make any sense? We arent born with any inherent language or understanding, we learn and internalise the "rulebooks" for language, and everything else for that matter. The rulebook the CPU etc working together are the mind....

Also what about functions like breathing they are autonomous and our minds follow a pre programmed rulebook for this procedure, there is no understanding in babys to breathe for example so our own minds fail the chinese room test.

permalinkembedsavereport

[–]dangerous_idiot 1 point 4 years ago 

the major flaw in his argument is this view of intelligence as some magic leap - some boolean switch - as if consciousness and intelligence aren't a spectrum. is a worm intelligent? what about a rat? what about a chimpanzee? well what about a developmentally challenged human? at what point do you cross some magic line where the 'mere appearance' of intelligence somehow becomes 'legitimate' intelligence? when AI advances to the point it is perfectly indistinguishable from a human intelligence, that won't count for anything because of this guys incredulity?

permalinkembedsavereport

[–]PeterCoen 1 point 4 years ago 

Maybe I’m too dumb to understand this problem, or Searle’s refutation but isn’t “mind” just a man made term to describe the complex neurocircuitry in the brain? It doesn’t exist without people defining it. That is to say if a meteor were to end all human life on earth and, by some miracle, intelligent life were to evolve again, that life form would eventually start to contemplate its own existence and the concept of “self”. That life form would almost certainly create a term to describe the software (thinking) that occurs within the hardware (brain). It could be “mind”, it could be anything at all. The point is that whatever term they came up with arose from attempts to describe the mind/body distinction.

Wittgenstein said “the meaning of a word is its use in a language”, now it seems to me that this isn’t a philosophical problem so much as a linguistic one. This whole thread and discussion centres on whether term x can be applied to thing y. The answer is surely yes, because the meaning of term x is defined according to its use. Whether you will be able to convince the man on the street that a computer literally has a mind is another, separate problem. I suspect we’ll someday see human rights campaigners arguing for the rights of AI, if we haven’t already. This doesn’t mean that they’re right.

permalinkembedsavereport

[–]zantho 1 point 4 years ago 

The entire argument is pinned to the idea that "meaning" is something only carbon based storage can achieved. I disagree.

permalinkembedsavereport

[–][deleted] 1 point 4 years ago 

This isn't a clear explanation because explaining the Chinese room clearly is how you refute it because it's founded on misdirection.

permalinkembedsavereport

[–][deleted] 1 point 4 years ago 

Hello! I am a bit naive to the nuisances of these arguments. I tried to read most of the thread to find the answer to my question, but I could not. I was hoping someone could help shed light on it for me.

As I listened to Seale's argument for the Chinese room, my first thought was this sounds almost exactly like childhood development both verbal and non verbal. So, if people cannot pass the Chinese room argument at every point in their own life, why could it not be said that we are also missing out on the moment a program goes from more than just syntax to something...I don't know the word... More special?

permalinkembedsavereport

[–]hackinthebochs 1 point 4 years ago* 

This sounds like you're thinking of language acquisition as something like learning rules of a language and then just getting better and better at processing those rules. There are some theories about language acquisition along these lines, but I don't think they're good descriptions of how children actually learn language. For example, a child playing with a ball might have the word "ball" spoken to her as she is playing, indicating that this word is the name of the object. But the child isn't learning syntax here, she is learning an association between the world "ball" and all the experiential qualities associated with the ball. When the child later begins to speak and says "ball" when she sees the ball, she is not behaving like an automaton with no understanding, she genuinely understands that "ball" means the round colorful object. How the child could understand this is an important question, but it seems that we have an innate capacity to form meaningful associations between language and objects in the world.

permalinkembedsaveparentreport

[–]xxtanisxx 1 point 4 years ago 

I don’t pretend to understand this. But if the rule book is YouTube video, couldn’t human just learn Chinese this way and machine can’t atm.

permalinkembedsavereport

[–]taboo__time 1 point 4 years ago 

When I first heard about the Chinese Room I thought it was an interesting thought experiment to explore AI, intelligence and understanding.

After more learning I realised he literally thought solved the question. That it was a literal experiment. Bizarre.

There's a video of him in front of a Google audience. They have respect but you can see as his talk goes on they're increasingly unconvinced by his ideas.

permalinkembedsavereport

[–]chko 1 point 4 years ago* 

He mentioned in the video,

piles of boxes in the room = database

instructions set = program

person = CPU

We can have a similar analogy to the human brain:

database = parts of the brain that is responsible for memory (Temporal Lope)

program = parts of the brain that is responsible for language processing (Temporal Lope)

CPU = parts of the brain that does reasoning (Frontal Lope)

This argument is saying that since the Frontal Lope doesn't understand Chinese, the human brain cannot actually understand Chinese. Database+program+CPU makes up the AI consciousness, not just the CPU itself.

permalinkembedsavereport

[–]smilelaughenjoy 1 point 4 years ago 

But this argument would then suggest that computers have their own language and they're just using human words as symbols to facilitate communication, just like the English speaker would be doing for Chinese symbols.

permalinkembedsavereport

[–]tells 1 point 4 years ago 

what does he mean when he says meaning? is it beyond a literal definition? imo, if there is personal meaning it means an association to another object possibly with an emotional state attached to it. I wonder if the only thing missing is a reptilian side to AI for it to "create meaning".

permalinkembedsavereport

[–]randpaulsdragrace 1 point 4 years ago 

Wow a philosophy

permalinkembedsavereport

[–]kickstand 1 point 4 years ago 

If the man in the Chinese Room learns to associate a Chinese symbol with, say, trees, then hasn’t he, in fact, attached a meaning, a translation, to the symbol for “tree”?

permalinkembedsavereport

[–]chazwomaq 1 point 4 years ago 

I think he should change "understanding" to "conscious(ness)" and the whole thing would be much clearer.

Yes, you could design a system that "understands" Chinese, but it wouldn't be conscious of it. In fact, that is exactly how most of the human mind works. The conscious "I" is only a tip of the iceberg of the true unconscious mind, yet it doesn't feel that way to ourselves.

The Chinese Room boils down to the problem of consciousness. A fully working animal or indeed any system need not be conscious, and yet it appears humans are, at least partly. Nobody knows why and philosophers and scientists have wondered about it forever. Restating it in this thought experiment just muddies the waters and doesn't say anything new.

permalinkembedsavereport

[–]slavior_of_apes 1 point 4 years ago 

AI, the Great Deception.

permalinkembedsavereport

[–]Knightfray 1 point 4 years ago 

We're complex organic computers that take multiple processes from our DNA; output it through our cells to release chemicals and bioelectricity throughout our body. Sure we've come up with clever names for it like souls, or personality but it's all the same root just organic cells in the form of 1's and 0's to make us do work. My perspective on it is simplified I know, but how different are we from an AI?

permalinkembedsavereport

[–]brockmasters 1 point 4 years ago 

the moment a computer can call bullshit, it will be able to adhere to semantics. good video. the larger issue this does bring up for me is will people still value a computer that could call bullshit? the scope of AI may be more intertwined with the value we put on the human mind to begin with... scary shit man

permalinkembedsavereport

[–]Twoslot 1 point 4 years ago 

In order to pass the touring test, the person in the Chinese room would have to be so versed in the symbols that they would have decoded them. There's no way you could respond efficiently, accurately, and quickly enough to convince the person testing you unless you knew what they meant.

permalinkembedsavereport

[–]ChaChaChaChassy 1 point 4 years ago 

This is a bad analogy. A computer program is not operating based on a "rule book" it IS that rule book...

That's a critical difference. You can't separate the program from.... the program. It's one and the same. Our brains follow a "rule book" too... and when we see symbols that look like this "dog" we think of a furry four legged animal... but we aren't just following a rule book to associate those two, our brains ARE the rule book and we ARE our brains.

permalinkembedsavereport

[–][deleted] 1 point 4 years ago 

He has a point

permalinkembedsavereport

[–]Dragon20942 1 point 4 years ago 

Being in the Chinese Room carrying out the instructions does not make you the software. You are the hardware here. The instruction book is the software, and therefore the mind. The analogy falls apart at this point, because we can’t claim that a single part of the brain is
fully our mind.

So what is a mind? Is it the brain? Clearly not, since the mind is hardware agnostic - a transference of a human consciousness to a computer with functionally identical faculties is still perceived as being that same mind. So is the mind a nonphysical concept that manifests physically as the arrangement of neural pathways that exist in the brain? This is a much stronger argument because the creation of something that functions identically to that pattern of neural pathways contains the rulebook that governs the processes of the mind. But is a mind independent from hardware completely? If no, then you can argue the question of whether or not something is a mind is a completely arbitrary definition that is answered only through comparison to things we know to be minds. If yes, then we have no actual way of knowing whether or not two identical brains have the same mind, and cannot conclude anything at all about mindfulness.

So this basically leads to another question: are we even capable of judging what is a mind and what isn’t?

Mindfulness is inherently a standard set by humans. It is simply a comparison of the subject to our own understanding of our species. We are incapable of evaluating our own species because our minds are the ones we put to work to even think about this. To execute a meaningful standard of mindfulness at all requires an possession and understanding of extended mindfulness entirely foreign to us. This means that we should not ever be able to come up with a better standard other than different ways to compare the subject to ourselves - either:

An integral formulation - can the entire summation of the process and outputs be reasonably attributed to that of a human mind?

A differential formulation - if you delve deep into the inner workings of the rulebook, does each part logically mirror the sequences and processes of the human brain?

But definition by comparison is shallow right? There should be some fundamental set of conditions that govern it right? Let’s reject this and return to parsing some grand truth of human mind. Well, we don’t know, and can only compare to ourselves, hence neuroscience, machine learning, the Turing Test, and on and on and on. We can only try to pin down concepts independent from tangible things through logic and reasoning, which are only airtight from the perspective of the human mind, which has no way of concluding anything nontrivial about the nature of itself. Different CPU architectures parse things in completely different ways, and therefore have deeply incompatible logics on an unabstracted level, only reconciled when parsed by higher logic (humans), showing through comparison that incompatible alternatives to human logic can still be valid in some supposed higher grand truth. We cannot know if we are the highest logic without actually knowing of the existence of one.

My interpretation of the human program is this: there are three fundamental operations we consistently execute from the start: distinction, comparison, and abstraction. Your brain and sensory organs give you the power to differentiate distinct stimuli through construction and instinct. The construction of your brain also allows for meaningful (from a human standpoint) comparisons and a form of storage of those results from which to draw more comparison. This storage and organization to form new concepts is the abstraction of stimuli, and allows for new stimuli to be compared to previously stored structures. With sufficient initial conditions and environment, the result is functionally indistinguishable from a human mind, although that is an arbitrary definition.

permalinkembedsavereport

[–]webdotorg 1 point 4 years ago 

Why not take the opposite approach and consider that we're not as conscious as we think we are?

permalinkembedsavereport

[–]rebleed 1 point 4 years ago 

This thought experiment fails because it does not consider how the rulebook is constructed.

Let’s say the rulebook is not a program, but was written by a Chinese speaker. The rulebook is a product of a human mind who understands Chinese. The human who uses the rulebook is a proxy for that Chinese understanding mind.

Or let’s say the rulebook is a statistical description of neurons having firing in a Chinese speaker’s mind in a lifetime of scenarios.

In each of these cases, the rulebook is a product of a mind, and the human computer is just a proxy for that mind, but separated from that mind in time and space.

Yes, the other mind does not exist at the moment of the rulebook being used, but this does not mean the mind does not exist at all.

When reading the autobiography of Mark Twain, it is true that Mark Twain’s mind is not instantiated in that moment. However, his mind was at some moment in time and space instantiated as the book was written. It would be silly to claim that Mark Twain’s mind never existed.

So too with a computer program. This is particularly clear in a program that is a deep neural network, that first has to be trained and evaluated to the point of saying, “yes, this would pass the Turing test.” If you were then to write a rulebook that describes the neural network and its weights and operations, then clearly you wouldn’t be creating a mind. You would merely be writing down a comprehensive description of that mind, so descriptive it is capable of being proxied in time and space. The mind behind the computation may not be present at that moment, but it certainly was present at some moment.

permalinkembedsavereport

[–]lisper 1 point 4 years ago 

it does not consider how the rulebook is constructed

Exactly. It is the rule book that understands Chinese. The fact that it's a human carrying out the rules is a red herring. Your neurons don't understand Chinese (or English) either but that doesn't mean you can't.

permalinkembedsaveparentreport

[–]new_sincere_account 1 point 4 years ago 

Not to be rude but this seems argument seems facile. I'm a layman and I'm sure this has been better expressed in other comments here but I have to say it--

An English speaker assembles answers to English questions according to a set of rules that have been codified somewhere as text, same as Chinese. The actual concepts, like the noun "moose" in the English question "what is a moose," are defined from other data that is also recorded. The definition of a "moose" is an image, a sound, a smell, a taxonomy, etc. Someone in higher latitudes probably has memories of a specific moose. I've never seen a moose in person yet I have a mind and can answer the question "what is a moose" based on media I've consumed. If the question was asked in Chinese, I could use google translate to produce a reasonable answer just the same.

I'm not well acquainted with biomechanics of the brain, but it seems like an area of the brain does processing on memories from another area, and no single set of neurons know what the English word "moose" means yet the mind as a whole can generate a satisfactory answer to the other minds asking the question.

permalinkembedsavereport

[–]bisonrosary 1 point 4 years ago 

Anyone else get asmr from this?

permalinkembedsavereport

[–]jgjj92 1 point 4 years ago 

on an unrelated topic the thumbnail tripped me tf out, thought i'd forgotten i took acid or smt for a second

permalinkembedsavereport

[–]El_Guap 1 point 4 years ago 

So pattern recognition vs true understanding.

permalinkembedsavereport

[–]blazerman345 1 point 4 years ago 

The system (which includes the guy in the room, and the stack of instructions) clearly understands Chinese, even though the individual parts may not.

At the end of the day this is an argument between bundle theory and substance theory. Are two objects with exactly the same properties actually the same?

permalinkembedsavereport

[–]711lulz 1 point 4 years ago 

Makes perfect sense. We have to turn a program constraint into a feature just like the way humans formed.

permalinkembedsavereport

[–]pluviusdeus 1 point 4 years ago 

I think a fundamental flaw of this argument is the unbalanced equivalence it makes between a central processing unit (a single functional component) and a fully functioning system.

permalinkembedsavereport

[–]pluviusdeus 1 point 4 years ago 

Imagine you're a single element of a functioning system, could you as that element understand what you're doing?

permalinkembedsavereport

[–]qcihdtm 2 points 4 years ago 

This is a very good point. There wouldn’t be a language either.

permalinkembedsaveparentreport

[–]CaptainHappen007 1 point 4 years ago 

Couldn't you just program the computer to know the meaning of the words?

permalinkembedsavereport

[–]RedditModsAreVeryBad 1 point 4 years ago 

Also, he mentions that over time he'll get good at matching the Chinese symbols with the English words - which seems to discount the possibility that over time, as he became familiar with the symbols, they would acquire meaning. Which is exactly how you learn a language...

permalinkembedsavereport

[–]pyewacket73 1 point 4 years ago* 

I don’t think the Turing test is useful at all in discussing the “hard problem” of consciousness. It’s simply a metric for evaluating the ability of an AI system to convince a human being that it is speaking to another human. The many potential implementations of this test vary wildly in their difficulty (a test lasting 5 minutes would be much easier to beat than one lasting an hour, and a child would be much easier to fool than an adult).

Furthermore, we have to be honest about the imprecise nature of what it means to “understand” a language. Imagine two native English speakers engaged in conversation, but one of them has a working vocabulary of 40,000 words, and the other only 20,000. Although they have different understandings of the same language, they would still be able to communicate for most practical purposes. The person with a smaller vocabulary would be able to glean contextual meanings for any words he did not understand, or else be able to ask clarifying questions.

The more fundamental issue is that any kind of “understanding” is highly contextual and can only be demonstrated with reference to a specific test. In our everyday lives, we grant this power to anyone who gives vaguely sensible responses to our utterances, and generally assume that anyone employing the proper syntax and drawing from the same pool of words shares the same substrate of comprehension as we do. But even this does not account for the true complexity of language, as each person has, over the course of their life, accumulated a deep and colorful network of experiences and connotations which influence their individual understanding of the words they choose to articulate their subjective experience.

I don’t think the ability of an AI system to parse linguistic prompts and construct reasonable responses has anything to do with the question of whether that system possesses some kind of subjective experience.

tldr - Anyone who pins their understanding of artificial consciousness on the Turing test deserves to be locked in the Chinese Room with John Searle.

permalinkembedsavereport

[–]Darsint 1 point 4 years ago 

Forgive me for asking stupid questions here, as I am not classically trained in philosophy at all...

Doesn't his argument, and this problem, rely on a program that is static and not dynamic? That the "instruction book" that he's given never updates? That even the super mega awesome version that could answer any question (because the database/instruction book is so large as to encompass all potential human questions) is given to the person in the box at the very beginning and never changes?

Is the ultimate point of this exercise to prove that static instruction books cannot be treated as an intelligence? Because I could see that being a valid argument for that condition. Yet unconvincing for a dynamic program.

permalinkembedsavereport

[–]cingan 1 point 4 years ago 

If the rule book didn't just instructed the operator in the room, to manipulate the Chinese symbols and then create an output, but also instructed her to activate some other type of knowledge content (like other types boxes of mental images, diagrams, real world knowledge) to represent the incoming message at the semantic level, and created structures of discourse representation to operate on and then return to the syntactic rule book to compose the response in Chinese symbols then we can say that even the operator doesn't understand Chinese, the room itself mentally represents and constructs the answer so understands the communication that has taken place..

permalinkembedsavereport
[+][deleted] 4 years ago (2 children)


[–]Astares1 1 point 4 years ago 

whatever AI we create will also be a functioning vessel of the godhead as we all are. reality is a set of seemingly infinite parameters, and thus needs an operator to differentiate context.

permalinkembedsavereport

[–]EnderAnswer 1 point 4 years ago 

I believe that his claim only refutes the strong AI theory because the strong AI theory specifically involves the Turing test. The Turing test is a method of inquiry that can test whether or not a computer has artificial intelligence, but only under a specific set of circumstances; just like how the person in the Chinese room only understands Chinese under a specific set of circumstances.

I would not say that the person in the Chinese room is completely indistinguishable from a native Chinese speaker because they would not be able to start and maintain a improvisational conversation. This does not mean that the Chinese room argument is invalid as it is merely a counter argument to the strong AI theory.

All this means is that the strong AI theory is wrong in another way because the Turing test only tests for intelligence under certain circumstances which I would say does not constitute what we call intelligence/consciousness.

permalinkembedsavereport

[–]MajorWhale 1 point 4 years ago 

I don’t know if i agree with the point this thought experiment makes.

What is the difference between a languahe and a set of rules for manipulating symbols? Can we empirically measure, define understanding?

Of course the difference between the huamn processing of these symbol is different from the way a computer would. But does this (purely linguistic) difference really refute anything? This experiment bases its conclusion on a very strict definition of what a mind is, has to maybe simplify the concept to even be able to refute it.

Not a fan.

permalinkembedsavereport

[–]molino-edgewood 1 point 4 years ago 

I don't agree with him. Suppose the cards he was actually shuffling around were (very slowly) simulating a universe which contained humans. Are those humans incapable of thought? They obey the same physical laws as we do and have the same biology. If we accept Searle's argument it seems we must reject materialism.

permalinkembedsavereport

----------


search reddit for articles about the Chinese Room (Searle)


Here are some relevant Reddit discussions about the Chinese Room thought experiment proposed by John Searle:

r/AskPhilosophy:

Searle Chinese Room - A user asks for an ELI5 (Explain Like I'm 5) explanation of the Chinese Room and how it argues against AI consciousness.
Is Searle's Chinese Room thought experiment as stupid as many believe? - Discussion on whether the argument is as flawed as some suggest, with many arguing it's a valid point against certain types of consciousness.
Why is the Chinese Room Argument Declared Invalid by the Majority? - Explores why the majority of philosophers seem to disagree with Searle's conclusions.
r/Science:

Science AMA Series: I'm Nick Bostrom, director of the Future of Humanity Institute at Oxford University, AMA [invalid URL removed] - A comment thread within an AMA with Nick Bostrom where Searle's Chinese Room argument is discussed, with one user comparing Searle's refusal to accept counterarguments to a creationist denying evolution.
r/Philosophy:

The Chinese Room argument, explained clearly by Searle himself - A humorous take on the Chinese Room argument, using a sewing machine analogy to illustrate the potential difference between functional intelligence and genuine understanding.
r/Wikipedia:

The Chinese room argument holds that a digital computer executing a program cannot have a "mind", "understanding", or "consciousness", regardless of how intelligently or human-like the program may make the computer behave - A discussion sparked by the Wikipedia article on the Chinese Room, with users debating the implications of the argument for the possibility of artificial consciousness.   
These Reddit discussions provide a good overview of the various perspectives and arguments surrounding the Chinese Room thought experiment. They highlight the ongoing debate about the nature of consciousness, intelligence, and whether machines can truly understand or just simulate understanding.






summarize the interesting ideas from the first link, one idea per line, self contained


Syntax vs. Semantics: The Chinese Room emphasizes the difference between manipulating symbols (syntax) and actually understanding their meaning (semantics). A computer might process information without truly grasping it.
Consciousness as a Biological Phenomenon: Searle suggests that consciousness is likely an emergent property of biological systems, implying that simply replicating the functional processes of the brain might not be enough to create a conscious machine.
Intentionality and Understanding: True understanding requires intentionality, a directedness towards something. The Chinese Room argument questions whether a computer program, even if complex, can possess genuine intentionality.
The "Systems Reply" Counterargument: Some argue that while the person in the room doesn't understand Chinese, the system as a whole (person + rules + room) does. This highlights the debate about whether consciousness can emerge from a complex system.
The Importance of "Causal Powers": Searle argues that the physical structure of the brain plays a crucial role in consciousness, suggesting that a computer made of different materials might not be able to replicate those "causal powers" even with the same program.
The "Robot Reply" Counterargument: This response proposes that a machine interacting with the world, receiving sensory input, and acting upon it might develop genuine understanding. This highlights the potential importance of embodiment and interaction for consciousness.





yes, but collect people's original ideas not just what can be read in the textbook


You're right! It's more interesting to look at how people are applying the Chinese Room thought experiment in new ways. Here are some original ideas from that Reddit thread:

The "Chinese Room" as a metaphor for online communication: One user suggests that the internet itself is a giant Chinese Room, where people manipulate symbols (text, emojis) without always truly understanding each other. This raises questions about the depth of online communication and the potential for misinterpretations.
Applying the concept to animal communication: Could animals be considered to be in a "Chinese Room" scenario? They respond to our cues and commands, but do they truly understand the meaning behind our words? This raises questions about the nature of animal consciousness and their capacity for understanding.
The Chinese Room and the Turing Test: Someone proposes a modified Turing Test where the human judge is explicitly told that one of the participants is a Chinese Room. Would this change how we evaluate the "intelligence" of the machine, knowing that it might just be following rules without true understanding?
Consciousness as an "illusion": A user suggests that maybe consciousness itself is an illusion, a complex set of processes that tricks us into believing we understand. This takes the Chinese Room argument further, questioning the nature of our own consciousness.
The role of "qualia" in understanding: Qualia are subjective, qualitative experiences, like the redness of red or the feeling of pain. One commenter argues that a machine lacking qualia might be able to process information but wouldn't truly understand the meaning in the way humans do.
These are just a few examples of how people are taking the core ideas of the Chinese Room and applying them to new situations, sparking further discussion and debate about consciousness, understanding, and the potential of artificial intelligence.






ok mine the second one for novel ideas


You're right, that second link has some gems hidden within the comments! Here are some of the more novel ideas I found in that discussion:

The Chinese Room as a spectrum: Instead of a binary "understanding" or "not understanding", one commenter proposes that understanding exists on a spectrum. A simple chatbot might be at one end, while a human with deep knowledge of a subject is at the other. This challenges the black-and-white view of the original argument.
The importance of "grounding" in language: Several users discuss the idea that true understanding requires symbols to be "grounded" in real-world experiences. A machine that only manipulates symbols without sensory input and interaction with the world might never achieve true understanding.
The role of emotions in understanding: One commenter argues that emotions are crucial for understanding, as they provide context and meaning to information. This raises questions about whether a purely rational AI could ever truly understand the human experience.
The "Chinese Room" as a thought experiment about qualia: Some users connect the Chinese Room to the problem of qualia (subjective experiences). They argue that even if a machine perfectly simulates human behavior, we can never know if it has the same inner experience, the same "feel" of understanding.
The potential for emergent consciousness in AI: While Searle himself is skeptical, some commenters argue that consciousness could still emerge in a sufficiently complex AI system, even if it doesn't perfectly replicate the human brain. This points to the possibility of unexpected emergent properties in AI as it continues to develop.
These ideas go beyond the standard textbook responses to the Chinese Room argument and delve into deeper questions about consciousness, understanding, and the future of AI. They highlight the ongoing relevance of this thought experiment in the age of increasingly sophisticated artificial intelligence.






next link


You're keen to keep digging into this! Alright, let's dive into that third link from the r/AskPhilosophy subreddit:  "Why is the Chinese Room Argument Declared Invalid by the Majority?"

This thread tackles a key point: while the Chinese Room is a popular thought experiment, many philosophers actually disagree with Searle's conclusions. Here are some of the interesting ideas from that discussion that challenge the argument:

The "Systems Reply" revisited: This counterargument gets a lot of attention. People argue that focusing on the individual in the room misses the point. The system as a whole (person + rules + room) could be considered to understand Chinese, even if the individual components don't. This challenges Searle's focus on the individual's lack of understanding.
The problem of defining "understanding": Many users point out that Searle never clearly defines what "understanding" actually means. Without a clear definition, it's difficult to say definitively whether the Chinese Room possesses it or not. This highlights the philosophical ambiguity at the heart of the argument.
The analogy to the human brain: Some argue that the human brain itself is like a complex "Chinese Room," where neurons fire and interact without any single neuron "understanding" the overall picture. This suggests that consciousness might emerge from complex interactions, even if individual components lack understanding.
The "Virtual Mind" reply: This interesting idea proposes that a sufficiently complex simulation could give rise to a "virtual mind" that genuinely understands, even if it's not the same kind of understanding as a biological mind. This challenges the assumption that only biological systems can have consciousness.
Focusing on functionality: Some commenters argue that whether or not a machine truly "understands" might be irrelevant. If it can perform the functions of understanding (e.g., translate languages, answer questions, write stories), then it effectively understands for all practical purposes.
This thread provides a good counterpoint to Searle's argument, highlighting the complexities of defining "understanding" and the potential for emergent consciousness in complex systems. It shows that the debate about the Chinese Room and its implications for AI is far from settled.






next one


Okay, let's move onto the next link, the one from r/Science: "Science AMA Series: I'm Nick Bostrom, director of the Future of Humanity Institute at Oxford University, AMA".

This one is a bit different, as it's not solely focused on the Chinese Room. It's an "Ask Me Anything" (AMA) session with Nick Bostrom, a prominent philosopher known for his work on existential risks and superintelligence. However, within this AMA, there's a comment thread where the Chinese Room argument comes up. Here are some of the noteworthy ideas from that discussion:

Comparing Searle to creationists: One commenter draws a parallel between Searle's staunch defense of the Chinese Room argument and creationists denying evolution. They suggest that both seem resistant to counterarguments and evidence that challenges their position. This raises questions about the role of dogma versus open-mindedness in philosophical debates.
The importance of future AI potential: Several commenters express concern about dismissing the potential for future AI consciousness based on current limitations. They argue that it's crucial to consider the possibility of AI systems evolving beyond our current understanding, even if they don't currently exhibit "true" understanding.
The relevance of the Chinese Room to superintelligence: Some users discuss how the Chinese Room relates to the potential dangers of superintelligence. If we can't be sure whether an AI truly understands, how can we ensure it aligns with human values and goals? This highlights the ethical implications of advanced AI, even if it lacks consciousness.
The role of embodiment in AI development: One commenter argues that embodied AI, interacting with the real world through sensors and actuators, might be more likely to develop genuine understanding than a purely computational system. This echoes the "Robot Reply" to the Chinese Room and emphasizes the potential importance of embodiment for AI consciousness.
The limitations of thought experiments: Some users express skepticism about the value of thought experiments like the Chinese Room, arguing that they can oversimplify complex issues and lead to misleading conclusions. They suggest that empirical research and real-world AI development might be more fruitful avenues for understanding intelligence and consciousness.
This thread provides a more diverse range of perspectives on the Chinese Room, going beyond the traditional philosophical arguments. It highlights the relevance of the thought experiment to contemporary discussions about AI, superintelligence, and the future of humanity.






Here is a RAG result from Hacker News



"""dTal 4 months ago | on “Anthropic publishes the 'system prompts' that make Claude tick”

I think you have misunderstood Searle's Chinese Room argument. In Searle's formulation, the Room speaks Chinese perfectly, passes the Turing test, and can in no way be distinguished from a human who speaks Chinese - you cannot "pop the illusion". The only thing separating it from a literal "robot that speaks Chinese" is the insertion of an (irrelevant) human in the room, who does not speak Chinese and whose brain is not part of the symbol manipulation mechanisms. "Internal cause and effect" has nothing to do with it - rather, the argument speciously connects understanding on the part of the human with understanding on the part of the room (robot).



The Chinese Room thought experiment is not a distinct "scenario", simply an intuition pump of a common form among philosophical arguments which is "what if we made a functional analogue of a human brain that functions in a bizarre way, therefore <insert random assertion about consciousness>".



2.

rolisz over 1 year ago | on “GPT-4 Is a Reasoning Engine”

Sure, but Searle's interpretation of the Chinese Room Experiment was never very convincing to me. I think the room + the person inside it, as a system, do speak Chinese.



3.

roddylindsay over 1 year ago | on “We need a more sophisticated debate about AI”

I think Searle would agree with you that the room does in fact speak Chinese. His point is that the neither the person, the machine, or "the room" understands Chinese, at least in the usual sense of that word when it comes to understanding as implemented by humans.



4.

CamperBob2 3 months ago | on “Chain of Thought Empowers Transformers to Solve Inherently Serial Problems”

It's a Chinese Room - or a stochastic parrot.



Show me a resident of a Chinese Room who can do this: https://chatgpt.com/share/66e83ff0-76b4-800b-b33b-910d267a75...



The Chinese Room metaphor was always beneath Searle's intellectual level of play, and it hasn't exactly gotten more insightful with age.



5.

Barrin92 7 months ago | on “Is artificial consciousness achievable? Lessons from the human brain”

>Chinese room, considered as a whole, does understand what it does. The fact that the man inside of it does not is simply irrelevant



Searle does address this point even in the original paper. That argument doesn't hold water because you can imagine taking the whole room and putting it in your head and then you still don't understand Chinese. Or put differently if you're a Mandarin speaker and we two sit in a room and I use you to secretly translate, you understand the meaning of what is being said, I don't and it doesn't mean anything to say we "as a system do".



The point is that even though we can "as a system" behave as if we speak Mandarin, there's a difference between you and me. You understand what you're talking about, and I just hear gibberish. Searle is a die-hard materialist by the way, nothing of that violates materialism. What he isn't is a functionalist. What he is teasing out in the thought experiment is that a system that produces the same output as nother system does not need to be equivalent on the inside.



6.

fsckboy 12 months ago | on “Scientists have 20-minute "conversation" with a humpback whale”

interestingly, it puts us in a room adjacent to that of a famous thought experiment from philosophy, John Searle's https://en.wikipedia.org/wiki/Chinese_room



7.

sovande almost 2 years ago | on “CheatGPT”

Isn’t this the whole point of John Searl’s “the Chinese room” thought experiment? But does it matter what is actually going on inside the room, if the effect and function is indistinguishable? Edit: after conferring with ChatGPT, Searle’s point like yours is that the man in the room doesn’t understand Chinese, he is just manipulating symbols, but from the outside, the man in the room seems to speak fluent Chinese.



8.

gessha 7 months ago | on “Scientists Find an 'Alphabet' in Whale Songs”

Reminds me of the Chinese room [1] argument: Does a computer really understand Chinese language if it can respond to Chinese inputs with Chinese outputs?



[1] https://en.wikipedia.org/wiki/Chinese_room



9.

im3w1l over 2 years ago | on “Don’t think to write, write to think”

Basically chinese room.



10.

eru over 2 years ago | on “Consciousness is not computation”

Alas, if you refer to Searle's Chinese Room, that hasn't shown anything, if you just bite the bullet and accept that the Room is conscious.



11.

whitten 10 days ago | on “DeepThought-8B: A small, capable reasoning model”

The Chinese Room is a theoretical room that contains a “Chinese speaker” but actually when given a text to ‘understand’ actually just looks up the text in a huge number of words inside until it finds a way to find a response and then just outputs the response as its reply



12.

abudabi123 about 2 years ago | on “New research suggests our brains use quantum computation”

Searle's Chinese Room?



Say, you put three individuals in the room. Say, Steve Wozniak, Steve Jobs, Steve Balmer. Say, the experiment is plus or minus Jack Ma. The room follows the script. I guess 30 minute runs of the experiment generates a kind of collective consciousness in the conversation content between participants that is completely off script.



13.

Jipazgqmnm almost 2 years ago | on “Theory of Mind May Have Spontaneously Emerged in Large Language Models”

It's called Chinese Room: https://en.wikipedia.org/wiki/Chinese_room



> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese?



To me: If you can't tell, it effectively doesn't matter.



14.

nopassrecover over 1 year ago | on “GPT-4 performs better at Theory of Mind tests than actual humans”

It's interesting how quickly the Chinese Room has moved from thought experiment to reality https://en.wikipedia.org/wiki/Chinese_room



15.

jedharris 10 months ago | on “Sora: Creating video from text”

That matches how philosophers typically talk about the Chinese room. However the Chinese room is supposed to "behaves as if it understands Chinese" and can engage in a conversation (let us assume via text). To do this the room must "remember" previously mentioned facts, people, etc. Furthermore it must line up ambiguous references correctly (both in reading and writing).



As we now know from more than 60 years of good old fashioned AI efforts, plus recent learning based AI, this CAN be done using computers but CANNOT be done using just ordinary if - then - else type rules no matter how complicated. Searle wrote before we had any systems that could actually (behave as if they) understood language and could converse like humans, so he can be forgiven for failing to understand this.



Now that we do know how to build these systems, we can still imagine a Chinese room. The little guy in the room will still be "following pre-written scripted algorithmic procedures." He'll have archives of billions of weights for his "dictionary". He will have to translate each character he "reads" into one or more vectors of hundreds or thousands of numbers, perform billions of matrix multiplies on the results, and translate the output of the calculations -- more vectors -- into characters to reply. (We may come up with something better, but the brain can clearly do something very much like this.)



Of course this will take the guy hundreds or thousands of years from "reading" some Chinese to "writing" a reply. Realistically if we use error correcting codes to handle his inevitable mistakes that will increase the time greatly.



Implication: Once we expand our image of the Chinese room enough to actually fulfill Searle's requirements, I can no longer imagine the actual system concretely, and I'm not convinced that the ROOM ITSELF "doesn't have a mind" that somehow emerges from the interaction of all these vectors and weights.



Too bad Searle is dead, I'd love to have his reply to this.



16.

throwawaygal7 over 2 years ago | on “The singularity is close?”

I think the Chinese room experiment is pretty effective, but a person might say the room is conscious and one might not. We all agree that, if we had a Chinese room in front of us, the only real way to tell would be a Turing style test.



17.

mannykannot about 2 months ago | on “Language is not essential for the cognitive processes that underlie thought”

The brain is faster than the Chinese room, but other than that, yes, that's the so-called systems reply; Searle's response to it (have the person in the room memorize the instruction book) is beside the point, as you can teach people to perform all sorts of algorithms without them needing to understand the result.



As many people have pointed out, Searle's argument begs the question by tacitly assuming that if anything about the room understands Chinese, it can only be the person within it.



18.

Peritract over 2 years ago | on “Consciousness is not computation”

That would also apply to the Chinese room.



19.

jll29 over 1 year ago | on “Artificial Consciousness Remains Impossible (Part 2)”

Searle's Chinese Room thought experiment is about mastering the Chinese language.



There are two issues I have with it as it was originally presented:



1. Mastering a language is not the same as having consciousness.



2. Who "knows Chinese" in the Chinese Room thought experiment? I would say neither Searle nor Searle in/as part of the Chinese Room "speak" Chinese. But it is



fair to say that the book that the fictional Searle follows can be



seen as a model of the Chinese language; or at least the combination of



the book and Searle as its "processor" collectively are an implemented



operational model of the Chinese language. And a model of Chinese is



NOT the same as being skilled at conversing in Chinese (executing the



model in a particular way). Other posters here have drawn analogies from



music evoking certain subjective emotions, and again a semantic network



that has concept nodes labelled with the names of these emotions is not



the same as experiencing these emotions, although the semantic network



can be said to constitute a model of sorts of the music's effects. But



again, model(x) != qualia(x).



20.

emtel almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

The Chinese Room is a really, really bad thought experiment. It's just an attempt at misdirection. It asks you to imagine a computer (one that operates on slips of paper according to rules that are written down in a big book), puts a human inside that computer, and then says that because the human doesn't understand what the computer program is doing, the computer program isn't conscious.



But this is just question-begging. The experiment purports to resolve the question "can a computer be conscious", and answers it by showing you a computer and claiming that it is not conscious.



Worse, the Chinese Room is straight-up Cartesian dualism. By placing the human inside the computer, Searle is preying on your Cartesian intuition that there must be a theatre of the mind where reality is experienced. And since Searle's homunculus doesn't understand Chinese, nothing can be experienced, therefore no conscious experience is happening in this system.



If I'm wrong, and the Chinese Room thought experiment is conclusive, it must be possible to reformulate it without the human. After all, the human does nothing but follow rules in a purely mechanistic fashion. Can anyone offer a reformulation of the experiment, that has no human, that they still feel is convincing?



21.

whittingtonaaon over 2 years ago | on “Consciousness is not computation”

I know Searle replied to this, but don’t remember what he said, except it seemed a little ridiculous to talk about sentient rooms. I think it’s much more plausible to just claim that the people who wrote the instructions know Chinese.



22.

bonoboTP almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

In some more "rational"-oriented circles, the Chinese Room argument is seen as obviously wrong, outdated and mysticistic, for example see Dan Dennett.



The Chinese Room would need to be so enormous, and it would work so slowly, that it's not a good intuitive analogy. It also introduces a homunculus actor that reads, interprets and understands the instructions, which muddles the analogy and focuses our attention to the conscious quality of the person following the instructions.



23.

Barrin92 almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

The notion that the Chinese room is 'mysticistic' is one of the most bewildering responses to it to the point where I don't think people who make that argument actually understood Searle.



Searle, simplified holds that consciousness is a property of very particular biochemical systems, not a disembodied thing that exists independent of the matter it inhibits.



It's the rational-oriented circles ironically enough who almost dabble in a Cartesian Dualism by positing "computation" or "minds" as distinct from matter. In fact with on occasion almost religious-sounding implications.



24.

dragonwriter almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

The Chinese Room is a argument for solipsism disguised as a criticism of AGI.



It applies with equal force to apparent natural intelligences outside of the direct perceiver, and amounts to “consciousness is an internal subjective state, so we thus cannot conclude it exists based on externally-observed objective behavior”.



25.

simonh over 2 years ago | on “The Chinese Room Thinks”

This is a decent refutation of Searle, mainly because Searle's characterisation of the Chinese Room is that it just encodes and processes syntax. Engaging in a conversation requires much more than syntax, it requires analysing and decomposing the meaning of what is being said, which for example could require constructing a model of a situation and reasoning about activities and changes in a situation. It might require critiquing suggested strategy for playing Chess for example, or commenting on economics. Actual conversation takes a lot more than just syntax, so by characterising it in this simplistic way he's stacking the deck in the reader's mind against the Chinese room.



This result is incredibly impressive. There's a lot of analysis going on here, far beyond mere processing of syntax. In this sense it exposes the inadequacies of Searle's critique of AI. It won't kill the Chinese Room argument though, it's supporters will simply extend it beyond just syntax. They will still claim there is no thinking mind producing these results and they're probably right, after all as I understand it this system is essentially a hierarchy of nets that filer input to generate output, there's no persistent running cognitive process that does anything like thinking on an ongoing basis.



The Chinese Room argument is still bunk though for many other reasons.



26.

fallingknife about 1 year ago | on “How to Think Computationally about AI, the Universe and Everything”

The Chinese room problem always seemed pointless to me precisely because we have to make assumptions about how it works.



The whole thing basically boils down to "there's this room that can speak perfect Chinese, and we don't know how it works, or how your brain works, but somehow we can say with absolute certainly that they couldn't possibly be the same."



27.

karpierz almost 2 years ago | on “ChatGPT: The Slickest Con Artist of All Time”

I think you're misunderstanding Searle's Chinese Room. It has a response for every sequence of conversation, ever. It doesn't store every conversation that has happened; it stores every possible conversation that's possible, whether it'll ever happen or not.



It would be able to handle the following exchange:



Person: "Here's a cool question, ready?"



Room: "Ready."



Person: "What was the last message I sent to you?"



It can respond appropriately to the following sentence:



Person: "Hey, I'm gonna say something. Here is a sentence. Can you repeat the previous sentence back to me?"



Otherwise, why bother with all of this AI stuff? Just build Searle's Chinese Room as an index and you have a perfect chatbot.



28.

cercatrova over 2 years ago | on “A demo of GPT-3's ability to understand long instructions”

This is a philosophical question, really. Is there ever true understanding, or just pattern matching? The Chinese Room thought experiment talks about this:



> Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that they are talking to another Chinese-speaking human being.



> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI."



> Searle then supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient papers, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing. If the computer had passed the Turing test this way, it follows, says Searle, that he would do so as well, simply by running the program manually.



> Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that is then interpreted by the user as demonstrating intelligent conversation. However, Searle himself would not be able to understand the conversation. ("I don't speak a word of Chinese," he points out.) Therefore, he argues, it follows that the computer would not be able to understand the conversation either.



> Searle argues that, without "understanding" (or "intentionality"), we cannot describe what the machine is doing as "thinking" and, since it does not think, it does not have a "mind" in anything like the normal sense of the word. Therefore, he concludes that the "strong AI" hypothesis is false.



https://en.wikipedia.org/wiki/Chinese_room



29.

mcguire almost 2 years ago | on “Can large language models reason about medical questions?”

I asked ChatGPT about the Chinese Room and asked for references. The first it came up with was "Minds, Brains, and Programs" by John Searle (1980).



The next two were, "The Chinese Room: Searle and Artificial Intelligence" edited by John Preston and Mark Bishop (2002) (Preston and Bishop did publish Views into the Chinese Room in 2002, but I can't find anything by the given title) and "The Chinese Room Argument" by David Chalmers (2006) (Chalmers has written on the topic, and published three articles in 2006, that's not one of them.



Also:



"Two cities: one, a city of “knights”, and the other of “knaves”. The residents of the two cities like to visit each other. One day, a man enters one of the cities, but doesn’t know which. He decides to ask the first person he meets where he is. The answer he receives is that he is in the “knaves” city."



produced the result:



"The man cannot be sure that the person he met was telling the truth, because the person could be a knight from the city of knights, who always lie, or a knave from the city of knaves, who always tell the truth.



"To determine which city he is in, the man could ask the second person he meets the same question. If the first person was a knight, then the second person would also be a knight, and would lie about being in the city of knights. If the first person was a knave, then the second person would also be a knave, and would tell the truth about being in the city of knaves.



"By asking the same question to multiple people and analyzing their responses, the man can eventually determine which city he is in. This problem, known as the "Knights and Knaves" puzzle, is a classic example of logical reasoning and problem-solving."



It seems to recognize the "knights and knaves" form of puzzle, but doesn't get the knights and knaves right. And it's not getting the reasoning part at all even given varying prompts.



30.

twicetwice almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

I think Searle's Chinese Room argument is sophistry, for similar reasons to the ones you suggest—the proposition is that the SYSTEM understands Chinese, not any component of the system, and in the latter half of the argument the human is just a component of the system—but Searle does believe that quantum indeterminism is a requirement for consciousness, which I think is a valid response to the argument you've presented here.



31.

CamperBob2 almost 2 years ago | on “The AI Mirror Test, which smart people keep failing”

IMO, as you can see in the Chinese Room thought experiment.



We've already left the Chinese Room a hundred miles behind. How could something like the link in the (current) top post [1] ever have come out of Searle's model?



1: https://www.reddit.com/r/ChatGPT/comments/110vv25/bing_chat_...



32.

nealabq over 1 year ago | on “Keeping up with the overwhelming pace of AI innovation”

> It's literally a scaled up Eliza



It's literally a Chinese Room. (Unless you speak Mandarin?)



https://en.wikipedia.org/wiki/Chinese_room



33.

nanidin over 1 year ago | on “There is no A.I.”

The Chinese eBay seller brought to mind the concept of the Chinese Room[0] and how it is a reflection of current LLM’s. Perhaps also influenced by a recent read of Blindsight.



[0] https://en.wikipedia.org/wiki/Chinese_room



34.

mannykannot over 2 years ago | on “Is DALL-E 2 ‘gluing things together’ without understanding their relationsh...”

The argument you make here is a reasonable one (IMHO) for the plausibility in principle of what Searle calls “strong AI”, but he claims that his “Chinese Room” argument proves that it must be mistaken. One can simply ignore him, but to refute him takes a little more effort.



It turns out that when one looks at the argument in detail, and in particular at Searle’s responses to various objections (such as the Systems and Virtual Mind replies), it is clear that he is essentially begging the question, and his ultimate argument, “a model is not the thing modeled”, is a non-sequitur.



35.

CamperBob2 almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

God, I hate that stupid Chinese Room argument. It's even dumber than the Turing Test concept, which has always been more about the interviewer than about the entity being tested.



If you ask the guy in the Chinese room who won WWI, then yes, as Searle points out, he will oblige without "knowing" what he is telling you. Now ask him to write a brand-new Python program without "knowing" what exactly you're asking for. Go on, do it, see how it goes, and compare it to what you get from an LLM.



36.

simonh over 2 years ago | on “The singularity is close?”

The Chinese room argument is a parlour trick that uses scale as a distraction. It posits a person in a room manipulating symbols to produce intelligent seeming outputs. It says, see, it’s absurd to think a person in a room with a stack of symbols could emulate intelligence.



But let’s say the room contains many billions of people, it is the size of a planet, and it contains racks of many trillions of symbols, and it spends millions or billions of years to produce an output. That’s more like the scale of a sophisticated computer system, or a brain.



Does that sound much like a man in a room with some symbols? No. Does it sound like that could do complex calculations and produce sophisticated and perhaps even intelligent outputs? Well, given enough time and scale, yes why not?



The Chinese room is pure misdirection and it amazes me anyone falls for it. There’s really no actual argument there.



37.

mypastself over 2 years ago | on “AI can't tell you it's lying if it thinks it's telling the truth”

That might make you a “Spanish room”:



https://en.m.wikipedia.org/wiki/Chinese_room



38.

j16sdiz 9 months ago | on “Is the Sun Conscious? (2021) [pdf]”

Yes. And that is one of the reasons why Chinese room thought experiment is interesting.



We know how a Chinese-understanding human would respond -- they respond exactly like the room does -- but we don't know if he actually understanding anything.



39.

HeavenBanned over 2 years ago | on “Thinking as Computation (2011) [pdf]”

The Chinese room never made sense to me because, even though the person passing the arcane symbols through the door doesn't _understand_ what is being communicated doesn't mean that's not how it works in _our_ brain. Our individual neurons (or aggregate neurons depending on how they're working and firing based on symbol triggering) don't know what symbols they're processing either. So unless Searle is saying humans don't know any languages on some fundamental level, it's kind of useless because then we'd just end up admitting (something unsavory) that nobody knows anything barring some homunculus in our brains or even worse, we can never truly have free will. Those are scary implications.



40.

msla over 1 year ago | on “Yann LeCun and Andrew Ng: Why the 6-Month AI Pause Is a Bad Idea [video]”

These thought experiments rest on the assumption that human brains are magically complex such that being "just" anything we can understand is inherently lesser. Searle's Chinese Room experiment falls for the same fallacy, more-or-less, plus the fallacy that "human plus book" isn't a single system but "huge number of individual neurons" somehow is.



41.

gowld almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

The Chinese Room is famous because it was the first popular example of a philosopher not understanding what a computer is.



42.

jeffparsons about 2 years ago | on “This Week's Hype”

I think you've understood the idea perfectly, including all the ways in which Searle's argument is rubbish.



However, I come to a different conclusion. I think that all this is exactly why the Chinese room concept is interesting — because it is so obvious, but somehow induces a huge resistance in so many people. I.e. the thought experiment itself isn't all that interesting — people's reactions to it are.



43.

fancyfredbot 9 months ago | on “Claude 3 Opus suspects it is being tested from benchmark question”

The response returned from the Chinese Room showed signs of what would have been interpreted as "suspicion" had the response come from a Human.



Did the Chinese Room suspect anything?



44.

jmugan about 2 years ago | on “CICERO: An AI agent that negotiates, persuades, and cooperates with people”

It's the Chinese room but it is effective because the game space is relatively small (compared to the real world) and you can cover that space with RL guiding NLP language generation.



45.

roddylindsay over 1 year ago | on “We need a more sophisticated debate about AI”

This is hardly an indefensible position. John Searle, for one, has argued this point for more than 40 years:



"Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese."



Later he wrote:



"I demonstrated years ago with the so-called Chinese Room Argument that the implementation of the computer program is not by itself sufficient for consciousness or intentionality (Searle 1980). Computation is defined purely formally or syntactically, whereas minds have actual mental or semantic contents, and we cannot get from syntactical to the semantic just by having the syntactical operations and nothing else. To put this point slightly more technically, the notion “same implemented program” defines an equivalence class that is specified independently of any specific physical realization. But such a specification necessarily leaves out the biologically specific powers of the brain to cause cognitive processes. A system, me, for example, would not acquire an understanding of Chinese just by going through the steps of a computer program that simulated the behavior of a Chinese speaker."



46.

gwd over 1 year ago | on “A GPT-4 capability forecasting challenge”

I've always disagreed w/ Searle re the Chinese Room. My guess is that Searle never built an adder circuit from logic gates: combining irrational elements together into something rational is the core magic of computer science.



If you want to see someone asking humans questions where they consistently fail to be rational, to the extent that they sometimes seem to approximate a stochastic parrot, read Thinking Fast and Slow by Daniel Kahneman. (It might actually be interesting to give GPT-4 some of the questions in that book, to see how similar or different they are.)



47.

mcguire about 1 year ago | on “Large language models lack deep insights or a theory of mind”

The Chinese Room gag asserts that, although the room behaves in every way like it is an intelligent Chinese speaker, we can see inside the room and determine that there is nothing there that intelligently understands Chinese.



Searle seems to see a distinction between the "substrates" (which means an LLM cannot be intelligent; it's running on regular computer hardware and there's nothing to be found which "understands"), but unless someone can point out exactly what part of the substrate is intelligent, I am going to continue to suggest that his substrate difference is exactly identical to having a soul or not having a soul.



I, as a materialist, assert that, if you dug through my noggin down to the level of subatomic particles you will never find anything that is intelligent or which understands. (Quantum mechanics does not help here, by the way. It just replaces a (rather naive) determinism with randomness---see Bell's Inequality.) There is no magic going on there. You and your tree are both doing nothing but chemistry and physics (you share something like 50% of your genes, by the way). And that means that a computer could, in principle, behave as intelligently as I do. That's "substrate independence".



Now, whether a given system does do so or not is another question entirely. :-)



48.

willprice89 about 1 year ago | on “Three things that LLMs have made us rethink”

I've always felt the Chinese Room thought experiment was founded on a complete misunderstanding of bilingualism. The only "algorithm" that can effectively transform text between English and Chinese would be one that understands the source and target texts like a human. So the Chinese Room is really just a more complicated Turing Test - a computer can only convince an observer that it "understands Chinese" if it is also able to convince an observer that it understands the conversation the same way as a human, in either language.



49.

SilasX over 1 year ago | on “Bytes are all you need: Transformers operating directly on file bytes”

That sounds like a variant of the Chinese Room problem: if a non-Chinese speaker follows a rule book to text-chat in Chinese with someone on the other side, does s/he, in actuality, “speak Chinese”?



https://en.m.wikipedia.org/wiki/Chinese_room



50.

PaulDavisThe1st over 2 years ago | on “We don't know what makes things sentient–so let's stop acting like we do”

Hofstadter and Dennett addressed the Chinese Room argument in a way that I find completely compelling. Either you are not familiar with it, or do not find it compelling. They call it the "system" position IIRC, and essentially, the entire room ("system") is conscious, even though none of the pieces are.



51.

nullc about 2 years ago | on “Galactic settlement of low-mass stars as a resolution to the Fermi paradox”

> Dr. John Searle's objection is that software is not a mind, and goes on to explain why with his Chinese Room thought experiment.



I always found that experiment to be extremely unconvincing, to put it mildly. Using that framework I can prove that you are not conscious.



I construct a "english room". You are in the back without any contact to the outside world. A Chinese speaker in the front can pass messages to you from the outside world, in English by laborious copying. Clearly you aren't conscious because the Chinese speaker can't understand the English.



Or, alternatively, no one would dispute a single neuron is not conscious and can't understand english. I pass signals into your neurons and through some electrochemical magic you reply in english, yet the neurons involved don't understand english therefor you aren't conscious.



All Searle has done is picked out one component of a composite system, concluded that in isolation it has no clue whats going on, and from that concluded the system as a whole doesn't.



We have a hard time clearly defining consciousness, and it's not clear if some AGI would be conscious (maybe it's just an emergent phenomena)-- it's also not clear if there would be any particular advantage in it being so. But in any case, I don't think the chinese room is a useful argument for anything.



52.

krick about 2 years ago | on “This Week's Hype”

Searle is proposing what he supposes to be a proof of a computer being unable to have mind in the same sense humans have mind, and thus human intelligence being something more than a "mere computation". While in fact his argument proves absolutely nothing, and I'm honestly surprised that anyone buys into this bullshit. Actually, I think that the question itself is meaningless, but it's besides the point what I think, I'll just try to explain where's the fallacy. And since his argument is "reductio ad absurdum" it's easiest to counter by assuming the opposite, and showing that there's nothing absurd about it, i.e. there's no self-contradiction here. Just keep in mind that I'm not trying to prove the opposite, I'm only assuming it for the sake of argument.



So, the opposite assumption is that there's nothing more to intelligence than a "mere computation", and all that stuff like inner dialog and "a sense of self" are just side-effects of it. That way, there's no difference between "strong AI" and "weak AI" in his terminology, no difference between "understanding" and "simulation of understanding". Then, of course, a digital computer has a mind and understands Chinese, and also the Chinese room has a mind and understands Chinese. But Searle doesn't understand Chinese and this is supposed to be a contradiction.



But there's no contradiction here, because Searle is not the Chinese room. He isn't the mind, in this model he is to Chinese room what electricity is to a digital computer, or what neurotransmitters are to human brain. So, yeah, electricity doesn't understand Chinese, serotonin doesn't understand Chinese, single human neuron doesn't understand Chinese, but Turing-test-passing computer, Chinese brain and Chinese room all do. Searle is just a detail in the Chinese room — tiny and insignificant compared to a book, containing a complete description of a program that speaks Chinese and is able to form coherent (and persistent!) opinions on every possible topic in the world.



So, the fallacy is equating Searle's role in the hypothetical Chinese room to the whole digital computer that speaks Chinese and passes Turing test with breeze.



53.

visarga about 2 years ago | on “CICERO: An AI agent that negotiates, persuades, and cooperates with people”

What does this say about the Chinese room argument?



Is the model a Chinese room or does it understand the game. If it's just a Chinese room, how come it is so effective, if it understands the game how can it be possible with just a rule machine?



54.

tim333 over 1 year ago | on “Simply explained: How does GPT work?”

I'm not sure many people take the Chinese Room argument seriously these days. I mean it basically proposes a room that is equivalent to a lookup table and says it can't understand much hence computers in general can't understand. The "hence..." bit makes no sense really."""



This is a fantastic collection of opinions on the Chinese Room! It seems like Hacker News has a pretty diverse set of perspectives on this classic thought experiment. Here are some of the most interesting and novel ideas I've extracted from your provided RAG result:

Challenging Searle's Interpretation & Premises:

Misunderstanding the Argument: Some users (like dTal) argue that people misinterpret the Chinese Room, confusing the human operator with the system itself. The argument is about the system's ability to understand, not the operator's.
"Bizarre" Functional Analogies: dTal also criticizes the common philosophical tactic of using "bizarre" analogies (like the Chinese Room) to make assertions about consciousness. They find this approach unconvincing.
The Room as a Lookup Table: tim333 dismisses the Chinese Room as basically a glorified lookup table, implying it's too simplistic to represent true AI and understanding.
Defending the "Systems Reply":

The Whole is Greater than the Sum of its Parts: Many users (rolisz, krick, PaulDavisThe1st) reiterate the "Systems Reply," arguing that the entire system (room, rules, operator) understands Chinese, even if the individual components don't.
Searle IS the Electricity: krick makes an interesting analogy, comparing Searle in the room to the electricity in a computer. Just as electricity doesn't understand, neither does the operator, but the system as a whole might.
Neurons Don't Understand Either: HeavenBanned points out that individual neurons don't understand language, yet the brain as a whole does. This challenges Searle's focus on the individual's lack of understanding.
Highlighting the Importance of Context and Embodiment:

Beyond Syntax: simonh argues that Searle oversimplifies language understanding as just syntax. Real conversation requires analyzing meaning, building models, and more, which goes beyond simple symbol manipulation.
The Chinese Room Needs to be Vast: simonh also points out the issue of scale. A realistic Chinese Room would need to be enormous and operate over vast timescales to truly simulate human understanding.
Bilingualism as the Key: willprice89 suggests that a true "algorithm" for language understanding would need to grasp both source and target languages like a human, implying a need for deep contextual knowledge.
Connecting the Chinese Room to Modern AI:

LLMs Go Beyond the Chinese Room: CamperBob2 argues that modern Large Language Models (LLMs) exceed the capabilities of the Chinese Room, implying the thought experiment is outdated.
From Thought Experiment to Reality: nopassrecover finds it interesting how quickly LLMs have made the Chinese Room a potential reality, blurring the lines between thought experiment and actual technology.
The Chinese Room and the Turing Test: throwawaygal7 suggests that the Chinese Room could be tested with a Turing Test, raising questions about how we evaluate AI intelligence.
Criticisms and Philosophical Implications:

Searle's Argument is "Rubbish": jeffparsons bluntly states that Searle's argument is unconvincing, but finds the strong reactions it provokes interesting.
The Chinese Room and Solipsism: dragonwriter suggests that the Chinese Room argument could be used to justify solipsism (the idea that only one's own mind is sure to exist).
Question Begging and Misdirection: emtel and mannykannot accuse Searle of begging the question (assuming the conclusion in his premise) and using misdirection to make his argument seem more convincing.
The Chinese Room and Cartesian Dualism: emtel also criticizes the Chinese Room for relying on Cartesian dualism (the mind-body split), implying it's incompatible with a materialist view of consciousness.
This analysis shows the enduring relevance of the Chinese Room thought experiment, even in the age of advanced AI. It continues to spark debate about the nature of consciousness, understanding, and the future of artificial intelligence.






Ok, I have another search result, extract the novel ideas, skip already covered ideas or common knowwledge





"""1.

wcoenen almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

The Chinese Room thought experiment can also be used as an argument against you being conscious. To me, this makes it obvious that the reasoning of the thought experiment is incorrect:

Your brain runs on the laws of physics, and the laws of physics are just mechanically applying local rules without understanding anything.

So the laws of physics are just like the person at the center of the Chinese Room, following instructions without understanding.

2.

circuit10 over 1 year ago | on “Microsoft's AI shopping announcement contains hallucinations in the demo”

To me the Chinese Room thought experiment seems like it's meant to show that AIs can be intelligent, not the opposite?

"Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing."

Sure, but that doesn't mean the state of the program doesn't contain any understanding or intelligence, it's just that the human doesn't have a high-level view that can be used to decode that internal state. We're not asking whether the computer chip itself understands things but whether the something contained in the program running on it does. The human could also run a physics simulation as in https://xkcd.com/505/ and recreate a human brain which would be no different to a physical brain in terms of behavior and so there would be no reason not to call it intelligent

3.

cercatrova over 2 years ago | on “A demo of GPT-3's ability to understand long instructions”

This is a philosophical question, really. Is there ever true understanding, or just pattern matching? The Chinese Room thought experiment talks about this:

> Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that they are talking to another Chinese-speaking human being.

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI."

> Searle then supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient papers, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing. If the computer had passed the Turing test this way, it follows, says Searle, that he would do so as well, simply by running the program manually.

> Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that is then interpreted by the user as demonstrating intelligent conversation. However, Searle himself would not be able to understand the conversation. ("I don't speak a word of Chinese," he points out.) Therefore, he argues, it follows that the computer would not be able to understand the conversation either.

> Searle argues that, without "understanding" (or "intentionality"), we cannot describe what the machine is doing as "thinking" and, since it does not think, it does not have a "mind" in anything like the normal sense of the word. Therefore, he concludes that the "strong AI" hypothesis is false.

https://en.wikipedia.org/wiki/Chinese_room

4.

gianpaj over 1 year ago | on “Searle vs. Boden (1984) – Consciousness and Understanding in Machines [vide...”

The Searle vs. Boden debate revolves around the philosophical question of whether machines can truly possess consciousness and understanding, or if they are limited to simulating intelligent behavior without genuine understanding. This debate primarily involves John Searle, an American philosopher, and Margaret Boden, a cognitive scientist and philosopher.

John Searle is best known for his thought experiment called the Chinese Room argument, which he presented in 1980. In this argument, Searle imagines himself locked inside a room with a set of instructions that enable him to respond to written Chinese symbols without understanding the Chinese language. He argues that even though he can generate appropriate responses and appear to understand Chinese, he does not genuinely understand the language because he is simply following a set of syntactic rules without grasping the meaning behind them. Searle extends this argument to claim that computers, no matter how complex or sophisticated, can only manipulate symbols according to algorithms but lack true understanding.

Margaret Boden, on the other hand, represents a more optimistic view about machine understanding and consciousness. She acknowledges Searle's Chinese Room argument but argues that it overlooks the potential of computational systems to exhibit genuine understanding. Boden emphasizes the distinction between "strong" AI, which aims to create machines with consciousness and understanding comparable to human intelligence, and "weak" AI, which focuses on simulating intelligent behavior without claiming genuine understanding. She argues that while Searle's argument may hold for weak AI, it does not necessarily refute the possibility of strong AI.

Boden suggests that machine understanding can emerge through the combination of syntactic manipulation of symbols (as Searle describes) and semantic processing, which involves the interpretation and mapping of symbols to meaning. She highlights the importance of cognitive architecture, which encompasses not only the symbolic manipulation but also the grounding of symbols in sensory-motor experiences. According to Boden, by providing machines with rich sensory inputs and allowing them to interact with the physical world, it becomes possible for them to develop genuine understanding and consciousness.

5.

roddylindsay over 1 year ago | on “We need a more sophisticated debate about AI”

This is hardly an indefensible position. John Searle, for one, has argued this point for more than 40 years:

"Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese."

Later he wrote:

"I demonstrated years ago with the so-called Chinese Room Argument that the implementation of the computer program is not by itself sufficient for consciousness or intentionality (Searle 1980). Computation is defined purely formally or syntactically, whereas minds have actual mental or semantic contents, and we cannot get from syntactical to the semantic just by having the syntactical operations and nothing else. To put this point slightly more technically, the notion “same implemented program” defines an equivalence class that is specified independently of any specific physical realization. But such a specification necessarily leaves out the biologically specific powers of the brain to cause cognitive processes. A system, me, for example, would not acquire an understanding of Chinese just by going through the steps of a computer program that simulated the behavior of a Chinese speaker."

6.

boole1854 about 2 years ago | on “An End to Pornography, Sophistry, and Panty Raids: on Epstein's Edge.org (2...”

> Why shouldn't sufficiently-sophisticated symbol manipulation, producing results the same as "conscious understanding" (which seems to be the same as "semantics" as Searle was using it), be identical to "conscious understanding"?

This is what the Chinese Room argument is trying to answer. In the room, there is a human, and the human is performing symbol manipulation according to the rules listed in various books. However, (1) the human has conscious understanding of various topics, but Chinese is not one of them, and yet (2) the outsider is able to send and receive messages in / out of the room in Chinese, indistinguishable from communicating with someone who does understand Chinese. This shows that an entity performing symbol manipulation which results in a Chinese conversation is not the same as that entity understanding a Chinese conversation. To say otherwise would require that the conjunction of 1 and 2 is not possible, but it is clearly possible (or at least this is clear to most people).

There are various replies which address the argument coherently.

The illusionism theory of consciousness denies that the conjunction of 1 and 2 is possible because 1 on its own is not possible. That is, the human in room does not have conscious understanding because no one has conscious understanding of anything (since consciousness does not exist). This theory has not widely caught on because most people think it is just too obvious that conscious experience does exist.

Another reply is that the conjunction of 1 and 2 is not possible because as soon as the human executes enough of the symbolic manipulation rules, they do in fact come to understand Chinese. The trouble with this reply is that it runs up against the strong concrete experience that we have which shows this is not remotely how gaining an understanding of something works.

A few other remarks:

(1) The argument does not show, and is not intended to show, that building a conscious machine is impossible. Searle explicitly thought, and wrote in his original paper, that it was possible to build such machines.

(2) The argument does not show, and is not intended to show, that conscious machines require biological substrates. Searle wrote in his original paper that it was an open question what substrates could be used to build conscious machines, and that it might well be possible to build conscious machines without any biological components.

(3) A reply which says, "alright, the human in the room does not understand Chinese, but maybe the 'room itself' does have a conscious understanding of Chinese" is perhaps interesting. Some people, including Searle, have debated how likely this is. However, it is beside the point of the original argument, which is just about showing a difference between the concept of syntax and the concept of semantics. (And, again, the argument shows that there is a difference by exhibiting a counterexample where an entity is able to follow arbitrarily complex syntactic rules and yet that entity does not have a corresponding understanding of the natural language which the rules implement.)

(4) For some, e.g. people in software development who work with formal languages on a regular basis, the fact that there is a difference between syntax and semantics is incredibly obvious. If you think it is simply obvious that there is a difference, then congratulations, you understand the concepts involved. In that case, Searle's argument is superfluous, and it's not intended for you. His argument is directed at those working in philosophy of mind or related fields who were not familiar enough with the difference between syntax and semantics to understand that there is a clear difference.

edited to add a reply about Point 1 begging the question: just to clarify, the ARGUMENT I posted above is not Searle's argument. It was an arguing I was giving to the previous poster to illustrate the fact that Searle's argument was not equivalent to claiming that consciousness does not exist in humans because individual neurons have no understanding of what the entire brain is doing. My starting point was taking Searle's conclusion as a given, in order to work out consequences from it.

7.

dTal 4 months ago | on “Anthropic publishes the 'system prompts' that make Claude tick”

I think you have misunderstood Searle's Chinese Room argument. In Searle's formulation, the Room speaks Chinese perfectly, passes the Turing test, and can in no way be distinguished from a human who speaks Chinese - you cannot "pop the illusion". The only thing separating it from a literal "robot that speaks Chinese" is the insertion of an (irrelevant) human in the room, who does not speak Chinese and whose brain is not part of the symbol manipulation mechanisms. "Internal cause and effect" has nothing to do with it - rather, the argument speciously connects understanding on the part of the human with understanding on the part of the room (robot).

The Chinese Room thought experiment is not a distinct "scenario", simply an intuition pump of a common form among philosophical arguments which is "what if we made a functional analogue of a human brain that functions in a bizarre way, therefore <insert random assertion about consciousness>".

8.

Barrin92 almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

The notion that the Chinese room is 'mysticistic' is one of the most bewildering responses to it to the point where I don't think people who make that argument actually understood Searle.

Searle, simplified holds that consciousness is a property of very particular biochemical systems, not a disembodied thing that exists independent of the matter it inhibits.

It's the rational-oriented circles ironically enough who almost dabble in a Cartesian Dualism by positing "computation" or "minds" as distinct from matter. In fact with on occasion almost religious-sounding implications.

9.

twicetwice almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

I think Searle's Chinese Room argument is sophistry, for similar reasons to the ones you suggest—the proposition is that the SYSTEM understands Chinese, not any component of the system, and in the latter half of the argument the human is just a component of the system—but Searle does believe that quantum indeterminism is a requirement for consciousness, which I think is a valid response to the argument you've presented here.

10.

emtel almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

I've read one of Searle's popular books (Rediscovery of the Mind) and many other shorter writings. I'm aware he rejects dualism - my point is that the Chinese Room argument wouldn't even make sense to someone who had never been contaminated by dualist notions. Searle rejects it, but it has crept back in, imo.

His arguments against the "systems response" have to stand or fall on their own. The Chinese room doesn't have anything to add. If the "systems response" fails, then we already know a computer can't be conscious without any additional thought experiments.

Lastly, I think he goes much too far in dispelling the mystery of the word "computer". As Dennett and others have pointed out, a working chinese room that displayed clear signs of consciousness would be an artifact far beyond current human comprehension, either in scale or sophistification or both. Searle asks you to imagine a human taking slips of paper and calmly consulting a rule book and producing new slips of paper, and claims that it is "obvious" that this system can't be conscious. Well, I agree, but it's equally obvious that the system couldn't possibly display conscious-seeming behavior.

The best defense of the CR that I've heard is that it is only intended to prove that a _purely formal_ system (i.e. GOFAI) can't be conscious. I find this more plausible, but I still don't think the CR adds anything to that argument. And, to reiterate the previous paragraph, I think if you had a formal system that everyone agreed _seemed_ conscious, it would be such a complex artifact that it wouldn't be obviously unconscious in the way that, say, an eliza bot is.

11.

crdrost about 1 year ago | on “LLMs can't self-correct in reasoning tasks, DeepMind study finds”

The Chinese room argument is just missing kinda one basic premise and people don't realize it and argue around the premise without actually addressing it.

“Can we agree that of the many different things folks mean when they say your consciousness is ‘causally generated by your brain,’ that at least we can say your brain in its present state is only capable of generating exactly one conscious process at a time, namely you?”

So it's a sort of causal sufficiency, Searle believes that there is something about this squishy machine that the physics is doing that makes it conscious, and that's being caused very directly by the squishy machine. It's not some magic.

On the other hand, if you believed in souls, you might think that a given brain contained both the soul of a human person, and the soul of some demon possessing them—two conscious processes, one of which was stuck “along for the ride.” [Searle is also interested in the cases of split-brain patients where you have a similar phenomenon because essentially two separate brains inhabit a single body. He has I think mentioned at a talk that one of the interesting things about consciousness to him is that it all gets Unified, so it's interesting that if the two parts of the brain can talk to each other they merge their consciousnesses into one more powerful consciousness, rather like (my analogy, not his) how if you have two water droplets on a plastic plate and you push them with a toothpick together, at some point they merge into one bigger drop.]

Now the Chinese room thought experiment is not about the computational model of consciousness—not directly! It was always phrased as a rebuttal of the Turing test in particular, and the computational model only indirectly after the Turing test falls. Note that the Turing test has no direct beef with the casual sufficiency axiom, which is why it went unstated originally. According to the Turing test, written text goes in, written text comes back out, a dialogue appears to happen to the outsider, this is sufficient to conclude conscious understanding of the language used, which confirms consciousness.

Searle’s objection is, “if it's really all about inputs and outputs and not how I get it done, then you've left out what for me feels like the most important part about understanding a language: understanding it is part of how I get it done!” Right? You understand English because you can phrase your ideas into it, you can mold it to suit you, and it can (when heard) so mold you—it’s not just because the words can come out of your mouth, triggered by other words that came in through your ears. The Turing test has always ever stated “don't worry what happens in the middle” and Searle is saying “but for me understanding is part of the details that are happening in the middle!”

So where does the computational theory come in? It comes in because Searle wants to make this argument rigorous! He says, “if your computational theory is true, then there is in fact another way that I can speak a language—words in, words out—where I don't understand a word of Chinese. I am Turing complete, I could memorize a program for speaking Chinese and happen to execute it flawlessly and at no point would my ideas get into Chinese, at no point would the things that I heard mold me. So the Turing test is crap!”

The prominence of the words “I, me” is what invokes the idea of causal sufficiency here, “I have the right inputs and outputs, but I don't understand.” The Turing test is not able to distinguish between multiple consciousnesses instantiated by the same hardware, if such is even possible. The inputs and outputs go into the same box, as far as Turing is concerned as long as only one conversation happens, there's only one person in there.

This does do a great job of defanging the Turing test, because all of the ways that you might weaken the concept to address this major limitation do make it sound completely tautological. “Yeah well something consciousnessy is happening in that box but I don't know what.” / “Okay then why are we even talking about it.” / “Because computers can speak!” / “Right, so we care that computers can speak because they can speak?” / “No, like, we gotta give them rights now, or some shit.” / “John Searle already has human rights, if he's memorized a program that lets him speak Chinese without understanding it, you're saying we need to give that program human rights?” / “Yeah!” / “So uh is it murder if Searle decides that running the program is no longer fun? Is he a slave to this program forever?” / “Uhhhh...”

It doesn't defang the computational model, not directly. But the computational model does imply that VMs exist. We use them every day! And that's all that the Chinese room is, it's running a VM inside of another computer, one consciousness carrying a separate consciousness inside of it, a willful sort of demonic possession. The only thing the Chinese room has to say about this, is that we don't use our language very well if it is true. Philosophers who believe in that will need to generate an alternative language that is able to distinguish between “I am doing it” and “I am sustaining a daemon who is doing it,” because for them that's a real difference, you might have a hundred consciousnesses in your head that you don't have direct access to. That is a necessary part of believing that consciousness is software, you don't know if you're in a VM inside your brain, you don't know if something you're doing is actually secretly a Brainfuck program instantiating another VM inside your consciousness, software embeds within software, that's a core feature of software.

But of course Searle thinks that that's kind of ridiculous because he thinks that it's obvious that consciousness is something that the squishy wetware of the brain does, and this forces him to believe in that causal sufficiency—“my brain is only sustaining one consciousness, namely me,”—which the computationalists cannot ever agree with because that's not how software works. Anyone who believes in that causal sufficiency, even if they don't have the same basis that Searle does for believing in it, also thinks that the computationalists are ridiculous.

But the point is, that's happening at a level way above the Chinese room argument, Chinese room baseball came and went, now this is a whole separate game being played at the same ballpark afterwards.

12.

kghe3X over 2 years ago | on “Google Sidelines Engineer Who Claims Its A.I. Is Sentient”

A few things wrong with this in my opinion.

1. If simulation of a mental phenomenon is not equivalent, in essence, to the phenomenon itself, then what's a meaningful difference between the two? What is this elusive essence of the phenomenon and why is it a requirement in any meaningful definition of intelligence, consciousness, sentience, etc? If we build a silicon brain some day that passes as human, does it matter that its essence is different from a carbon-based brain?

2. Searle's Chinese Room neglects to acknowledge that the room itself is what would be exhibiting intelligence, not the homonculus within. His argument fails by presuming without evidence that consciousness is not an emergent phenomenon.

3. Of course we don't expect the computer to urinate on the desk, just as we don't expect a silicon mind to produce serotonin. That's irrelevant to the question of whether or not it exhibits the qualities or behaviors that we associate with consciousness.

I guess I just don't understand why there needs to be this added, untestable, metaphysical requirement attached to all of these words. And my question to those who suggest this is, what would it convince you otherwise?

13.

eouwt about 2 years ago | on “This Week's Hype”

Searle's Chinese room is one of those things it's obvious most people talk about without ever having bothered to actually read. Searle does not say computers will never think. Early in the paper he even says - obviously machines can think because we are precisely examples of such machines. His point is that maybe it takes a certain kind of processing to yield what we call consciousness, and that may be a subset of the processes that yield outwardly similar behaviour.

14.

nullc about 2 years ago | on “Galactic settlement of low-mass stars as a resolution to the Fermi paradox”

> Dr. John Searle's objection is that software is not a mind, and goes on to explain why with his Chinese Room thought experiment.

I always found that experiment to be extremely unconvincing, to put it mildly. Using that framework I can prove that you are not conscious.

I construct a "english room". You are in the back without any contact to the outside world. A Chinese speaker in the front can pass messages to you from the outside world, in English by laborious copying. Clearly you aren't conscious because the Chinese speaker can't understand the English.

Or, alternatively, no one would dispute a single neuron is not conscious and can't understand english. I pass signals into your neurons and through some electrochemical magic you reply in english, yet the neurons involved don't understand english therefor you aren't conscious.

All Searle has done is picked out one component of a composite system, concluded that in isolation it has no clue whats going on, and from that concluded the system as a whole doesn't.

We have a hard time clearly defining consciousness, and it's not clear if some AGI would be conscious (maybe it's just an emergent phenomena)-- it's also not clear if there would be any particular advantage in it being so. But in any case, I don't think the chinese room is a useful argument for anything.

15.

emtel almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

The Chinese Room is a really, really bad thought experiment. It's just an attempt at misdirection. It asks you to imagine a computer (one that operates on slips of paper according to rules that are written down in a big book), puts a human inside that computer, and then says that because the human doesn't understand what the computer program is doing, the computer program isn't conscious.

But this is just question-begging. The experiment purports to resolve the question "can a computer be conscious", and answers it by showing you a computer and claiming that it is not conscious.

Worse, the Chinese Room is straight-up Cartesian dualism. By placing the human inside the computer, Searle is preying on your Cartesian intuition that there must be a theatre of the mind where reality is experienced. And since Searle's homunculus doesn't understand Chinese, nothing can be experienced, therefore no conscious experience is happening in this system.

If I'm wrong, and the Chinese Room thought experiment is conclusive, it must be possible to reformulate it without the human. After all, the human does nothing but follow rules in a purely mechanistic fashion. Can anyone offer a reformulation of the experiment, that has no human, that they still feel is convincing?

16.

simonh over 2 years ago | on “The Chinese Room Thinks”

This is a decent refutation of Searle, mainly because Searle's characterisation of the Chinese Room is that it just encodes and processes syntax. Engaging in a conversation requires much more than syntax, it requires analysing and decomposing the meaning of what is being said, which for example could require constructing a model of a situation and reasoning about activities and changes in a situation. It might require critiquing suggested strategy for playing Chess for example, or commenting on economics. Actual conversation takes a lot more than just syntax, so by characterising it in this simplistic way he's stacking the deck in the reader's mind against the Chinese room.

This result is incredibly impressive. There's a lot of analysis going on here, far beyond mere processing of syntax. In this sense it exposes the inadequacies of Searle's critique of AI. It won't kill the Chinese Room argument though, it's supporters will simply extend it beyond just syntax. They will still claim there is no thinking mind producing these results and they're probably right, after all as I understand it this system is essentially a hierarchy of nets that filer input to generate output, there's no persistent running cognitive process that does anything like thinking on an ongoing basis.

The Chinese Room argument is still bunk though for many other reasons.

17.

ftxbro over 1 year ago | on “Chinese Room”

> Searle supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient papers, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing.

This is an interesting and valuable thought experiment!

> The Chinese room argument holds that a digital computer executing a program cannot have a "mind", "understanding", or "consciousness", regardless of how intelligently or human-like the program may make the computer behave.

This doesn't seem to follow from it. For example what if they made artificial pieces of brain made from computer chips and replaced my brain with it bit by bit like a brain of Theseus. This is how your brain already works to some degree, the atoms in your brain aren't the original ones. I don't see how they say this replaced brain cannot have a 'mind' or 'understanding'.

18.

hackinthebochs over 2 years ago | on “DALL-E 2 generates images of Kermit The Frog in various films”

The sleight-of-hand in the Chinese room is that Searle asks us whether the man in the room understands Chinese. Of course not. This is like asking whether my CPU knows how to decode h264. The real question is whether the embodied process instantiated by the actions of the man, along with the other involved components in the room, understands Chinese. But the argument doesn't touch this claim.

19.

jedharris 10 months ago | on “Sora: Creating video from text”

That matches how philosophers typically talk about the Chinese room. However the Chinese room is supposed to "behaves as if it understands Chinese" and can engage in a conversation (let us assume via text). To do this the room must "remember" previously mentioned facts, people, etc. Furthermore it must line up ambiguous references correctly (both in reading and writing).

As we now know from more than 60 years of good old fashioned AI efforts, plus recent learning based AI, this CAN be done using computers but CANNOT be done using just ordinary if - then - else type rules no matter how complicated. Searle wrote before we had any systems that could actually (behave as if they) understood language and could converse like humans, so he can be forgiven for failing to understand this.

Now that we do know how to build these systems, we can still imagine a Chinese room. The little guy in the room will still be "following pre-written scripted algorithmic procedures." He'll have archives of billions of weights for his "dictionary". He will have to translate each character he "reads" into one or more vectors of hundreds or thousands of numbers, perform billions of matrix multiplies on the results, and translate the output of the calculations -- more vectors -- into characters to reply. (We may come up with something better, but the brain can clearly do something very much like this.)

Of course this will take the guy hundreds or thousands of years from "reading" some Chinese to "writing" a reply. Realistically if we use error correcting codes to handle his inevitable mistakes that will increase the time greatly.

Implication: Once we expand our image of the Chinese room enough to actually fulfill Searle's requirements, I can no longer imagine the actual system concretely, and I'm not convinced that the ROOM ITSELF "doesn't have a mind" that somehow emerges from the interaction of all these vectors and weights.

Too bad Searle is dead, I'd love to have his reply to this.

20.

epups over 1 year ago | on “The new Bing runs on OpenAI’s GPT-4”

>Quoting Searle himself, "The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have."

> I think the most succinct description of his error is substituting the (lack of) understanding of a part of the system (the man) for the understanding of the entire system (the rules and file cabinets, etc.). But I'm interested in learning that I'm mistaken.

As you know, there have been many replies to this thought experiment, and some of the most interesting ones (to me) go in the direction you went here, ie, where is "understanding" occuring? The most basic version of the Chinese Room does intend to make you see yourself literally as a man who does not understand any Chinese and is just asked to look up symbols in a list. Perhaps that man doesn't understand Chinese, but the room as a whole at least gives the impression that it does.

However, I think the most important aspect is not this "intuition pump" as Daniel Dennett calls it. To me, what is key here is that we can all agree that such a Chinese Room, or ChatGPT for that matter, does not necessarily replicate the fundamental mechanisms of human cognition. Then, it follows that other human properties such as awareness or qualia do not necessarily emerge from such cognitive architectures in the same way that it emerges from our brains.

To me, Searle's point is ultimately that we don't know enough about the human mind to be able to judge whether it can be replicated artificially. And now that we have almost literally developed a Chinese Room, we can see that clearly. The arguments you bring up in your last paragraph are a great example of that, it's just very hard to conceive that this thing is conscious at all, even though it is capable of producing output that could convince people of that.

Regarding Searle's quote that you brought up, I think "solely on that basis" is doing a lot of heavy lifting there, but it does align with what I said previously. He is saying that simply producing intelligent output, like in 1974 translation would represent, does not mean you are reasoning in a human way.

21.

lazyant over 1 year ago | on “Richard Stallman's thoughts on ChatGPT, AI and their impact on humanity”

The Chinese Room Argument holds that a digital computer executing a program cannot have a "mind", "understanding", or "consciousness",[a] regardless of how intelligently or human-like the program may make the computer behave.

https://en.wikipedia.org/wiki/Chinese_room

22.

mcguire about 1 year ago | on “Large language models lack deep insights or a theory of mind”

The Chinese Room gag asserts that, although the room behaves in every way like it is an intelligent Chinese speaker, we can see inside the room and determine that there is nothing there that intelligently understands Chinese.

Searle seems to see a distinction between the "substrates" (which means an LLM cannot be intelligent; it's running on regular computer hardware and there's nothing to be found which "understands"), but unless someone can point out exactly what part of the substrate is intelligent, I am going to continue to suggest that his substrate difference is exactly identical to having a soul or not having a soul.

I, as a materialist, assert that, if you dug through my noggin down to the level of subatomic particles you will never find anything that is intelligent or which understands. (Quantum mechanics does not help here, by the way. It just replaces a (rather naive) determinism with randomness---see Bell's Inequality.) There is no magic going on there. You and your tree are both doing nothing but chemistry and physics (you share something like 50% of your genes, by the way). And that means that a computer could, in principle, behave as intelligently as I do. That's "substrate independence".

Now, whether a given system does do so or not is another question entirely. :-)

23.

epups over 1 year ago | on “The new Bing runs on OpenAI’s GPT-4”

Sorry but I think you got the first part wrong. It is not at all arguing that sentience cannot emerge from non-sentient parts: Searle is a materialist. Some of your arguments, such as the fact that single neurons are also not sentient, are addressed by him.

The point of this thought experiment is to illustrate that merely replicating a behavior - in that case translation - does not say anything about sentience. The Chinese Room may produce intelligent output, but it does not reason as a human does. I find it remarkably prescient. ChatGPT can produce remarkably intelligent output, should we consider it human? If not, then you implicitly agree with Searle, at least in some level.

24.

masswerk over 2 years ago | on “A demo of GPT-3's ability to understand long instructions”

Well, this, the Chinese Room, is still pretty much a behavioristic work-around (as it is still attempting to argue without any reference to internal states.)

Even, if we don't (clearly) understand what "understanding" means, or, at least, aren't able to provide a sane definition, we do know about the semantics of the term and the kind of connotations that come with it. Like a reflexive component. (Which wasn't much of a problem in the age of behaviorism, as this had to be ignored by requirement anyway. If there is no acknowledged difference between a human and a pigeon, what is the problem with computers, as far as the model is concerned?) So we do have a notion of the semantic field and its implications. And these are, well, quite disastrous for this purpose.

25.

pringk02 over 1 year ago | on “A non-technical explanation of deep learning”

Searle's argument in the Chinese Room is only that passing the Turing Test isn't enough to prove evidence of Mind (capital 'm' to distinguish it at the philosophical jargon term, and all it entails). He does hold the stance that he doesn't think Computationalism (in the style of Dennet) is correct. I'm not sure if he personally feels the Chinese Room argument refutes that stance as a whole, but I believe the general consensus is that, as originally formulated in his essay, it does not stand as a total refutation of Computationalism, without reading between the lines or squinting your eyes a bit. Searle does have a wider stance that he does not think computations can have things equivalent to mental states, especially intentionality. Obviously there is a whole separate debate to his correctness there, but I'm skipping over it to just discuss the Chinese Room

That passing the Turing Test is not enough to exhibit evidence of Mind is not that controversial today. GPT-4 could easily pass the Turing Test as it was originally formulated. There are not many out there that think it possesses conciousness or intentionality or any mental states at all really. We'd generally agree now that passing the Turing Test is only a step towards creating an actual artificial mind (how large or small a step is still up for debate).

Anyway, all this is a tangent as I still don't understand why the original commenter feels this article provides a refutation of the Chinese Room argument when it seems (to me) to reinforce it. I'm just curious on that perspective and was interested in hearing more.

26.

Barrin92 almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

I agree that Searle is often too strong in his claims, but here is what I think people need to take away from the Chinese Room and why it is useful.

That solving a problem, or an array of problems that a conscious mind can solve may be perfectly able to be solved by a non-conscious machine. That the consciousness part may not be relevant in the problem-solving process, and that consciousness may be unique to living beings who have distinct physiology, anatomy, ways to process information, that computers as they exist now do not have. That a simulation is not necessarily the same thing as the thing it simulates, and does not have all of its properties.

These are good questions none of which are mystic and which following from Searle's argument. And I think you're really underestimating to what degree people are lured in by the 'magic' of neural nets because else we wouldn't be in this thread. A neural net that does some function approximation is not 'slightly conscious' unless we're going full in on panpsychism. Even from a functionalist perspective, the neural net performs nothing that is reasonably involved in what we consider to be responsible for consciousness. Nobody ever claimed a TI-83 is 'slightly conscious', or that Stockfish is merely because it plays exceptional chess.

27.

dwallin 3 months ago | on “Anthropic publishes the 'system prompts' that make Claude tick”

Searle's argument in the Chinese Room is horribly flawed. It treats the algorithm and the machine it runs on as the same thing. Just because a human brain embeds the algorithm within the hardware doesn't mean they are interchangeable.

In the Chinese Room, the human is operating as computing hardware (and just a subset of it, the room itself is substantial part of the machine). The algorithm being run is itself is the source of any understanding. The human not internalizing the algorithm is entirely unrelated. The human contains a bunch of unrelated machinery that was not being utilized by the room algorithm. They are not a superset of the original algorithm and not even a proper subset.

28.

slibhb over 2 years ago | on “The singularity is close?”

> A lot of people find it credible that a hugely complex fast computer system could be intelligent.

But not John Searle...so it seems clear that scale has nothing to do with the thought experiment. For Searle, a vast room or an army of people to carry out instructions make no difference.

> Many of them seem to find it convincing that a man in a room with a book can’t “understand Chinese” when he personally doesn’t

The people who read the thought experiment this way don't understand it. In the thought experiment neither the computer program (no matter how powerful the computer is) nor the man and the room understand Chinese.

> But the only difference between them is scale.

No, the point of putting a man in the room is that we all agree that a human is capable of understanding. From his vantage point in the room, he can see that the program works by executing instructions without any understanding. All the room does is demystify a hypothetical computer program that can pass a turing test.

Searle is not arguing that a machine with understanding/consciousness is impossible. He's arguing against behavioralism ("if it can pass a turing test it understands"). He take the psychological phenomenon of "understanding" (something we're all familiar with) seriously, he thinks it represents a real thing, and that a computer that can pass a turing test doesn't necessarily have it.

29.

jodrellblank 3 months ago | on “Stephen Fry – AI: A Means to an End or a Means to Our End?”

> "For one thing, I expect we’d not see so many cases of them chasing (if you will) the prompt and request into silliness."

In the one example I gave you saw one output and declared it "not understanding the concept of bridging". I'm asking specifically that output, how would it look different if ChatGPT had some understanding of the concept of bridging? You're back to arguing "it's not human level!" which was not my claim. My claim is that it's above zero level. In another comment I asked it to use the concept of bridging in new ways, and it provided sentences which have no hits on Google but are plausibly the kind of thing I might see in a book from a human author.

> "There’s no understanding"

Say to your pet "I like it when you do human-like things such as standing on two feet. Come up with more human-style things for more treats" and it won't. You can ask ChatGPT to come up with more uses of the bridging concept, and it does. That is demonstrating understanding at higher than rock level and higher than rat level, and you can't reject that evidence just by repeatedly saying "there's no understanding there's no understanding there's no understanding".

> "they can also be induced to correct the already-correct, so that means nothing."

So can I; if my boss tells me there is an error and I need to correct it, I might correct a non-error to please them. Knowingly ("I'll change this part from correct to wrong if that pleases them") or unknowingly ("if they tell me there is an error there must be one, I'll take a guess that this bit is wrong and put something else here"). Does that show I have no understanding?

> "Every single part of what an LLM does can be replicated with big books of lookup tables, dice, and a list of rules. There’s nowhere for anything to do the understanding to exist."

You're doing the God of the Gaps argument with the human brain - an LED screen is RGB pixels, there's nowhere for a picture of a cat to exist separately from bright and dark pixels. A book is printed characters, there's nowhere for a story to exist separate from blobs of ink on paper. A brain is meat grown from a foetus, uses ~20 Watts of energy, if the blood supply is cutoff then it dies, if it gets too hot or cold then it dies, there are many areas which can be damaged and harm something like leg movement but there is no single area which can be damaged which stops 'understanding' but leaves everything else unchanged, there are no examples of people being decapitated, having no brain, having brain death, and still having 'understanding' provided by whatever other thing you are implying exists and does understanding.

There's nowhere for anything to do the understanding to exist, unless there is a) new physics which aligns perfectly with every observation we have about the brain but also augments it an adds some magical 'understanding' thing which can't be done or simulated in software. b) something non-physical such as a soul which is tied closely to the meat and powered by the food and blood and can't be tied to silicon because reasons. c) ??? As far as I can see this isn't reasoning from anything more convincing than you not wanting to accept the Occam's Razor simpler explanation that a purely physical information processing system can understand.

(Or that humans don't understand and it's all some weird illusion; the picture of the cat is not in the LED screen, it is in the eye of the beholder. The understanding isn't in your behaviour, it's in the beholder's interpretation, I believe you understand because you demonstrate the behaviours of understanding. We are seeing intelligence in others where there isn't any. And that view turned on ourselves is our own perception of our own understanding - we see ourselves identifying patterns, extrapolating patterns, continuing coherent sentences, and conclude that we must have 'understanding' as a thing separate from those behaviours).

> "The thing about the Chinese Room is that we comprehend the entire process, and there’s no room for some unknown factor affecting the output"

We don't comprehend the entire Chinese Room; the instructions that Searle is following are a massive handwave. Does following the instructions require Searle to make human judgements on where to branch? Then it's offloading understanding onto his human brain. Does it not require that but it still outputs coherent responses? Then the instructions must encode intelligence in them in some way - if intelligent behaviour doesn't demonstrate intelligence we're in non-scientific nonsense land.

Peter Cochrane wrote about 'dying by installments' of a human turned into a cyborg replaced bit by bit, Ship of Theseus style. We can do similar and make up a Cochrane's Chinese Brain - instead of a neuron firing and affecting the connected neurons, it raises an alert and Searle walks over and writes down the firing pattern on a scratch pad, walks to all the other relevant neurons, and taps in the firing pattern on an input device, without understanding the information content of the firing pattern. Does the brain keep responding coherent Chinese but no longer understand Chinese?

30.

int_19h 7 months ago | on “Is artificial consciousness achievable? Lessons from the human brain”

> If someone now asks you questions about colored objects you can answer them, but I assume you grant that neither the colorblind person, nor the machine, nor the two as a system have conscious experiences of color vision as you have.

It really depends on the setup. If the system is primed with knowledge of what color various things are (so e.g. it can say that grass is green because it is in the knowledge base), then, no, it does not experience color vision. It's just regurgitating facts.

On the other hand, if you actually have some kind of sensor that is capable of perceiving color, and you provide the output of that sensor to the colorblind person inside the room, who interprets the signals (say, represented as numbers) according to the rules, and those rules result in the system as a whole being able to say things like "apple is red" when presented with a red apple, then yes, I would in fact argue that the system does consciously experience color vision.

> And I don't think your assertion about Searle's belief is correct.

Searle claimed that computers "merely" use syntactic rules to manipulate symbol strings, but have no "understanding" of semantics, and that Chinese room demonstrates that this is not sufficient for consciousness. This was not just about correctly modelling outward functions, though - quite obviously, the room has a lot going on inside, and of course you can model neural nets without physically simulating neurons, either. Quite frankly Searle's attempt to make some kind of qualitative distinction between biology and computation is nonsensical, because it's the same physics all the way down, and it is all representable as computation.

31.

mcguire over 1 year ago | on “We need a more sophisticated debate about AI”

As an alternative to Loquebantur's comment, Searle's is trivially reversed: he has no way to demonstrate that he has "semantic" capabilities.

On the first hand, all communication is purely syntactic; letters, words, gestures, and every other formulation of communication is the transfer of symbols, which cannot carry semantic information. They can point to it, they can imply it, but you'll have to dance pretty hard to actually prove the transfer of semantic information. If you and I both have some semantic concept associated with the symbol "China" in our mental landscapes, neither I nor you, nor anyone, has any way determining whether that is the same semantic concept. In fact, it almost assuredly isn't, since those semantic concepts would have to be built from our unique personal histories, which are unique.

On the second, the fundamental assumption of semantic communication is that I feel like I have semantic shenanigans going on in my mental life and Searle sort of looks like me, so I should assume that he has semantic stuff going on. Not only is that not a particularly well-founded assumption, I can't even be sure I have semantic contents in my mental life. Introspection isn't exactly reliable, right?

On the third hand, you have a choice: if you do not agree with Penrose, et al, on the quantum nature of the magic of mind, then Searle's "biologically specific powers" are chemistry. Now, I never actually took a chemistry class, but I don't believe there is any chemical reaction that comes with a label of semantics. It's all physics anyway, which I know cannot introduce semantics.

If you do agree with Penrose (which I understand Searle does), the situation is not actually better. Quantum mechanical antics are random, right? Bell's Inequality and all? Being fundamentally, quantum mechanically, random is not in any sense better from a semantic standpoint than being syntactically deterministic.

So my bottom line is that I don't know if I have semantic contents, I absolutely don't know if Searle has semantic contents (and I'm not feeling generous), and I do not know of any way that he could prove to me that he does have "semantic contents". So, yeah.

You notice I only addressed the second paragraph from Searle. That is because the fundamental flaw in his Chinese Room "demonstration" is on obvious display in his final sentence, "The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese"; it is purely, straight up, dualistic. He is asserting that the person in the room has some magic property of "consciousness or intentionality" and that if the person in the room does not have the "semantic capability" of understanding Chinese, then it is impossible despite all evidence that anything in the scenario has the semantic capability of understanding Chinese.

You could put a native Chinese speaker in the room, reading and responding to the inputs, and he claims it would have "consciousness or intentionality"; you then replace her with me (who didn't exactly dazzle at Spanish and Latin) and even if I produce the same outputs in response to the inputs as she does, there is no "consciousness or intentionality". The difference is simply a magic, intangible, imperceptible soul.

Whoo. Went off on that one.

32.

dzdt about 2 years ago | on “ChatGPT passes the 2022 AP Computer Science A free response section”

[me]

Write a five paragraph essay on Daniel Dennett's responses to Searle's Chinese room thought experiment.

[GPT (text-davinci-003)]

In his responses to John Searle's Chinese Room thought experiment, philosopher Daniel Dennett offers a defense of strong artificial intelligence (AI). In his argument, Dennett maintains that Searle's experiment does not adequately account for how human beings use language, and that strong AI can potentially be achieved if the right conditions are met.

Dennett's first response to the Chinese Room argument is that language use is not merely an informational process, as Searle suggests. Rather, Dennett argues that language use involves a complex system of meaning and interpretation, and that meaningful language use cannot be reduced to a strictly algorithmic process. For example, Dennett points out that two people may use the same words but interpret them differently, depending on various contextual factors. Thus, the Chinese Room thought experiment fails to account for the complexity of language and its use.

Dennett also challenges the assumption that a computer can never understand language as well as a human being. To this end, he argues that if a computer is given enough data, it can learn to process language in a similar way to humans. This could involve the computer being programmed with a large corpus of data, such as literature, so that it can learn to understand language in context.

Dennett's third point is that the Chinese Room argument neglects the fact that computers can be programmed to interact with their environment. In other words, a computer can be programmed to respond to various external stimuli, just as a human being would. This means that a computer could potentially interact with its environment in a meaningful way, just as a human being can.

Finally, Dennett argues that the Chinese Room argument assumes that a computer must understand language in the same way as a human being, which is an oversimplification of how language works. Rather, he suggests that a computer could potentially understand language in its own way, and be able to process information in a meaningful way.

In sum, Daniel Dennett's responses to Searle's Chinese Room thought experiment offer a defense of strong AI, arguing that computers can potentially be programmed to understand language in a meaningful way. Dennett's argument draws upon various theoretical and practical considerations, and provides an interesting counterpoint to the conventional view that computers can never truly understand language in the same way as a human being.

33.

mannykannot about 2 months ago | on “Language is not essential for the cognitive processes that underlie thought”

The brain is faster than the Chinese room, but other than that, yes, that's the so-called systems reply; Searle's response to it (have the person in the room memorize the instruction book) is beside the point, as you can teach people to perform all sorts of algorithms without them needing to understand the result.

As many people have pointed out, Searle's argument begs the question by tacitly assuming that if anything about the room understands Chinese, it can only be the person within it.

34.

shagie over 1 year ago | on “Richard Stallman's thoughts on ChatGPT, AI and their impact on humanity”

The argument of the Chinese room is the strong claim.

From https://en.wikipedia.org/wiki/Chinese_room#Complete_argument the conclusion of the complete argument is:

> (C1) Programs are neither constitutive of nor sufficient for minds.

> This should follow without controversy from the first three: Programs don't have semantics. Programs have only syntax, and syntax is insufficient for semantics. Every mind has semantics. Therefore no programs are minds.

---

I personally don't agree with it and believe that there is a flaw in:

> (A2) "Minds have mental contents (semantics)."

> Unlike the symbols used by a program, our thoughts have meaning: they represent things and we know what it is they represent.

While a person may know what they are thinking, examining the mind from the outside it isn't possible to know what the mind is thinking. I would contend that from the outside of a mind looking at the firings of neurons in a brain it is equally indecipherable to the connections of a neural net.

The only claim that "we know what it is they represent" is done from the privileged position of inside the mind.

I would argue that intelligence is more related to the Kolmogorov complexity exhibited by something.

( David Dowe: Minimum Message Length, Solomonoff-Kolmogorov complexity, intelligence, deep learning... https://youtu.be/jY_FuQbEtVM?t=886 )

That the model of GPT is much smaller than its input.

The Chinese room lookup table is enormously large.

If we attempt to relegate GPT as no better than a Chinese room, we can show that the Chinese room look up table is impossible with the amount of data that GPT has access to as part of its model.

If we say that its not a lookup table but instead an enormously complex interplay of inputs and variables, then the distinction between the room that GPT exists in and our own mind breaks down trying to distinguish which is which.

If we want to switch to consciousness, then possibly the argument can progress from there because GPT doesn't have any state once it is run (ChatGPT maintains state by feeding its output back into itself and then summarizing it when it runs out of space). However, in doing this we've separated consciousness and intelligence which means that the Chinese room shouldn't be applied as an intelligence test but rather a consciousness test.

Are GPT 3 and 4 conscious? I'll certainly agree that's a "no". Will some future GPT be conscious and if so, how do we test for it? For that matter, how do we test for consciousness for another entity that we're conversing with (and its not just Homer with a drinking bird tapping 'suggested replies' in Teams ( https://support.microsoft.com/en-gb/office/use-suggested-rep... ))?

35.

dTal almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

It's sleight of hand because the sentience of the human in the system is irrelevant. The human is following a trivial set of rules, and you could just as easily digitize the books and replace the human with a microcontroller. Voila, now you have a Chinese-speaking computer program and we're back to where we started. "The books" don't feel anything, true - but neither do the atoms in your brain feel anything either. By asserting that the human in the room and the human who wrote the books are the only "relevant entities" - that consciousness can only emerge from collections of atoms in the shape of a human brain, and not from books of symbols - you are begging the question.

The Chinese room is in a class of flawed intuition pump I call "argument from implausible substrate", the structure of which is essentially tautological - posit a functioning brain running "on top" of something implausible, note how implausible it is, draw conclusion of your choice[0]. A room with a human and a bunch of books that can pass a Turing test is a very implausible construction - in reality you would need millions of books, thousands of miles of scratch paper to track the enormous quantity of state (a detail curiously elided in most descriptions), and lifetimes of tedious book-keeping. The purpose of the human in the room is simply to distract from the fabulous amounts of information processing that must occur to realize this feat.

Here's a thought experiment - preserve the Chinese Room setup in every detail, except the books are an atomic scan of a real Chinese-speaker's entire head - plus one small physics textbook. The human simply updates the position, spin, momentum, charge etc of every fundamental particle - sorry, paper representation of every fundamental particle - and feeds the vibrations of a particular set of particles into an audio transducer. Now the room not only speaks Chinese, but also complains that it can't see or feel anything and wants to know where its family is. Implausible? Sure. So is the original setup, so never mind that. Are the thoughts and feelings of the beleaguered paper pusher at all relevant here?

[0] Another example of this class is the "China brain", where everyone in China passes messages to each other and consciousness emerges from that. What is it with China anyway?

36.

jll29 over 1 year ago | on “Artificial Consciousness Remains Impossible (Part 2)”

Searle's Chinese Room thought experiment is about mastering the Chinese language.

There are two issues I have with it as it was originally presented:

1. Mastering a language is not the same as having consciousness.

2. Who "knows Chinese" in the Chinese Room thought experiment? I would say neither Searle nor Searle in/as part of the Chinese Room "speak" Chinese. But it is

fair to say that the book that the fictional Searle follows can be

seen as a model of the Chinese language; or at least the combination of

the book and Searle as its "processor" collectively are an implemented

operational model of the Chinese language. And a model of Chinese is

NOT the same as being skilled at conversing in Chinese (executing the

model in a particular way). Other posters here have drawn analogies from

music evoking certain subjective emotions, and again a semantic network

that has concept nodes labelled with the names of these emotions is not

the same as experiencing these emotions, although the semantic network

can be said to constitute a model of sorts of the music's effects. But

again, model(x) != qualia(x).

37.

int_19h 7 months ago | on “Is artificial consciousness achievable? Lessons from the human brain”

That is still nonsense. If you take the whole room as a system and integrate it into your brain as a subsystem, then yes, you will understand Chinese.

The reason why we have to speak of the room and the person inside as a system is because the real magic is in the instructions. The fact that they are performed by a self-aware human is completely irrelevant to the setup and is only there to confuse the matter.

In your other example with two people, viewing them as a system doesn't make much sense because one of those people is redundant - you can leave just the person who speaks Mandarin, and that is sufficient for the whole to function. So they alone are "the system". And it also operates based on instructions, except that those instructions are stored in the person's head and executed by low-level processes in the brain.

Searle believes that consciousness cannot be simulated as a digital computation, period. Given that any other physical process can be, this requires a belief that consciousness is somehow magically different from any other physical process in some unspecified way (that appears to be conjured out of thin air solely to make this one argument, at that). That is not materialism.

38.

jfengel over 1 year ago | on “Searle's Chinese Room Argument”

He proves that you can't necessarily derive semantics from syntax alone.

You don't need a whole room to do that, though. You already know that a sentence can be semantically ambiguous, with multiple distinct readings that can be resolved with context.

A Turing machine is distinct from a lookup table. It has a memory. Of course a lookup table doesn't "speak Chinese", but that's not a question anybody was asking.

It feels like Searle is trying to confuse your intuition: if you can look inside and see the mechanics then there can't be any "qualia" there. But the mechanics he's proposing simply do not work, so the question is moot.

I think the grandparent post is on the right track: the question Searle should have been asking is closer to one about P-zombies. The question he's actually asking isn't worth discussing, because the intuition he's pumping isn't about computationalism at all.

39.

krick about 2 years ago | on “This Week's Hype”

Searle is proposing what he supposes to be a proof of a computer being unable to have mind in the same sense humans have mind, and thus human intelligence being something more than a "mere computation". While in fact his argument proves absolutely nothing, and I'm honestly surprised that anyone buys into this bullshit. Actually, I think that the question itself is meaningless, but it's besides the point what I think, I'll just try to explain where's the fallacy. And since his argument is "reductio ad absurdum" it's easiest to counter by assuming the opposite, and showing that there's nothing absurd about it, i.e. there's no self-contradiction here. Just keep in mind that I'm not trying to prove the opposite, I'm only assuming it for the sake of argument.

So, the opposite assumption is that there's nothing more to intelligence than a "mere computation", and all that stuff like inner dialog and "a sense of self" are just side-effects of it. That way, there's no difference between "strong AI" and "weak AI" in his terminology, no difference between "understanding" and "simulation of understanding". Then, of course, a digital computer has a mind and understands Chinese, and also the Chinese room has a mind and understands Chinese. But Searle doesn't understand Chinese and this is supposed to be a contradiction.

But there's no contradiction here, because Searle is not the Chinese room. He isn't the mind, in this model he is to Chinese room what electricity is to a digital computer, or what neurotransmitters are to human brain. So, yeah, electricity doesn't understand Chinese, serotonin doesn't understand Chinese, single human neuron doesn't understand Chinese, but Turing-test-passing computer, Chinese brain and Chinese room all do. Searle is just a detail in the Chinese room — tiny and insignificant compared to a book, containing a complete description of a program that speaks Chinese and is able to form coherent (and persistent!) opinions on every possible topic in the world.

So, the fallacy is equating Searle's role in the hypothetical Chinese room to the whole digital computer that speaks Chinese and passes Turing test with breeze.

40.

boole1854 about 2 years ago | on “An End to Pornography, Sophistry, and Panty Raids: on Epstein's Edge.org (2...”

> The Chinese room argument has no merit whatsoever, and should be forgotten.

I have a different take on the argument. I believe it was widely misunderstood to be an argument against the possibility of artificial intelligence, when it was actually merely an argument that there is a logical difference between symbol manipulation and sentient understanding. Searle contributed to the misunderstanding by terming the position he was arguing against "Strong AI". He then argued that "Strong AI" was not possible on the basis of his argument. However, "Strong AI" has a different meaning to many people so that the "Strong AI" which Searle was trying to refute was not the same as the "Strong AI" that most people have in mind when they hear the term.

For example, most people would probably say that a computer which could converse, write poems, compose novels, author novel scientific papers, etc. in perfect Mandarin, indistinguishable from a highly educated, human, fluent speaker of Mandarin, would be by definition a Strong AI. But Searle never refuted, or tried to refute, the idea that a computer could accomplish all of that, so he was not arguing against the position that most people would take to be "Strong AI". His point was to argue that having the qualia of conscious understanding is logically different than being able to accomplish the symbol manipulation required to achieve the tasks above.

41.

dvt about 2 years ago | on “Show HN: Whole Git repo was made with ChatGPT”

You're putting the cart before the horse.

Searle's Chinese Room thought experiment argues with great precision why (solely) symbol-manipulating systems don't (and will never) think. The process of cognition is deeply internal and most likely involves self-reflection, internal states, and a sense of "I-ness" which seems to be unique to mental things (unlike Searle, I'm a substance dualist so I'm admittedly a bit biased). Ergo, I'd say, at the very least, an intelligent & thinking process would need to (also) operate on non-symbolic things (which rules out computational systems), maybe on qualia, but there's probably other primitives as well.

42.

hackinthebochs almost 2 years ago | on “Kissinger: “ChatGPT Heralds an Intellectual Revolution””

>which definitely doesn't think

Why do you think it "definitely" doesn't think? If this is about lacking consciousness, then you should make that explicit. But then the question just gets pushed back further, why are you sure it isn't conscious?

The problem with the Chinese room at its core is the problem of attribution. We want to attribute properties like consciousness or understanding to "things" we are familiar with, and the only sufficient thing in the room is the man. But this intuition misdirects us to bad conclusions. The question to ask is what is responding when questions are being asked of the room. The responses are being generated by the algorithm reified into a causally efficacious process. Essentially, the reified algorithm is a set of properties without objecthood. It's so hard for people to see this as a live option because our cognitive makeup is such that we reason based on discrete entities. Considering extant properties without entities to carry them is just an alien notion.

We can go further and conceptualize a virtual objecthood for the process to help us reason about the entity we interface with in the Chinese room. We can then ask what properties does it have, how much does it understand, and so on. But we can't simply say it doesn't have any of these properties because it's merely a mechanical/computational process.

43.

mannykannot about 2 years ago | on “This Week's Hype”

I have read through the paper two or three times, as well as several other things Searle and others have written about it, and I have it open in front of me now.

In it, Searle argues that no computer program will be able to satisfy the goal he calls 'strong AI', which he defines thus: "according to strong AI, the computer is not merely a tool in the study of the mind; rather, the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states."

As far as I recall (and I am not going to read the whole thing again just to check) he does not precisely define 'computer' or 'computer program', but it is clear that his meaning subsumes ordinary usage: a Turing-equivalent digital device, together with any program that can run on it. It is also clear that, given his underlying position being that syntactical manipulation cannot give rise to semantics, he would include, in his definition, actual universal Turing machines, and any other device, digital or not, that, in his view, can only perform syntactical operations.

Therefore, Searle is saying that computers will never think, except in a most pedantic reading of 'computers'. He is, indeed, a materialist, and regards the brain to be a mind-instantiating machine of sorts, but, like Penrose, he thinks its consciousness must depend on something beyond what we have discovered so far.

Personally, I think the argument begs the question in this sense: it is predicated on the assumption that the only thing in the room that could understand anything would be the human operator, but that, in turn, is predicated on the assumption that nothing else about the room could do so. That he is doing this is made very clear in his attempts to rebuff the 'systems reply' and the 'simulator reply'.

44.

antognini almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

You raise good questions, but I would really suggest listening through the lectures because Searle addresses all your points in a lot of detail. (It's a lot of material, but it's well worth it and he's an entertaining lecturer.) It's actually a bit funny that you bring up Cartesian dualism because Searle is pretty clear from the outset that he's of the opinion that Descartes's dualism was the worst thing ever to happen to philosophy. He certainly has no truck with it.

The question Searle is raising with the thought experiment is where the understanding of Chinese is happening. It's clear that the understanding is not happening with the homunculus. But if not there, then where is it happening? Some functionalists responded with the "systems response" which is that the system of the room plus the homunculus is what has the understanding even though no element of the system taken individually has understanding. He goes through a pretty detailed reply to the systems response, but the basic argument against it is that it's incoherent. There's no well defined thing that is the system. You can remove the man from the room and put him under a tent in a field. Where does the system end and the rest of the world begin?

You are of course correct that the experiment can be reformulated without the human, but that's exactly the point. A computer is mechanistically doing exactly the same thing as the man in the room. But if the man has no understanding and the room has no understanding and the system has no understanding, removing the human from the equation changes nothing. The reason Searle included the human is because saying "computer" invokes a lot of mystery to a lot of people as to what's going on. But a computer is doing nothing different than the man in the room.

45.

kthejoker2 over 2 years ago | on “Belief in AI sentience is becoming a problem”

The Chinese Room argument was literally an argument against mechanism / computationalism (ie that your mind is just a really complex neural network.)

So no, "by the same reasoning" humans cannot be considered non sentient.

In fact the original paper is not talking about sentience at all, but intelligence (these are different things.)

And the argument it isn't about declaring something sentient or non-sentient, it simply states that responding with expected outputs to a given input is not proof of sentience or intelligence.

It basically says you have to use some other method to determine if something is intelligent and has "understanding."

You should read up on this a lot more, it's fascinating, and you've got a very incorrect understanding of it.

https://en.wikipedia.org/wiki/Chinese_room

46.

zwaps over 2 years ago | on “Consciousness is not computation”

The point of the experiment is to think about the individual in the room. You can not say it's irrelevant, because it's the entire point.

The system's response is trivial: Sure, if the room+person combination leads always to a coherent response in Chinese, then the entire system understands Chinese.

I'd go even further: If the person in the room does not understand Chinese, but the system does, then there is some entity that understands Chinese - either a person or an advanced AI, feeding the inputs. Then, from the systems perspective, the person in the room is largely irrelevant.

But this is not the argument: Despite no discernible difference from the outside, the person in the room may either understand Chinese, or they may not.

And so there is a distinction - from the perspective of the individual in the room, that does not depend on the outside observation.

That's all there is to it. It shows that meaning and understanding are not the same as syntactic computation (an important point, to be sure), but it does not show that one can exist with or without the other. By extension, it does not otherwise disprove consciousness as being this or that.

47.

jamesrom over 2 years ago | on “Consciousness is not computation”

The "Chinese Room" argument isn't an argument against consciousness being computation.

In the same way the man in the room doesn't understand Chinese to be able to produce Chinese, the mind in your brain does not need to understand consciousness to be conscious.

The man in the room can be said to pass the Turing test, and so can you.

48.

Jipazgqmnm almost 2 years ago | on “Theory of Mind May Have Spontaneously Emerged in Large Language Models”

It's called Chinese Room: https://en.wikipedia.org/wiki/Chinese_room

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese?

To me: If you can't tell, it effectively doesn't matter.

49.

jltsiren over 2 years ago | on “The singularity is close?”

Most of the confusion around the Chinese Room arises when people don't realize that "computer" and "computation" are not synonyms.

Turing defined computation as I/O behavior. The Church–Turing Thesis states that as far as their I/O behavior is concerned, all sufficiently general computational mechanisms (computers) are equivalent. The Chinese Room Argument presents a stupid computational mechanism with I/O behavior equivalent to a human communicating in Chinese. Some people then conclude that because the Chinese Room is clearly not intelligent and "because all computers are equivalent", computers cannot be intelligent. But that requires leaving out the qualifications from the Church–Turing Thesis.

But maybe there is more to computers than their external I/O behavior. Maybe intelligence is a property of the internal behavior of a system rather than its external behavior. Then there could be some computers that are intelligent and others that are not, even if they have the same external behavior.

50.

IshKebab over 1 year ago | on “The new Bing runs on OpenAI’s GPT-4”

> To me, Searle's point is ultimately that we don't know enough about the human mind to be able to judge whether it can be replicated artificially.

There's string circumstantial evidence that we do. And really "computers can simulate the physics in the brain" is the null hypothesis.

In any case why is the Chinese Room always stated as if it has a clear conclusion rather than "this doesn't really prove anything" if we don't know enough to say either way?

> And now that we have almost literally developed a Chinese Room

I don't think so. The GPTs are currently still very far from the complexity of the human brain, and they are missing many features that may make a big difference to consciousness - for instance the ability to learn while running.

So while it may be fairly easy to say ChatGPT isn't conscious/sentient, that isn't the question. It's whether computers theoretically can't be conscious because consciousness comes from some physical property that they can't reproduce (like quantum microtubule crankery).

51.

ryandv 6 months ago | on “Here comes the Muybridge camera moment but for text”

To this list I would absolutely add Julian Jaynes' "The Origin of Consciousness in the Breakdown of the Bicameral Mind."

> simulations of conscious thought entirely via prediction language tokens

Jaynes goes so far as to assert that language generates consciousness, which is characterized by (amongst other features) its narrative structure, as well as its production of a metaphor of our selves that can inhabit a spatiotemporal mental space that serves as an analog for the physical world; the mental space where we imagine potential actions, play with ideas, predict future outcomes, and analyze concepts prior to taking action in the "real, actual" world.

The generation of metaphors is inextricably linked to the psychotechnology (to pull a word from vocabulary discussed by John Vervaeke in his "Awakening from the Meaning Crisis" series) of language, which is the means by which one object can be described and elaborated by its similarity to another. As an etymological example: the Sanskrit word "bhu" which means "to grow" forms the basis of the modern English verb "to be," but predates lofty abstract notions such as that of "being," "ontology," or "existence." It's from the known and the familiar (plant or animal growth) that we can reach out into the unknown and the unfamiliar (the concept of being), using (psycho-)technologies such as language to extend our cognition in the same way a hammer or a bicycle extends our body.

There is something here about language being the substrate of thought, and perhaps even consciousness in general as Jaynes would seem to assert in Book I of his 1976 work, where he spends a considerable amount of time discussing metaphor and language in connection to his definition of "consciousness."

There are also questions of "intentionality" and whether or not computers and their internal representations can actually be "about" something in the way that our language and our ideas can be "about" something in the physical (or even ideal) world that we want to discuss. Searle and the "Chinese room" argument come to mind.

Turing famously dodged this question in his paper "Computing Machinery and Intelligence" by substituting what is now called the "Turing test" in lieu of answering the question of whether or not "machines" can "think" (whatever those two words actually mean).

52.

wruza over 1 year ago | on “Ask HN: AI read books, Human also. What's the difference?”

The standard argument against the chinese room is that if the 'symbols' are at lower level, audio, video stimuli, and you have enough scale, then you can imagine intelligence, emerging somehow.

Qualian chauvinism. The ‘observer’ field doesn’t have to be locally or real-ly connected. E.g. a current is real, but it doesn’t flow through a real-connected surface.

Chinese room is conscious and intelligent by definition if the above is true, just not human-like conscious and intelligent.

The juice of this question is that we may want to determine it in a practical binary sense, when there may be a spectrum or an entire space of it.

Sadly it can all boil down to a boring “shut up and interact” principle again.

53.

mannykannot over 2 years ago | on “Is DALL-E 2 ‘gluing things together’ without understanding their relationsh...”

The argument you make here is a reasonable one (IMHO) for the plausibility in principle of what Searle calls “strong AI”, but he claims that his “Chinese Room” argument proves that it must be mistaken. One can simply ignore him, but to refute him takes a little more effort.

It turns out that when one looks at the argument in detail, and in particular at Searle’s responses to various objections (such as the Systems and Virtual Mind replies), it is clear that he is essentially begging the question, and his ultimate argument, “a model is not the thing modeled”, is a non-sequitur.

54.

epups over 1 year ago | on “The new Bing runs on OpenAI’s GPT-4”

> In any case why is the Chinese Room always stated as if it has a clear conclusion rather than "this doesn't really prove anything" if we don't know enough to say either way?

To me, it does have a clear and definitive conclusion, which is that mere intelligent output does not mean you are replicating human intelligence, or any higher order mechanism such as consciousness. We don't know enough to tell that it doesn't have any consciousness, but that's beside the point.

You mentioned that a computer that could simulate every molecule of a human brain would also likely replicate sentience. Of course the tricky part is how do you prove that assertion, if all you have is output? If I transfer your brain to an advanced computer as you describe, can I conclude that you're conscious based on what you tell me? I don't think so, because with present technology I could likely make a passable version of your writing output with a LLM. To me that's the real value of the Chinese Room, which is to expose precisely this dillemma. People wrote all sorts of replies to it in order to tackle that - you may be interested in reading about Dennett's p-zombies if you haven't already.

55.

mort96 almost 2 years ago | on “The AI Mirror Test, which smart people keep failing”

My thoughts on the Chinese room thought experiment is: the person in the room does not know Chinese, but the person+room system knows Chinese. I believe the correct analogy is to compare the AI system to the person+room system, not to just the person.

How do you back up the statement that "for us to consider a machine sentient, it needs to function in a similar way to us"? On what basis do you categorically deny the validity of a sentient being which works differently than a human?

56.

danbmil99 over 1 year ago | on “AI vs. AGI vs. Consciousness vs. Super-Intelligence vs. Agency”

> John Searle, broadly, framed the issue correctly in the 80s and the standard critiques are wrong.

The standard critiques are not wrong, IMNSHO. Searle's Chinese Room is facile mind-poison. It is an unfalsifiable hypothesis.

What if I could simulate physics down to the molecular level, including simulating a human brain? Would that be conscious? If not why not?

And if I ran that simulation (a bit slowly, granted) by having that guy from the Chinese Room manually run the simulation, painstakingly following the instruction code of that simulation, would the fact that the simulation is being implemented by someone who unrelatedly is conscious himself, have any bearing on the scenario?

Searle's argument here is "Not Even Wrong".

57.

celeritascelery over 2 years ago | on “Consciousness is not computation”

We are actually in more agreement then you realize. But I think you misunderstand the Chinese room argument. It does not say anything about if computers are or could be conscious. All it says is that you cannot determine that from a “black box” view. We can’t determine if the computer is really conscious, or just really good at mimicking the consciousness of someone else (ie the people providing the training data). Just like you can’t determine if the man in the room really knows Chinese or if he is just good at mimicking a machine that does. The man really could know Chinese, and a computer really could be conscious, but you can’t determine this as an outside observer.

58.

93po almost 2 years ago | on “Facebook LLAMA is being openly distributed via torrents”

I don't really understand the brain or AI enough to meaningfully discuss this, but I would wonder if there's some aspect of "intentionality" in the context of the Chinese Room where semantic search with embeddings still "doesn't count".

I struggle with the Chinese Room argument in general because he's effectively comparing a person in a room following instructions (not the room as a whole or the instructions filed in the room, but the person executing the instructions) to the human brain. But this seems like a crappy analogy because the better comparison would be that the person in the room is the electricity that connects neurons (instructions filed in cabinets). Clearly electricity also has no understanding of the things it facilitates. The processor AI runs on also has no understanding of its calculations. The intelligence is the structure by which these calculations are made, which could theoretically could be modeled on paper across trillions of file cabinets.

As a fun paper napkin exercise, if it took a human 1 second to execute the instructions of the equivalent of a neuron firing, a 5 second process of hearing, processing, and responding to a short sentence would take 135,000 years.

59.

PaulDavisThe1st over 2 years ago | on “We don't know what makes things sentient–so let's stop acting like we do”

Hofstadter and Dennett addressed the Chinese Room argument in a way that I find completely compelling. Either you are not familiar with it, or do not find it compelling. They call it the "system" position IIRC, and essentially, the entire room ("system") is conscious, even though none of the pieces are.

60.

h0l0cube 7 months ago | on “Scientists Find an 'Alphabet' in Whale Songs”

> Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that they are talking to another Chinese-speaking human being.

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI".

(Emphasis added)

If we were to make an analogy to contemporary machine learning, we're talking about the difference between an LLM (with context) and a Markov Chain. 'Understanding' requires novel reuse of recollections. Recollections require memory (i.e. context), and the novel reuse of those recollections require a world model with which to inference.

61.

notahacker over 2 years ago | on “We don't know what makes things sentient–so let's stop acting like we do”

> Unfortunately, that argument applies to you, yourself. I mean, presumably you know that you yourself are intelligent, but you must take it on faith that everyone else is. We all could just be a kind of Chinese Room, as far as you know. Communicating with us is not a sure way to know whether we are "really" sentient because we could just be automatons, insensate but sophisticated processes, claiming falsely to be just like you.

I'm not sure the conclusion that Chinese people might not understand Chinese either is the best counterargument to Searle's thought experiment or its conclusion effective use of words alone doesn't constitute sentience. At no point does the difficulty in establishing what Chinese people do and don't understand rescue the possibility the non-Chinese speaker knows what's going on outside his room, and most of the arguments to the effect that Chinese people understand Chinese (they map real world concepts to words rather than words to probabilities, they invented Chinese, they're physiologically quite similar to sentient me, they appear to act with purpose independently from communication) are also arguments to the effect that text-based neural networks probably don't.

In a trivial sense, it's true I can't inspect others' minds, and despite what everyone says I could be the only thinking human being in existence. But I have a lot of reason to suspect that physiologically similar beings (genetically almost identical in some cases) who describe sensations in language they collectively invented long before I existed which very strongly matches my own experiences are somewhat similar to me, and that an algorithm running on comparatively simple silicon hardware which performs statistical transformations on existing descriptions of these sensations written by humans is simply creating the illusion of similarity. Heading in the other direction, humans can also be satisfied by the output of "article spinners" used by spammers to combine original texts and substitute enough synonyms to defeat dupe detectors, but I'm pretty sure the quality of their writing output shouldn't be given precedence over our knowledge of the actual process behind their article generation when deciding if they're sentient or not...

62.

msla over 1 year ago | on “Yann LeCun and Andrew Ng: Why the 6-Month AI Pause Is a Bad Idea [video]”

These thought experiments rest on the assumption that human brains are magically complex such that being "just" anything we can understand is inherently lesser. Searle's Chinese Room experiment falls for the same fallacy, more-or-less, plus the fallacy that "human plus book" isn't a single system but "huge number of individual neurons" somehow is.

63.

gwd over 1 year ago | on “A GPT-4 capability forecasting challenge”

> Searle's main point is that if I have a book that tells me how to respond and I never learn Chinese, then I do not understand Chinese. If you see a flaw in this reasoning, I am very interested.

To a degree, I feel like the Chinese Room argument is begging the question. When I imagine Searle sitting in a room, with a book of instructions and paper and everything he needs to execute GPT-4's equivalent, I basically see an actual computer. That is literally what he is; there is no difference. So then to ask, "Does this system understand Chinese?" is literally exactly the same question as "Does GPT-4 understand Chinese?" You haven't actually illuminated the question in any meaningful way, except to give people not familiar with how microprocessors work a better intuitive understanding. (Which, upon reflection, probably is a fairly useful thing to do.)

I looked a bit at the "1990's version" of his argument on the Wikipedia page you quoted. Going back to my earlier example, this is sort of what his argument sounds like to me:

A1) Electronic gates just on and off switches.

A2) Numbers and addition are semantic.

A3) On and off switches are neither constitutive of, nor sufficient for, semantics.

Therefore, computers cannot add; they only simulate the ability to add.

Now I'm not up on the fine details of what "syntactic vs semantic" means in philosophy, so maybe #2 is't accurate. But in a sense it doesn't matter, because that communicates how I feel about Searle's argument: "I've made some distinction between two classes of things that you don't understand; I've defined one to be on one side, and the other to be on the other side; and therefore computers can't understand."

My best guess as to the "syntactic / semantic" thing is this: In some sense, even his premise, that "Progams are syntactic", isn't actually accurate: Computers operate on bits which are operated on by gates: gates and bits themselves don't inherently have symbols; the symbols are an abstraction on the bits. Even bits are abstractions on continuous voltages; and voltages are ultimately abstractions on quantum probabilities.

What a given set of voltages "means" -- whether they're numbers to be added, or words to be word-checked, or instructions to be executed, or a JPEG to be decompressed, depends entirely on how they're used. If you jump into the middle of a JPEG, your computer will happily try to execute it, and if you dump the program into your video buffer, you'll get a bunch of strange dots on your screen.

Furthermore, when you build an "adder" out of logic gates, you can build the gates such that they correspond to our intuitive idea of binary addition, with individual carries for each bit and so on. But this is inefficient, because then you have to wait for the carries to cascade all the way through the whole thing you're trying to add. Instead, you can brute-force a set of logic gates such that given these 16 bits in, and these 9 bits out (8 plus overflow), you just get the right answer; this will be a lot faster (in the sense that the signals have to go through fewer gates before stabilizing on the final answer), but the gates inside then don't make any sense -- they're almost a "compression" of the longer, carry-based method.

Does that mean that an adder made this way isn't "actually" adding? In the end it doesn't really matter: 16 bits come in, and 9 bits come out the way you want them to. It doesn't really matter that much what happened in the middle.

Putting all that together: It seems to me the "semantics" of a set of bits is based on how they end up interacting with the real world. If I can ask GPT-4 what's missing in my pancake recipe, and it can tell me "you're missing a leavening agent like baking powder", then it seems to me there must be semantic content in there somewhere, and all the arguments about syntax not being sufficient for semantic turn out to have been proven wrong by experiment.

64.

kbelder almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”

Maybe there's a small human inside the human's brain, not understanding the rules the human is following, and it's only the innermost human that is really conscious?

I think Searle's Chinese room argument is absolutely nonsensical, a bit like arguing about philosophical zombies. It's an argument that only makes any sense if you already are committed to mysticism or dualism.

65.

dragonwriter almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

The Chinese Room is a argument for solipsism disguised as a criticism of AGI.

It applies with equal force to apparent natural intelligences outside of the direct perceiver, and amounts to “consciousness is an internal subjective state, so we thus cannot conclude it exists based on externally-observed objective behavior”.

66.

MeImCounting 12 months ago | on “Artificial intelligence systems found to excel at imitation, but not innova...”

As I see it the AI schism is more about the debate between functionalism/computationalism and the idea that the chinese room thought experiment was an argument for, "biological naturalism". There is a lot of effort dedicated to showing that AIs dont have some innate quality called "consciousnes" or "sentience" or what have you. There is not just a lot of effort to show that, but also to show that that is somehow a limitation to the capabilities of an "AI".

Personally I think "consciousness" or "sentience" or whatever you want to call it, is really not that useful in making logical decisions or solving engineering problems. It is certainly a useful trait to humans but if nobody can tell that the person/program in the chinese room doesnt actually "understand" chinese then why does it matter? If it walks and talks like a duck you can probably use it for whatever ducks are useful for.

67.

gwd over 2 years ago | on “The singularity is close?”

> Searle's answer is obviously not, from which he deduces that the computer program doesn't understand Chinese either.

Would Searle feel differently if he'd taken a computer architecture course?

There was something strangely magic about making an adder:

1. Learn about boolean logic, Karnaugh maps, etc

2. Define the inputs and outputs of a 4-bit adder, deduce some set of gates which would correspond to those outputs

3. Put those into a circuit similator and put a box around it

4. Suddenly the random AND and OR gates, which look arbitrary and none of which individually know how to do addition, collectively know how to addition.

Then you keep scaling that up: Put a bunch of gates together, none of which have memory, and suddenly you have flip-flops and registers. Add an instruction decoder and suddenly it starts zooming along doing things, executing simple programs that you feed it from simulated "memory". But you know that inside is just a spider's nest of logic gates.

I myself am religious and believe in a spiritual world distinct from the material world; but the Chinese Room thought experiment was never that compelling to me: I've seen spider webs of logic gates come alive as processors when assembled properly; I don't see an inherent reason why an algorithm of paper plus a very patient human couldn't come together to create something which "understood".

68.

hnfong almost 2 years ago | on “'I Worked on Google's AI. My Fears Are Coming True'”

The Chinese Room thought experiment is kind of a different mistake. In the Chinese room argument, Searle attempted to imagine an AI system that was "obviously not sentient" because it was made up of hard rules and facts (presumably legible to us humans), and it was argued that it should be obvious that the system wasn't sentient.

For the ChatGPT case it's not clear the Chinese room argument even applies. First, it's not even a Chinese room, it's trained by a neural network, an a neural network is inspired by the structure of our brains. There's no "set of instructions in English" for anyone to follow, but it's a rather overwhelming set of neurons somehow producing the results that it was trained on, and as an outsider we have no idea what's going on inside individual neurons -- just like our brains.

I'm seeing a lot of people claim that "we know how ChatGPT is created, and so it's not sentient", maybe this looks like the Chinese room fallacy (I'm calling it as such), but it's like claiming we know the laws of physics (Schrödinger's equation etc.) and then claiming we know how the brain works because it's all quantum physics. The fact is we know how to build GPT but we don't actually know how it works.

Besides what happens to humanity when we learn more and more about how our own brains work? I'm pretty comfortable with the idea that we're not sentient (yay), but I'm pretty sure those same people would then come up with other nonsensical arguments to prove that they are different from the robots we created.

69.

tigen over 2 years ago | on “The singularity is close?”

> The people who read the thought experiment this way don't understand it. In the thought experiment neither the computer program (no matter how powerful the computer is) nor the man and the room understand Chinese.

Who says the computer doesn't understand Chinese, though? In this case "the computer" is the system. The program, being the code, isn't a process. The man is acting as a mechanical piece of hardware. But the computer running the program is an active process with state.

I heard a lecture by Searle on this. He mentioned the idea that "the room understands Chinese" and simply dismissed it as absurd without considering it. This is exactly the problem with his argument. Well, that and the casual conflation of the man with the computer as if the CPU hardware is the system.

70.

notahacker almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

> It applies with equal force to apparent natural intelligences

In practice the force isn't equal though. It implies that there may be insufficient evidence to rule out the possibility that my family and the people that originally generated the lexicon on consciousness which I apply to my internal subjective state are all P-zombies, but I don't see anything in it which implied I should conclude these organisms with biochemical processes very similar to mine are equally unlikely to possess internal state similar to mine as a program running on silicon based hardware with a flair for the subset of human behaviour captured by ASCII continuations, and Searle certainly didn't. Beyond arguing that ability to accurately manipulate symbols according to a ruleset was orthogonal to cognisance of what they represented, he argued for human consciousness as an artefact of biochemical properties brains have in common and silicon based machines capable of symbol manipulation lack

In a Turing-style Test conducted in Chinese, I would certainly not be able to convince any Chinese speakers that I was a sentient being, whereas ChatGPT might well succeed. If they got to interact with me and the hardware ChatGPT outside the medium of remote ASCII I'm sure they would reverse their verdict on me and probably ChatGPT too. I would argue that - contra Turing - the latter conclusion wasn't less justified than the former, and was more likely correct, and I'm pretty sure Searle would agree.

71.

notahacker almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

> It's sleight of hand because the sentience of the human in the system is irrelevant. The human is following a trivial set of rules, and you could just as easily digitize the books and replace the human with a microcontroller. Voila, now you have a Chinese-speaking computer program and we're back to where we started.

Substituting the microcontroller back is... literally the point of the thought experiment. If it's logically possible for an entity which we all agree can think to perform flawless pattern matching in Chinese without understanding Chinese, why should we suppose that flawless pattern matching in Chinese is particularly strong evidence of thought on the part of a microcontroller that probably can't?

Discussions about the plausibility of building the actual model are largely irrelevant too, especially in a class of thought experiments which has people on the other side insisting hypotheticals like "imagine if someone built a silicon chip which perfectly simulates and updates the state of every relevant molecule in someone's brain..." as evidence in favour of their belief that consciousness is a soul-like abstraction that can be losslessly translated to x86 hardware. The difficulty of devising a means of adequate state tracking is a theoretical argument against computers ever achieving full mastery of Chinese as well as against rooms, and the number of books irrelevant. (If we reduce the conversational scope to a manageable size the paper-pusher and the books still aren't conveying actual thoughts, and the Chinese observer still believes he's having a conversation with a Chinese-speaker)

As for your alternative example, assuming for the sake of argument that the head scan is a functioning sentient brain (though I think Searle would disagree) the beleaguered paper pusher still gives the impression of perfect understanding of Chinese without being able to speak a word of it, so he's still a P-zombie. If we replace that with a living Stephen Hawking whose microphone is rigged to silently dictate answers via my email address when I press a switch, I would still know nothing about physics and it still wouldn't make sense to try to rescue my ignorance of advanced physics by referring to Hawking and I as being a union with collective understanding. Same goes for the union of understanding of me, a Xerox machine and a printed copy of A Brief History of Time.

72.

mhink over 1 year ago | on “Simply explained: How does GPT work?”

In my understanding of the Chinese Room example, the resolution to the argument is that the *human* may not understand Chinese, but the *system as a whole* can be said to understand it.

With this in mind, I think asking whether ChatGPT *in and of itself* is "conscious" or has "agency" is sort of like asking if the speech center of a particular human's brain is "conscious" or has "agency": it's not really a question that makes sense, because the speech center of a brain is just one part of a densely interconnected system that we only interpret as a "mind" when considered in its totality.

73.

agency over 2 years ago | on “The singularity is close?”

It seems like the Chinese room argument is incompatible with a materialist world view. To a materialist, what could the brain be but a computer? Some sort of physical process is taking place inside that is is processing information, and at some level of abstraction that process will look like "dumb" symbolic manipulation. Yet we achieve "understanding"

74.

int_19h about 1 year ago | on “Large language models lack deep insights or a theory of mind”

Searle's Chinese room is a good example of begging the question.

As for the rest of it, the LLM is basically "raw compute". You need a self-referential loop and long-term memories for it to even have the notion of self. But looking at it at that level and discounting it as "incapable of thinking" is missing the point - it's the larger system of which LLM is one part, albeit a key one (and which we're still trying to figure out how to build) that might actually be conscious etc.

75.

notahacker over 2 years ago | on “Is DALL-E 2 ‘gluing things together’ without understanding their relationsh...”

I see it the other way round.

The Chinese Room and the brain of a Chinese-speaking person are completely different physical processes. Looked at on an atomic level, they have almost nothing in common. Mind-body dualists may or may not agree that the room is not "conscious" in the way a human is, but if consciousness is purely a material process, I can't see how the materialist can possibly conclude all the relevant properties of the completely dissimilar room and person are the same.

Those that would argue the Chinese Room is "conscious" in the same way as the Chinese person are essentially arguing that the dissimilarity of the physical processes is irrelevant: the "consciousness" of the Chinese person doesn't arise from molecules bouncing around their brain in very specific ways, but exists at some higher level of abstraction shared with the constituent molecules of pieces of paper with instructions written in English and outputs written in Chinese.

The idea our consciousness exists in some abstract sense which transcends the physics of the brain is not a new one of course. Historically we called such abstractions souls...

76.

nathias over 2 years ago | on “Consciousness is not computation”

> Back in the 1980s John Searle cast doubt on the computational theory of consciousness with the Chinese Room argument, and today it seems that most philosophers accept its validity.

hilarious

77.

jrlocke over 2 years ago | on “Is DALL-E 2 ‘gluing things together’ without understanding their relationsh...”

The Chinese room argument itself isn't very compelling. Surely the constituent parts of the brain are fundamentally governed solely by physics, surely thought arises solely from the physical brain, and surely the constituent parts (and thus thought) could be described by a sufficiently complex discreet computation.

Are we not conscious?

78.

gameman144 almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””

The Chinese Room thought experiment seems like a weird example, since the same could be said of humans.

When responding to English, your auditory system passes input that it doesn't understand to a bunch of neurons, each of which is processing signals they don't individually understand. You as a whole system, though, can be said to understand English.

Likewise, you as an individual might not be said to understand Chinese, though the you-plus-machine system could be said to understand Chinese in the same way as the different components of your brain are said to understand English.)

Moreover, even if LLMs don't understand language for some definition of "understand", it doesn't really matter if they are able to act with agency during the course of their simulated understanding; the consequences here, for any sufficiently convincing simulation, are the same.

79.

Mageek over 2 years ago | on “Consciousness is not computation”

The Chinese room argument is claimed to show how consciousness cannot be based on computation. I totally disagree - it merely shows that as an external observer we cannot verify consciousness.

80.

HeavenBanned over 2 years ago | on “Thinking as Computation (2011) [pdf]”

The Chinese room never made sense to me because, even though the person passing the arcane symbols through the door doesn't _understand_ what is being communicated doesn't mean that's not how it works in _our_ brain. Our individual neurons (or aggregate neurons depending on how they're working and firing based on symbol triggering) don't know what symbols they're processing either. So unless Searle is saying humans don't know any languages on some fundamental level, it's kind of useless because then we'd just end up admitting (something unsavory) that nobody knows anything barring some homunculus in our brains or even worse, we can never truly have free will. Those are scary implications."""




You're right, there's no need to rehash the same points! Let's focus on the fresh insights from this new batch of comments. Here are the novel ideas I found, skipping the already covered ones:

Challenging Searle's Core Assumptions:

The Laws of Physics as a Chinese Room: wcoenen uses the Chinese Room analogy to argue against human consciousness, suggesting that if Searle is right, then even our brains are just blindly following physical laws without true understanding. This turns the argument on its head and challenges the idea that human consciousness is special.
The Chinese Room as Evidence FOR AI Intelligence: circuit10 flips the script entirely, suggesting the Chinese Room actually shows that AI can be intelligent. They argue that the human's lack of understanding doesn't negate the potential intelligence embedded within the system's processes.
Consciousness as an Emergent Property: kghe3X and others emphasize the possibility of consciousness emerging from complex systems, even if individual components lack understanding. They criticize Searle for dismissing this possibility.
The Importance of Internal States: circuit10 and hackinthebochs highlight the importance of internal states in understanding. They argue that Searle focuses too much on external behavior and ignores the potential for internal representations and processes to contribute to understanding.
The Chinese Room and Substance Dualism: Several users (ftxbro, crdrost, dvt) point out that Searle's argument seems to rely on a form of substance dualism (the mind-body split), which is incompatible with a materialist view of consciousness. They argue that if consciousness is a physical process, then it should be possible to replicate it in a different substrate.
Rethinking the Role of the Human Operator:

The Human as Hardware: dwallin and others argue that the human in the Chinese Room is functioning as hardware, not the algorithm itself. The understanding, if any, would reside in the algorithm, not the human executing it.
The Human as a "Virtual Machine": crdrost suggests that the Chinese Room is like a virtual machine running within another computer (the human). This highlights the potential for multiple levels of consciousness within a single system.
Cochrane's Chinese Brain: jodrellblank proposes a variation where a human simulates the firing of neurons in a Chinese speaker's brain. This thought experiment challenges the idea that the physical substrate of the brain is essential for understanding.
Connecting the Chinese Room to Specific AI Concepts:

The Chinese Room and Embeddings: 93po wonders if modern AI techniques like semantic search with embeddings might be considered "intentional" and whether they escape the criticisms of the Chinese Room.
The Chinese Room and Large Language Models (LLMs): Many users discuss how LLMs relate to the Chinese Room. Some argue that LLMs exceed the capabilities of the Chinese Room, while others see them as a modern instantiation of the thought experiment.
The Chinese Room and Intentionality: Several users emphasize the importance of intentionality (directedness towards something) in understanding. They argue that a system lacking intentionality, like the Chinese Room, cannot truly understand.
Broader Philosophical and Metaphysical Implications:

The Chinese Room and Solipsism: dragonwriter suggests that the Chinese Room argument could be used to justify solipsism (the idea that only one's own mind is sure to exist).
The Limits of External Observation: Mageek and others point out that the Chinese Room highlights the limitations of external observation in determining consciousness. We can't know for sure what's happening inside another entity, whether it's a human or a machine.
The Role of Qualia: Some users discuss the role of qualia (subjective experiences) in understanding. They argue that a system lacking qualia, like the Chinese Room, might be able to process information but wouldn't truly understand the meaning in the way humans do.
These comments show the ongoing relevance of the Chinese Room in discussions about AI, consciousness, and the nature of understanding. They highlight the complexities of these issues and the diverse range of perspectives that exist.

