Disagreeable Me
Philosophical thoughts from an unapologetic contrarian

Tuesday, 27 November 2012
In Defence of Strong AI: Semantics from Syntax

Is it possible to get semantics from syntax?

If it were impossible to get semantics (meaning) from syntax (symbols and formal rules for manipulating them) then we probably wouldn't have bothered broadcasting the Arecibo image to the universe in the hope of communicating with aliens.

Without any way to explain the meaning of this image, it seems to consist of syntax alone. Whether it could be deciphered by aliens in practice or not, it at least seems possible in principle, which seems to suggest that it may be possible in some cases to derive semantics from syntax.

The assertion that you can't, that there is no way to get from syntax to semantics, is perhaps the strongest response from John Searle to the systems/virtual mind refutation of his Chinese Room thought experiment (as outlined in my post on the topic).

Searle illustrates his point by saying that if he is in The Chinese Room and has a formal system of rules (syntax) for manipulating Chinese words, he has no way to get from this syntax to the semantics of what is being said. If he has no way to do this, then how can the system do it?

In Searle's view, the systems/virtual mind reply to the Chinese Room is just hand-waving until this is explained.

Let's see if I can attempt to do just that.

You don't learn to catch a ball by consciously applying the equations of physics, applied mathematics and geometry to your visual perceptions in order to know where to position your hand. These calculations are performed effortlessly by your brain with no conscious awareness.

Similarly, Searle should not expect to have an intuitive understanding of Chinese by following the syntax of The Chinese Room. The difficulty of consciously performing the calculations going on in a brain belies the effortlessness with which these calculations are performed by the brain itself. Just as catching a ball is effortless, the construction of semantics by a virtual mind from the formal rules of The Chinese Room may be trivial. In fact, it may be inevitable.

My suspicion is that minds are syntax (in the sense of formal systems of rules), and that the meaning perceived by those minds does in fact come from that syntax alone. In this post, I will attempt to paint a picture of how this is a coherent view, and so to prove that the assertion proposed by John Searle is dubious.

Definitions

Suppose we have a statement, such as
1+1=2
The meaning of this statement is easily understood by anybody with a rudimentary education. The full meaning of the statement depends on understanding concepts such as natural numbers, equality and addition.

The semantics of a statement are the concepts that are being expressed (the meaning).

The syntax refers to the way the meaning is expressed, usually using a set of predefined symbols according to a system of agreed grammatical rules.

The same semantics or meaning could be expressed in lots of different ways according to different syntactic systems or usages of those systems.
One plus one equals two
You get two when you add one to one
Un plus un égale deux 
In every case, the semantics are clear and the same.

However I think Searle's meaning of syntax is a little broader than the simple grammatical rules of representation according to some language. He would, I believe, also include the formal rules that would allow you to operate on these symbols, e.g. to assess whether the statements were true or not.

Assume you do not have to understand the meaning of the rules in order to put them into operation -- all you have to do is mindlessly follow some operation such as looking up addition tables.

So while 1+1=3 may be syntactically valid according to the grammatical rules of arithmetic expressions, it would not be valid under this broader definition that would include the formal rules of arithmetic.

I will continue to use this sense of syntax to refer to formal systems of potentially complex rules throughout this post.

The syntax refers to the system of rules and symbols that determine how to produce representations of concepts and how to perform operations on those representations.

Searle's view (the intuitive position)

Searle's point of view is that syntax is meaningless until you attach meaning to it.

Consider the statement
J^J_3<
To all intents and purposes, that appears to be a meaningless piece of junk text. It has no clear semantics.

I could give you a system of grammatical rules within which this statement is legal. I could tell you there is a set of symbols that you can put together in certain patterns, and when you follow the rules you have created syntactically valid statements.

Though you now have the ability to generate these statements, and even assess whether these statements are valid or not, you have no way of determining if these statements actually mean anything. Until I give you some sort of semantic mapping, you have no way to infer what concepts are being represented.
J means 1
^ means +
_ means =
3 means 2
< means end of statement
Therefore J^J_3< means 1+1=2, but you had no way of knowing until I told you. There is, after all, no way to get semantics from syntax. Right?

Well, maybe there is...

My view (the unintuitive position)

There is no difference between syntax and semantics.

Perhaps I'm just saying that to get your attention. What I actually believe could be expressed in a number of different ways, but these statements sum it up.
Syntax always implies at least "limited" semantics.
Syntax may imply "real" semantics
"Real" semantics are just complex syntactical constructions in human minds
Syntax always implies at least limited semantics.
No matter how simple or abstract your syntactical system, from a certain point of view, that system must always have semantics in some limited sense.

For example, lets say we have a set of symbols A {1,2,3} and a set of symbols B{+,-}, and that our syntax allows us to create valid statements of the form ABA, such as 1+2, 3-3 etc. 5+1 is not valid, and 1+1=2 is not valid.

In fact, to make sure we forget the meaning of these symbols (we're trying to define a formal abstract syntax which is as devoid of meaning as possible), let's replace these symbols with less familiar ones.

Let's say A is {!,",£} and B is {%,^} so our valid statements look like !%".

So now we have a formal syntactical system which has no semantics, right?

Not quite. Because in the statement !%", the % symbol, for example, could be said to have the semantics of "a separator of elements of A".

So, what I'm saying is that syntax always implies at least limited semantics, because in order to have a syntactical system each symbol or group of symbols must have some limited "meaning" (or "role" if you prefer) when interpreted in the context of that syntax. The rules of a language may not directly imply the semantics of concepts such as addition or subtraction, but they do at least imply the semantics of how to use symbols within that langauge.

This may not be "semantics" as others would use the term, but it is semantics of a sort.
Syntax may imply "real" semantics
But this was a very simple and pretty useless example system. If we had a much more complex set of rules, which distinguished elements of A and B from one another and introduced the equality operator, we might have a more useful system capable of expressing concepts which we use and understand every day.

If our more advanced syntax allows us to assess all true arithmetical statements such as "1+1=2" as valid but recognise as invalid all false statements such as "1+1=3", then my contention is that this system does actually imply the semantics of addition, equality etc.

Now, if these symbols were replaced by more alien ones, and especially if the grammatical rules were changed so as to be utterly different, then the task of recognising these familiar concepts embedded within the syntax becomes much more difficult. It might take a very clever person to spot that the inherent semantics of this syntax are the same as our concepts of addition, subtraction etc.

And yet, for more simple examples at least, it might still be possible to do so (again, see the Arecibo message). If it is possible in principle to derive meaning from syntactical systems in some cases, and especially if a concept is completely and perfectly captured in a syntax, then allow me to propose that in these cases the full semantic content of the statements are in fact directly implied by the syntax -- the "true semantics" are identical to the "syntactical semantics". When a mathematician discovers that an alien syntax represents addition, all she is doing is recognising this identity. She is not conferring meaning that was not already there.

For very complex systems (e.g. The Chinese Room), it may be impossible in practice to map the concepts expressed to familiar ones, even if such mappings exist. As such, no mathematician investigating the systems would be capable of recognising the result of syntactical operations as statements which have meaning beyond that syntax (e.g. Searle doesn't understand Chinese). Even so, these systems would have their own internal semantics of syntax as outlined above.

So, in some cases, a mathematician can recognise the semantics of a syntax and in other cases she cannot. What does this ability to recognise have to say about whether the semantics of a syntax are actually meaningful or not?

Nothing!

As I have pointed out, all syntax is inherently meaningful within the context of the rules of that syntax. If the semantics of a syntax happens to be identical to a concept familiar to humans, this is true whether or not humans can appreciate this fact.
"Real" semantics are just complex syntactical constructions in human minds
If syntax can imply semantics unambiguously, then those semantics are inherent in the syntax. This is relatively easy to understand for simple concepts such as arithmetic or the rules of propositional logic. It is harder to see how this could be the case for statements such as "The weather outside is frightful".

In propositional logic, the statement "P v Q" means "P is true or Q is true". It is my view that this meaning is inherently implied by the syntax of propositional logic and the truth tables of the "v" (or) operator.

And yet when we explain propositional logic to students, we usually give examples which confer additional semantics to these examples.
P means "It will be warm tomorrow". Q means "I will wear a jumper tomorrow". P v Q means "Tomorrow will be warm or I will wear a jumper."
Clearly these additional semantics are not implied by the formal syntax of propositional logic, and yet we can use propositional logic to make deductions of a very complex nature with real world implications. We take the output from the formal systems and infer meaning from the results, meaning that does not come from the formal systems themselves. The semantics do not come from the syntax alone in these cases.

And this is how we use computers. When we program a computer to simulate something or to perform a computation, we add the meaning when we interpret the results. We associate the symbolic output from the computer with concepts we understand but the computer has no comprehension of what those results mean to us.

It is perhaps this laying on of meaning through interpretation that leads to the intuition that you can't get semantics from syntax alone and the belief that computers cannot truly understand. This belief is mistaken in my view.

I would argue that while the computer may not understand that P means that tomorrow is sunny, this does not mean that the symbols on which the computer operates are inherently meaningless. It does at least operationally understand that P is a symbol which may be in state "true" or state "false". This is a much more anaemic interpretation than we would infer, but it is all the computer has available to it. We have not programmed our full understanding of the concepts of "sunny", "day", "warm", "wear", "I", "jumper" etc into the system because we do not need to do so for the system to be useful to us.

In practice, it is pretty much impossible for us to program much of our knowledge and understanding into a computer, because this massive, tangled web of inter-associations is infeasible to capture in electronic form with current technology.

Take language as an example. Each person has an intuitive understanding of the rules of language, and to a superficial extent language seems to follow a grammar and looks like it should be amenable to automated analysis. Even so, the codification of these rules as a formal syntax has so far largely eluded computational linguistics researchers. Despite tremendous advances in the field, there is still no convincing computer system that can reliably interpret arbitrary verbal commands as input, Star Trek notwithstanding.

This is perhaps because of the interconnectedness of everything. In order to perfectly understand language, a computer would need to perform a number of complex operations which come naturally to humans. For example, it would be helpful if it could infer unspecified intentions of the speaker. In order to do this, the computer might need to be capable of some degree of empathy. And in order to do that, it would need to understand human emotions such as feeling cold, happy, warm, bored, angry. And in order to do that, it might also need to understand things about the world that can provoke these states, so it would need to understand rain, entertainment, air conditioning, etc.

And that's just a sample. In order for a computer to fully understand anything, it needs to understand nearly everything. And that's hard to do. So hard to do that it hasn't been done. Yet.

So, to sum up, what we take to be "real semantics" are just the "syntactical semantics" inherent in the unbelievably complex web of symbols represented by the "syntax" of our minds. If we succeeded in creating a computer that was able to operate on these symbols in much the same way that we can, I see no reason to doubt that that system would truly understand just as we do.
Posted by Disagreeable Me at 18:06
Labels: consciousness, John Searle, metaphysics, mind, philosophy, semantics, strong ai, subjectivity, syntax, the chinese room
57 comments:

Inti13 August 2014 at 21:47
Is it possible to get from semantics to syntax, you ask?
Surely this question is the wrong way round; to refute the Chinese Room you need to get from syntax to semantics, not the reverse.

I assume from your next paragraph that this was just a slip of the keyboard.


Your image argument falls down at once; non-abstract visual depictions (pictures!) are not purely syntactic; they have derived intentionality -derived from the intentional states of the mind that created them., That's exactly what distinguishes them from abstract art!. They are called, in artistic terms, representational art, and representation implies semantic content. The hope is that the receiving minds will be able to reverse the process and retrieve that semantic content, for instance by recognising that the transmitting species would be likely to put their own image at the centre, and by linking the shape of the Arecibo dome to their knowledge of physics and electrical engineering. S it looks as if you have produced a small argument against your case.
As for the rest, I think that you are using terms syntax and semantics in ways that don't resemble those usually used in philosophy of language or linguistics. Of course you can prove almost anything by redefining terms, but that isn't convincing. You are simply confusing the formal property of syntactical operators with the semantic, contentful properties of meaningful contents, in whatever language. Of course the syntactic operators have to have procedural or formal meaning – that'#s definitional – it's part of it means to be syntactical. What you are doing is analogous to refuting “all bachelors are married men” by redefining the word bachelor – it works, in its own little world, but is a useless, invalid move outside that world.

Next, you seem to assume that when I catch a ball, my mind solves equations unconsciously, brining together inertia, gravity, air resistance to calculate my response. You may be right, but I don't think most neuroscientist agree with you. That's another computationalist prejudice. Rather than solving equations, my nervous system uses experience and extrapolation through repeated trial and improvement to hone a skill in extrapolating paths; no equations involved. As I say, I could be wrong, it's an empirical question. But as far as I know, the balance of evidence and theory supports my interpretation.

The meaning of semantics is not just that it means something, but that it can be used to make statements about the world, beyond the formal system used to express that meaning. That, too, is definitional.

Your main argument seems like a really desperate move to sustain the computationalist faith that you assert at the beginning in the face of huge, perhaps insuperable difficulties.

You use the term rules in some places; again, a dodgy word in this concept, and one which, like your mental unconscious equations in the intro, smuggles in question-begging computationalist assumptions (that the mind works via “rules”), but rules are observer dependent things. As Raymond Tallis shows in the last section ODF “why the mind is not a computer, rules are either a circular question-begging assumption (because the same objections applies as to algorithm, or procedure etc., that they are observer relative, mind-created things in the first place, or they a misconceived false synonyms for “laws of nature”. As he mentioned, Wittgenstein went further and claimed that rules can only exist in societies, because rule-following is an essentially social practice. Rocks don't obey a rule to fall downwards; that's an outdated Aristotelian physics. And there's simply not reason to believe that minds/brains obey rules, or even that rule (except in the everyday, commonplace and socially determined sense of school rules, rules off the game etc). What's more, rules are the sort of thing that we can disobey, or make a mistake about. Not the sort of thing that could underpin minds.

Reply
Replies

Disagreeable Me13 August 2014 at 23:39
Hi HPW,

Thanks for your feedback.

"Is it possible to get from semantics to syntax, you ask?
Surely this question is the wrong way round; to refute the Chinese Room you need to get from syntax to semantics, not the reverse."

I can't find where I made this mistake. The title of the post is semantics from syntax, which means the same thing as "from syntax to semantics". The word order is less important than which prepositions are used where.

"Your image argument falls down at once; non-abstract visual depictions (pictures!) are not purely syntactic; they have derived intentionality -derived from the intentional states of the mind that created them."

But you miss the point. The receiving mind only gets the raw data (syntax) and has to infer the meaning (semantics). How is this achieved? How is it possible to get semantics from structures such as this? How does it even figure out that the data stream is to be interpreted as an image rather than an audio file or a 3D mesh? The Arecibo image is not after all a physical painting.

You do give a plausible sketch of how it might work, but this sketch is in my view analogous to how I think the Chinese Room works so it actually suits my purposes.

The formal symbols in the Chinese Room, like The Arecibo message, really do have meaning. Furthermore, like the Arecibo message, the mapping between the formal structure and the meaning is not entirely arbitrary, because the causal and formal relationships between the symbols in the Chinese Room are isomorphic to those of the objects being mapped.

In any case, the Arecibo message is only an introduction to the problem so it's not particularly important. Besides, I think this image is bordering on the abstract, at least in places.

"What you are doing is analogous to refuting “all bachelors are married men” by redefining the word bachelor "

I understand where you're coming from but I do not agree that this is what I am doing. To you, I appear to be equivocating when I use the term semantics to refer to the formal role of a symbol in how it tends to relate to other symbols. My view is that this is not an equivocation, but that what you call semantics is in fact an expression of very complex syntax. You see a fundamental distinction where I see continuity. I think perhaps a new article explaining my view of how syntax and semantics work in human minds would help to clarify this point.

It's hard to express this idea clearly, but perhaps another example might help. Imagine you're in the Chinese Room and you see the symbols 你好 on a regular basis and note that they are most commonly responded to with 你好. Even if you know nothing of human communication, you at least know that seeing 你好 at least means that the next output will also be 你好. You may also infer that this is some sort of protocol for beginning a conversation because these communications usually occur at the beginning of a series of exchanges.

(你好 just means Hello).


Disagreeable Me13 August 2014 at 23:39


So even in a field as complex as human communication, it is possible to infer some limited meaning from just the roles of the symbols alone. This article argues that this undermines Searle's thesis that you can't get semantics from syntax.

Of course, if you were to perform this inference, you would do so as an observer of the system and draw a little on your own world knowledge to do so. I agree that it is quite another thing to argue that these symbols actually have meaning to the system itself. However, I do see a similarity in the two problems which perhaps you do not. Mainly, I'm just trying to introduce a little doubt into Searle's contention that syntax and semantics are entirely distinct.

"I could be wrong, it's an empirical question. But as far as I know, the balance of evidence and theory supports my interpretation."

I think you just have a narrower interpretation of what solving equations entails. Construed broadly, a ball travelling in a parabolic trajectory is itself solving an equation. I'm not suggesting that a brain has a mental pen and paper and that it solves an equation step by step by performing algebraic operations as an equation would be solved manually. Rather it has circuits, more like a calculator, that implement a lot of complicated trigonometry related functions natively and "automatically". As far as I know, this is compatible with current knowledge -- some savants seem to have tapped into the ability to do this kind of thing consciously. But it's not that important a point. This is just illustrating a difference between how Searle seems to conceive of computationalism and how computationlists themselves conceive of it.

"The meaning of semantics is not just that it means something, but that it can be used to make statements about the world, beyond the formal system used to express that meaning. That, too, is definitional."

OK, but I'm not sure how that defeats my argument. In order for a statement to be useful for communicating a meaning, it has to have that meaning in some sense and this is the question I'm exploring

In any case, the question of semantics from syntax in the context of the Chinese Room and the like is not really about communication between two conscious observers. It is about whether symbols within an information processing system can have meaning to that system itself. This is the idea I am addressing in this article.

"but rules are observer dependent things"

Not necessarily. I would characterise physical laws as rules. To me, rules are just regularities. They may or may not be observer dependent. I would indeed say that the law of gravity is a rule. If you disagree then you're just adopting a more specific interpretation of rule. Which is fine, and I'm happy to draw that distinction also if you deem it helpful, but then our disagreement is not particularly profound.

However, I would suggest that the rules of an algorithm are not optional or social. The computer doesn't decide to obey, much less disobey. When the program is running they're more like the laws of nature. So, you tell me, is the word "rule" inappropriate? What word would you prefer?

These rules are usually laid out by a programmer, so I guess this programmer may be the observer you require, but the rules of software can continue executing long after the programmer has ceased to be. Furthermore, there doesn't have to be a programmer in the first place. Rules can evolve. This has been done with experiments in genetic programming and, I would argue, in natural evolution. It is possible that the mind is an algorithm and this does not presuppose the mind of a programmer.

Also, I would admit that my position is predicated on Platonism. On Platonism, algorithms, as mathematical objects, exist mind-independently. If you don't accept Platonism then that's another point where we part ways.


Inti17 August 2014 at 20:43
a) Intrinsic intentionality. ;” I am now thirsty, really thirsty, because I haven’t had anything to drink all day”. This is an example of a sentence that ascribes a real intentional mental state to oneself.
b)
c) C) Ascribed intentionality; In French, “J’ai grand soif means “I am very thirsty.” In this case, Searle writes, “I ascribe intentionality to the French sentence, that is, the French sentence means literally what I say it does. But the intentionality is not intrinsic to that particular sentence just as a syntactical object. That very sequence might have meant something very different of nothing at all. Speakers of French can use it to express their intentionality, but it is not intrinsic intentionality. It is derived from the intrinsic intentionality of the users of the language.”
d)
So, I think I can put that together with my description of the way that the aliens might come to understand the Arecibo picture without common syntax or language. The picture is, as you point out, just some data, which can be assembled to look like objects. If none of these objects are familiar enough for the aliens to recognise, the picture will remain meaningless to them, and the communication will fail. However, perhaps they too have built radio dishes, so they may be able imagine what they are in the picture. If so, the intentionality we ascribed to that pattern will be understood, and so the aliens will form the real intrinsic intentional state of thinking “Ahh, that’s a radio dish, that’s how they sent this message”, which is what we hoped to achieve.

The important point is, that’s how all communication works; spoke and written language and pictures. In the first tow cases, we can communicate much more complex messages, using the combination of symbols and syntax to organise those symbols. Using these means of communication, we can start with the real intrinsic intentionality in the send/speaker’s mind, encode it in the ascribed intentionality of the particular language, and the receiver/hearer can interpret it to reproduce real intrinsic intentional states, that is, conscious awareness of the intentional states (which means very roughly thoughts or knowledge) in the sender's minds. BUT the ONLY works when there is a prior shared understanding of both the syntax and the meaning of symbols in the language, or in the case of pictures, enough resemblance to play that role in the absence of language.

In the light of this, I can’t see how you can claim that my sketch of the way the Arecibo message could successfully convey meaning is “analogous to how I think the Chinese Room works so it actually suits my purposes.” This explanation is not only complete different from any computationalist/functionalist argument, but it touches on why those arguments fail; both in my explanation of successfully communicating via pictures and in the use of language, the meaning, the semantics of the message are not intrinsic to the message; they depend either on share cultural agreement between conscious minds about what the elements of the message means, or enough shared experience between the minds to allow informed guessing in the pictorial case. This is exactly in line with the Chinese Room, and completely antithetical to the computationalist objections to the CR.

That took a bit longer than I expected – I’ll try to be briefer in the rest of my reply.

Reply

Inti17 August 2014 at 20:43
“To you, I appear to be equivocating when I use the term semantics to refer to the formal role of a symbol in how it tends to relate to other symbols. “
No, you are not equivocating. An equivocation means sliding unacknowledged between different meanings of the same word. But syntax and semantics have never had the meaning you are trying to ascribe to them. Of course, anyone can change the meaning of a word whenever they like.

“'When I use a word,' Humpty Dumpty said, in rather a scornful tone, 'it means just what I choose it to mean — neither more nor less.'
'The question is,' said Alice, 'whether you can make words mean so many different things.'
'The question is,' said Humpty Dumpty, 'which is to be master — that's all.'”


If you change the meaning like that, you end up speaking a private language (Wittgenstein) Trouble is, nobody else is speaking that language, so communication fails.

“It's hard to express this idea clearly, but perhaps another example might help. Imagine you're in the Chinese Room and you see the symbols 你好 on a regular basis and note that they are most commonly responded to with 你好. Even if you know nothing of human communication, you at least know that seeing 你好 at least means that the next output will also be 你好. You may also infer that this is some sort of protocol for beginning a conversation because these communications usually occur at the beginning of a series of exchanges.”

Yes, in this case, Searle in the room is noticing regularities in the sequence of inputs and outputs. Of course, these regularities are already specified by the syntax, the books of rules. But all he sees are combinations of symbols that remain absolutely meaningless to him. So this doesn’t get you any further forward; still only syntax, either written down or guessed by Searle. Still no semantics.

“So even in a field as complex as human communication, it is possible to infer some limited meaning from just the roles of the symbols alone. This article argues that this undermines Searle's thesis that you can't get semantics from syntax.”
No, sorry, this doesn’t show that we can get any meaning form the syntax or “the role of the symbols, it only asserts it again. Searle did not have any more meaning after noticing that this symbol follows that; he already had that info, in the rule books, and they are purely syntactic in themselves.

On your point about solving equations: I’m not sure whether you are actually agreeing with me here. If the process that leads me to catch the ball “functions natively and "automatically"” that question is, does it work by following symbolic algorithms, which is what computers and calculators do, or does it do it by means of the balance of electrical/chemical weightings underpinned by the structure of neurons and neurotransmitters which (as far as we can tell at the moments) is make up what Searle calls the intrinsic causal powers of the brain. I think the latter is at least the more parsimonious explanation; no need for a structure of symbol based computation encoding equations over and above that.
In any case, you seem to beg the question in your original post, by first assuming that the mind “solves equations” in a computationalist sense, then using that as a premise for your argument to prove precisely that point

Reply

Inti17 August 2014 at 20:44
In response to my comment that “"The meaning of semantics is not just that it means something, but that it can be used to make statements about the world, beyond the formal system used to express that meaning. That, too, is definitional."

You reply
“
OK, but I'm not sure how that defeats my argument. In order for a statement to be useful for communicating a meaning, it has to have that meaning in some sense and this is the question I'm exploring”
It defeats you argument by pointing out the qualitative difference between syntactic rules and the semantic contents that hey help to convey. Syntax is about how utterances are structured, how the elements of an utterance hang together, are sequenced. In that sense, syntax points inwards to the utterance, pointing out how it’s constructed and helping the receiving mind to interpret it. Semantics, however, points outward, via the socially- determined meaning of the symbols used (words and characters etc.), towards the world outside, and convey meaning about the wider world. Syntax doesn’t do that; it simply organises and marshals the things that do. And that difference if definitional (like “all bachelors are unmarried men), it’s derived from the very meaning of the words and concepts “syntax” and “semantics”. You can’t argue about that, you can only redefine the terms individually and arbitrarily. That is like saying “When I say bachelor, I mean men who weren’t married in church!” You haven’t found out something new about the concept, you’ve changed its meaning by fiat.

On the question of rules it would be a bit repetitive to go too deeply into this, I think. I mentioned that there are differences between rules and laws of nature, for instance you can break or ignore a rule, or just not apply it because you are unaware of it. It seems to me that you are using “rule” to mean something like “program”; and defined set of instruction that defines of describe how you have to carry out a task. If so, you are just repeating the claim that Searle set out to refute.

I think that genetic algorithms are a red herring here. They are still the product of a human programmers work, and in fact humans play the role of natural selection; in fact they are a real-life legitimate example of intelligent design, by human, not divine designers.

I said rules are observer dependent (i.e. the product of social processes between people in societies, not of brute physics) because I’m taking rule to be roughly the same as algorithm or program, for our purposes. There are two types of things in the world; observer independent ones that are intrinsic to physics, like rocks, planets, stars, atoms; and observer relative things like political positions, money and languages. The first type would still be here if no conscious being existed or have even existed. The second are real but only exist because humans agree about the, collectively, and ascribe their meaning to them. And, as Searle shows, computing falls into the latter group. If you don’t accept this, I’m happy to argue it further.

Now we’ve moved beyond the Chinese Room, which shows that syntax is not enough for semantics. Searle went on to argue that syntax is not intrinsic to physics, along the lines I’ve outlined, amongst other telling points (I think) against computationalism.

I find that a lot of people have only read the CR, and nothing else that Searle’s written. In fact, quite a few people who comment adversely online have clearly only read summaries of Searle’s CR argument and hostile summaries at that. Still, I suppose that happens to many philosophers. Anyway, I really suggest that anyone interested take a look at his paper “Critique of Cognitive Reason” for a good summary of some of his arguments about Ai and computational theory of mind. It’s only 15 pages long and available at http://www.federaljack.com/ebooks/Consciousness%20Books%20Collection/John%20R.%20Searle%20-%20The%20Critique%20of%20Cognitive%20Reason.pdf


Reply

Disagreeable Me19 August 2014 at 19:59
Hi Inti,

The Arecibo message contains not only pictures of objects that may look familiar to aliens. It also contains representations of information or concepts (e.g. numbers, elements, chemical formulae). These representations are not visual so much as structural, so part of the job of interpreting unfamiliar representations is in identifying structural similarities between representations of concepts and concepts themselves (also illustrated by my example of interpreting "hello" in The Chinese Room). This ties in with my thesis that semantics is partly achieved by such structural or syntactic isomorphisms. I think that what gives mental representations their intentionality is structural similarity between the mental representations and the concepts so represented (e.g. my concept of a car is associated with my concept of wheels; a real car is associated with four real wheels) as well as causal links (e.g. I see a real car and my mental concept of a car is activated, or my mental concept of a car is activated in a certain way and I decide to go for a drive).

I agree with your sketch of intrinsic and ascribed intentionality. In fact it agrees so well with my thoughts on this area that it is a great pleasure to read someone else capture my thoughts so well.

"the meaning, the semantics of the message are not intrinsic to the message; they depend either on share cultural agreement between conscious minds about what the elements of the message means, or enough shared experience between the minds to allow informed guessing in the pictorial case"

Absolutely! I have been in arguments with philosophers before making exactly this point. The thing is, I think The Chinese Room is one of these conscious minds.

The problem is that Searle is assuming that failure to ascribe intentionality means there is no intrinsic intentionality. At best, it's an appeal to an intuition I simply do not share. I think that intrinsic intentionality is just how a mental representation feels to the host mind. In The Chinese Room, the mind is not Searle's but that of the system, and to that mind I believe the shuffling of symbols will be intrinsically meaningful due to structural similarities with and causal relationships to objects in the real world and concepts shared with other minds -- the same kind of features that give biological mental representations their intentionality.

I disagree that I am speaking a private language. I am using syntax and semantics in the same sense used by Searle when he insists that you can't get semantics from syntax. In Searle's usage, syntax is not "how utterances are structured". He is using the term in a broader sense to describe all formal systems such as the representations in The Chinese Room. These are not utterences but states of an algorithm with causal relationships to other states. Nor is semantics "the socially-determined meaning of the symbols used". What he means by semantics are real intentional states -- real understanding, beliefs, desires etc and not merely representations of same.

Of course your definitions of syntax and semantics are perfectly sound and related to Searle's usage, but your interpretation is more applicable to philosophy of language than to philosophy of mind. If you object to my usage here, your beef is with Searle and not with me. What I am discussing is how intentionality can arise from mechanism or computation. Semantics from syntax is just a snappier way of saying this (which is presumably why Searle phrased it so).

Reply
Replies

Disagreeable Me19 August 2014 at 19:59
"Of course, these regularities are already specified by the syntax, the books of rules."

No they aren't! This implies a very naive interpretation of what the book of rules would need to be like. If the mind is supposed to behave analogously to a human brain, there will be no simple high-level rules. In fact, the algorithm in the rules will amount to a program which simulates a human brain, neurons, neurotransmitters and all, with inputs being converted into optical or auditory signals before processing. Such a simulation contains no simple rules of the form "Respond to 'Hello' with 'Hello'". If there are such regularities, they will only come about through many layers of emergent, complex, unpredictable interactions.

To restate the point, Searle can start knowing nothing about the symbols, but just from observing how they are used he can begin to glean some very simple ideas about their functional roles. My thesis is that semantics is simply a very complex suite of functional roles. The examples I have offered (the structural representations in Arecibo and the functional role of "Hello") are of course trivial compared to the complexity of what the brain can represent and process, but to me at least they help to visualise how this can be.

"does it work by following symbolic algorithms, which is what computers and calculators do, or does it do it by means of the balance of electrical/chemical weightings underpinned by the structure of neurons and neurotransmitters"

This is a dichotomy I do not recognise. If the universe appears to be governed (at least approximately) by computable mathematical rules, then every process in the universe can be simulated on a computer, including chemical processes. Again, it seems to me you have a very naive idea of what a brain algorithm would look like. Algorithms do not have to be neat and orderly with sensible representations and rules. They can be messy, dynamic, complex, chaotic. I don't know what is required at a minimum to achieve concsiousness in a computer, but for the sake of argument we can imagine that it takes the simulation of neurons and neurotransmitters. The actual processing of meaningful mental concepts may occur at a great remove, many levels of description above the actual code itself. This is why one couldn't tell from looking at the code that "Hello" begins a conversation.


Disagreeable Me19 August 2014 at 20:00

"In any case, you seem to beg the question in your original post, by first assuming that the mind 'solves equations' in a computationalist sense, then using that as a premise for your argument to prove precisely that point"

No, I'm not using it to prove my point. I'm using it as an example to show how Searle's conception of algorithmic mentality is naive. There is much that the brain does that is automatic or unconscious. Searle seems to think that just because he is not aware of this physical, mechanistic information processing means that it is not happening at all, or perhaps that the effortless intuitive ease with which we do many routine mental tasks cannot be reconciled with the laborious information processing it seems to be when we understand it in detail. The process of human deliberation does not at all feel like implementing an algorithm, but that does not mean that it does not supervene on an algorithm of some kind.

I disagree that genetic algorithms are a red herring. For genetic algorithms, all you need to specify is some problem and then the system, over time, will evolve a solution to that problem using the same kind of process as natural selection. Often the solutions to the problems will appear ingenious and surprising to the designers of the system. This is quite unlike manually designing a solution. Sure, they still had to set up the system, but consider for a moment the hypothesis that this universe has a creator. If this were true, would that suddenly mean that we are philosophical zombies or that our minds only have the intentionality put there by the designer? I don't think so. The creator might not even have foreseen that life would evolve at all. I see no difference in principle between this hypothetical situation and that of genetic algorithms.

I see where you're going now with rules being observer relative. I am familiar with Searle's argument in this regard. Computation, according to Searle, is in the eye of the observer. Pretty much any physical system can be interpreted as computing something, and if your interpretation is byzantine enough you can force nearly any interpretation. It follows, according to Searle, that computation doesn't actually exist without some observer to interpret it so.

I am a mathematical Platonist. I think all mathematical structures exist independently of mathematicians. I think the human mind is such a structure. I think the computations I'm talking about exist independently of any observer and indeed independently of any physical instantiation -- they are not really located in time or space. If Searle says that computationalism leads to the absurd conclusion that all possible minds exist, then I agree with him with the caveat that I don't find it absurd. The other (in my view) tenable computationalist position is the materialist one, denying that minds really exist at all -- that consciousness is an illusion and all there is is the brain. I think the difference between the two views is relatively trivial -- we just define the term 'existence' differently. My Platonism is really just a preference to use the language of existence to describe mathematical objects such as minds.

I have not yet read Searle's paper but I have listened to many of his lectures. It's just a format I prefer. I feel I am familiar enough with his arguments to comment, although I will read his paper if you insist that it will clarify some misunderstandings on my part (though I doubt that very much).

Reply

Inti22 August 2014 at 16:02
Since you use the car example, I’ll repeat a quote from Searle (Critique of Cognitive reason) that I posted elsewhere – it seems relevant.

“To make this difference clear, let us go through an example. Suppose I see a car coming toward me. A standard computational model of vision will take in information about the visual array on my retina and eventually print out the sentence, "There is a car coming toward me." But that is not what happens in the actual biology. In the biology a concrete and specific series of electrochemical reactions are set up by the assault of the photons on the photo receptor cells of my retina, and this entire process eventually results in a concrete visual experience.

The biological reality is not that of a bunch of words or symbols being produced by the visual system; rather, it is a matter of a concrete specific conscious visual event- this very visual experience. That concrete visual event is as specific and as concrete as a hurricane or the digestion of a meal. We can, with the computer, make an information processing model of that event or of its production , as we can make an information processing model of the weather, digestion , or any other phenomenon ,
but the phenomena themselves are not thereby information processing systems.

In short, the sense of information processing that is used in cognitive science is at much too high a level of abstraction to capture the concrete biological reality of intrinsic intentionality “
You reply to another of my points:
“ "the meaning, the semantics of the message are not intrinsic to the message; they depend either on share cultural agreement between conscious minds about what the elements of the message means, or enough shared experience between the minds to allow informed guessing in the pictorial case"

Absolutely! I have been in arguments with philosophers before making exactly this point. The thing is, I think The Chinese Room is one of these conscious minds.
The problem is that Searle is assuming that failure to ascribe intentionality means there is no intrinsic intentionality. At best, it's an appeal to an intuition I simply do not share.”

No, he is saying that there has to be a conscious, intentional mind already in order to ascribe intentionality. And that a digital data processing system can’t possibly do that, because it is entirely composed of elements that only have ascribed intentionality. So there must already be a conscious mind present in order to ascribe meaning to the purely symbolic elements. That is not an intuition at all, it’s logically entailed. Of course, you think that the System Reply gets you out of it, so let’s look at that.:
“I believe the shuffling of symbols will be intrinsically meaningful due to structural similarities with and causal relationships to objects in the real world and concepts shared with other minds -- the same kind of features that give biological mental representations their intentionality.”
There are two features you use to explain how conscious meaning arises; (a) “Structural similarities with (objects in the real world and concepts shared with other minds)” and (b) “causal relationships to objects in the real world and concepts shared with other minds” I’m not sure if you meant both “objects in the real world” and “concepts shared with other minds” to apply to both explanatory elements? I’ve assumed so.

Reply
Replies

Disagreeable Me25 August 2014 at 14:35
Hi Inti,

I have answered that car analogy point elsewhere. Indeed, my original car analogy shows how this Searle's fails -- cognitive representations are not sentences, as in Searle's analogy, but rich, dynamic substructures of a semantic web, as illustrated in my simple sketch of how the car concept is associated to the wheel concept.

On the problem of ascribed intentionality... information processing systems do not have to have ascribed intentionality. They can learn and evolve much as humans do and can develop their own representations for external objects. There doesn't have to be a mind deciding which representations to use for which real objects.

Besides, you're very much begging the question when you're assuming that information processing systems are entirely composed of elements that only have ascribed intentionality. The computationalist thesis is that intentionality is nothing more than a network of causal associations. By reducing it to its basic elements, you are ignoring these associations and therefore missing the forest for the trees. Individual neurons have no intentionality either. If there has to be a conscious, intentional mind in order to ascribe intentionality, where do brain structures get their intentionality? It's because the brain, as a whole, comprises a conscious, intentional mind, right? So that's just the point I'm making for an AI system. By making the point that there cannot be intentionality since there is no mind you are begging the question by assuming there is no mind.

Reply

Inti22 August 2014 at 16:03
(I suspect we are both riding roughshod over some complex debates in philosophy of mind and language, but let’s do our best.)
Lets start with (a).I can’t imaging a naturalistic philosophy of language that doesn’t assume that there has to be some relationship between concepts (however they are made up) and objects in the outside world. But it’s not clear to me what exactly “structural similarities” means. What are the structures concerned? Biological structures? Algorithms? Linguistic structures? I feel in need of more detail in order to understand what you mean. I presume you don’t mean to suggest any naïve coresp[onedce of form, and you are not supporting a position that would entail that everyone who believes, say “Paris is the capital of France” has the exactly the same configuration in their brains encoding that?

Now in the case of (b)
I have some of the same difficulty, but with an added complication. What is a “causal relationship”? Here’s a source of possible confusion through equivocation. When Searle talks about the “causal powers” of the brain, he means actual physical processes, comparable with those involved in digestion, for example. In this sense, causal is counterposed to a symbol-manipulation system, such as a computer. Obviously, there are physical causal processes at work when a computer runs a program, but the whole point of the computationalist argument is that these are irrelevant; it’s the form of the program that matters, and it’s multiply realisable on any hardware; electronics, cogwheels, mice and cats and cheese.
So you can’t mean by causal what Searle means, or you’re conceding his point entirely. So what can you mean? That something going on in my brain causes me to act in certain ways? Trivially true, for non-Cartesians. That purely computational structures can have the power to cause consciousness? That’s exactly what’s at issue, so if that is what you mean, you’re just reasserting what you are setting out to prove. But I think it becomes clearer in a moment.

“I disagree that I am speaking a private language. I am using syntax and semantics in the same sense used by Searle when he insists that you can't get semantics from syntax.

If so, I submit that you’ve shifted your meaning from your starting position. There, you claimed that syntactical elements have “some limited meaning” and therefore that we can move from syntax to semantics. I think I showed that this is wrong; that the only meaning that syntax has refers to how elements of an utterance are organised (inward looking) while semantics, to be worthy of the name, must be outward looking. Whether I’ve succeeded in showing that or not, the point is that your use of “syntax” isn’t just “counterintuitive” – it describes a different concept from that usually implied by the word. Hence, private language. If you don’t see that, I will just have to leave it to any other readers of this discussion to judge.

Reply
Replies

Disagreeable Me25 August 2014 at 14:54
In regards to (a), I mean logical structure. I picture mental representation as a semantic web, formed largely of nodes wired up to other nodes with various kinds of links. The semantic web represented in a person's brain will be similar in logical structure to a concept map of that person's knowledge.

http://en.wikipedia.org/wiki/Concept_map

The disconnect between ordinary concept maps and actual meaning is in my view largely a result of their sparsity. In this view, to have a fully accurate concept map of an actual person would be to have a dynamic, active, firing concept map with embedded qualia and sensations and processing functions, with the ability to grow new nodes and form new links, such that it would essentially be a complete and conscious reproduction of a mind.

I am not at all saying that two people would have the same representations of 'Paris is the capital of France', or indeed of any concepts. Each of these words will have different associations and memories in the mind of each person, and the representation of a single sentence cannot easily be separated out from the rest of a semantic web. It only has meaning in context as part of that web, and each person has a different web because each person has different attitudes and aptitudes, experience and knowledge.

As for (b), what I mean by causal relationship is something about how mental representations are linked to objects in the world and concepts in other minds. If I see a car, there is a physical chain of cause and effect, beginning at my retina, passing through my optic nerve, into my visual cortext, etc, until eventually the neurons which correspond to nodes in my semantic web which are associated with the concept of 'car' are activated. Causality also works the other way. Part of the decision-making process involved in deciding to go for a drive will involve activating those same neurons. So the semantic link between these neurons and the concept of car does not have to be ascribed and it is not arbitrary, it's there at a physical, causal level in a way that can be reproduced in computers.

On my usage of the word syntax, you believe you have shown how my usage of the term is a private language. However you have not really answered my point that I am using it in the same sense Searle is using it, and if not then Searle's allegation that you can't get semantics from syntax may not be wrong but is completely irrelevant to computationalism. Neither of us are discussing how elements of utterances are organised, we are describing how mental or computational representations are organised, and these are not utterances. They can be many-dimensional and they can be dynamic. They can perform information processing tasks in a way that words in a sentence cannot. As such they form structures much more complex than a simple linear sequence of symbols. These structures can be structurally analogous to and causally related to other objects and so it is not hard (for some at least) to see how they can have intrinsic meaning, referring to these objects.


Inti25 August 2014 at 20:20
It’s striking that each of us thinks the other is begging the question. Perhaps we are at least clarifying some points of disagreement though.

Clearly each of us has conflicting and deeply imbedded assumptions. This is evident when you say that your semantic networks “have a dynamic, active, firing concept map with embedded qualia and sensations and processing functions” To me, you are simply restating the computationalist thesis, including its critical failure to understand that all computational representations must be observer relative, no matter how complex and many layered they are. I believe that you can’t have embedded qualia in a representation, no matter how complex or layered it is, because all representations are symbolic statements TO a conscious representational mind. We have a clash of presuppositions, it seems.

I know you are trying to show me that your concept of computationalist mind is much more elaborate than the way Searle describes them. But in principle, and in the way they work, they are the same as the ones that Searle’s addresses in this vital respect..

Searle Is simplifying to bring the underlying principles to the foreground, not because he believes that the kind of structure computationalists propose is really simple.

“If I see a car, there is a physical chain of cause and effect, beginning at my retina, passing through my optic nerve, into my visual cortext, etc, until eventually the neurons which correspond to nodes in my semantic web which are associated with the concept of 'car' are activated. Causality also works the other way. Part of the decision-making process involved in deciding to go for a drive will involve activating those same neurons.”

Great, I agree with almost all of that description, but there is no necessary computational element in your description; it’s a biological, neurological process, through and through. I know you see the semantic web as as necessarily a computational element, but why assume that? Why not take it as simply a description of a biological process? I realise that you can say that computational processes underlie your description at a lower level, but I can’t see why they are necessary. That’s what I mean when I say computationalism is overdetermined as a description. (And to repeat, anything can be described in terms of computation; the question is, does it have to be?)


Inti25 August 2014 at 20:22
“So the semantic link between these neurons and the concept of car does not have to be ascribed and it is not arbitrary, it's there at a physical, causal level in a way that can be reproduced in computers.”

To me, these two phrases are contradictory; ‘at a physical, causal level’ and ‘in a way that can be reproduced in computers’. The point is that nothing that is physically causal can ever be reproduced in computers, by any program no matter how sophisticated. Why? Because computing and computational objects only describe, they do not reproduce physical effects. I realise that here must be a physical substrate to any computation, but that is irrelevant to the nature of the computation. That’s why multiple realisability is so important; it was a starting point of computationalism, but it comes back to bite it. To recap, computation can be based on cogwheels or transistors, water pipes and so on. It is the representational structure that makes it computational, and representation is never a physical phenomenon. And to answer your question as to why the explanation via physical causal processes in the brain don’t share the same difficulties as computationalism, it’s because causal process of the brain are observer independent, while computation is observer relative. The physical causal processes are amongst the brute facts about how the physical world works, while computational process, as computation, are not. To put it very crudely, causal brain processes actually push matter about, computation doesn’t, ever.
That’s why this sentence of yours makes no sense for me: “The computationalist thesis is that intentionality is nothing more than a network of causal associations.” Associations, in your sense, are representations, (even when cast in complex computational networks) and representations cannot be causal. Only physical, observer-independent processes can be causal. At best, representations can describe or depict processes.

I’m reminded of the mistaken “luminous room” refutations by Pat and Paul Churchland. They claimed that Searle is like someone who denies that light is electromagnetic radiation because waving a bar magnet doesn’t produce light. But that’s a false analogy, and Searle easily defeats it. The account of light as electromagnetic radiation is a physical on right down to the ground. But the analogy with formal symbols fails because formal symbols possess no physical, causal powers. The only power that symbols have, qua symbols, is the power to cause the next step in the program when the machine is running (says Searle, and I think he gives too much away there; I’d say they don’t even have that power- it’s the underlying machine substrate that has even that power, and the program instruction only denotes that. Any error in this is all mine, not Searle’s, though). The relevant properties of 0’s and 1’s are abstract computational properties. And the Churchlands, like you, claimed that Searle was looking at the wrong layer, or at the wrong scale, and they too missed the point that it is a qualitative difference, not one of scale or complexity, all the way through.


Inti25 August 2014 at 20:23
On genetic algorithms.
By coincidence, I’d been thinking about genetic algorithms and evolved computation before we started this discussion. I asked myself what empirical finding could disprove Searle’s point. As I’ve mentioned, I think the genetic algorithms we have produced are far less independent of human design than you do. Setting that aside though, I had imagined a little science fiction story.

Suppose we encounter an alien species whose brains clearly function very differently from ours. They seem to be conscious, at least, they behave like it (they pass the Turing test perfectly, face to face) and they tell us that they are conscious. The more we learn about those alien brains, the more we see that they reproduce the function of what we commonly call a computer. There are direct analogues of the circuit elements, and messages pass and are controlled in ways which look very like those in our microcircuits. Does this prove the computationalist case, once and for all? Not yet; we also need to know how this state of affairs came about. If on the one hand there is a clear fossil record or equivalent evidence showing that these brains came about by the process of natural selection, that would make it much more likely that Searle is wrong, and computers really can be intentional, conscious beings. If on the other hand historical evidence showed that the aliens were the product of the kind of process some transhumanists advocate, of uploading ourselves into computational devices, that would not be any evidence against Searle. It might be the aftermath of a terrible tragedy, one that could face us if we were mislead down that path. We would have turned ourselves into philosophical zombies, which have all the external behaviour of conscious beings, but without consciousness; and there would be no possible empirical way to tell whether this was true just b examining them as individuals, as artificial phenotypes. Only the history of how they came about would provide evidence.

I see that you think that philosophical zombies are logically impossible, assuming your strong version of computationalism. I think your are write, given your premise, but your premise is wrong.

You mentioned Boltzmann brains; oddly I’d written a section about Boltzmann computers, naturally occurring computational structures that arose just like Boltzmann brains. However, I cut it out because I didn’t think it added much to the discussion.

I’ll leave the definition of syntax for now. I can’t see how you are using the word in the same way as Searle, at all.

I’ll have to respond to your other points later. Thanks for coming back to me. I’ve found this discussion very stimulating.

Reply

Inti22 August 2014 at 16:03
You say
“In Searle's usage, syntax is not "how utterances are structured". He is using the term in a broader sense to describe all formal systems such as the representations in The Chinese Room. In Searle's usage, syntax is not "how utterances are structured".”
First, you seem to contradict this when you say “Of course your definitions of syntax and semantics are perfectly sound and related to Searle's usage, but your interpretation is more applicable to philosophy of language than to philosophy of mind. If you object to my usage here, your beef is with Searle and not with me. “By the way, Searle has written, rightly I think, that philosophy of language is a subset of philosophy of mind. That’s true because without mind, there is no meaning, just empty symbols.

“In Searle's usage, syntax is not "how utterances are structured". He is using the term in a broader sense to describe all formal systems such as the representations in The Chinese Room.”
There really isn’t a difference there; I meant the term utterance to mean any sequence of symbols, encoded in any medium, and organised according to formal rules.
“These are not utterances but states of an algorithm with causal relationships to other states.”
Ah, but a state of an algorithm has not causal relationship to other states. It is a symbolic and abstracted description of one of a series of physical states in the machine that implements it. It’s the physical process of that machine that possesses ALL of the causal powers. The algorithm is just a symbolic description of the series of states that we set out to achieve in the machine. Indeed, if you don’t accept that you are contradicting a central proposition of the computational theory of mind; that the underlying hardware is completely irrelevant, and that mind is a program that can run on any hardware. The algorithm is like the marks on a clock face – it doesn’t drive the machine forward, it just points out, in an observer relative, way, dependent on the existence of mind outside itself for meaning. But it’s the hardware that does it. Remember that some early computers didn’t even possess a programming language, even at the level of machine code. They were programmed by plugging in the leads into sockets something like an old fashion telephone exchange. This shows how the programs are just the notes we write for yourself about how to “rewire” the machine, in a more convenient form, certainly, but they are still just another form of human language, just “utterance” in that sense. And as such, they can no more have “causal powers in themselves than a sentence that I utter.
And of course we talk about words having power, we say the pen is mightier than the sword and so on. But when you trace carefully what’s happened, the words are just more carriers of ascribed intentionality form one (set of) brains to another. It’s the brains, the minds that have the causal powers.
“What I am discussing is how intentionality can arise from mechanism or computation.”
Of course it can! Searle says loudly and repeatedly that your statement here is true – but it isn’t the point at issue. By what type of mechanism can do it? I’m sure we would agree that the brain is a mechanism, it gives rise to intentionality. QED. And you mention later the point that “computation” doesn’t name something in the physical world, but any reasonably complex phenomenon can be described as computational.
That’s not the question – we are discussing whether syntax, in itself, without semantics, can give us minds. I’d say that you try to show that you could move directly from syntax to semantics (a pretty desperate move, I’d say) you tacitly acknowledge the importance of this point.
“No they aren't! This implies a very naive interpretation of what the book of rules would need to be like…
…are of course trivial compared to the complexity of what the brain can represent and process, but to me at least they help to visualise how this can be.”

Reply
Replies

Disagreeable Me25 August 2014 at 16:06
I don't see a contradiction in what I said. I note that there is a similarity between your definition of syntax and semantics and that which Searle appears to be using, but I also note that there seems to be a difference, because no computationalist thinks of mental representations as sequences of symbols. As mentioned in my last comment, they are many-dimensional and dynamic. There seems to be a deep-seated confusion here between the idea of computer programs as sequences of formal statements and computer programs as running processes with causal power. Computer code itself, with its own syntax and ascribed semantics, is only a representation of an information processing algorithm and should not be confused with it.

This confusion is again evident in your account of the causal powers and the roles of the physical system and algorithm. By insisting that it is not the algorithm that has causal powers but the physical system, you seem to me to be saying "It is not the elephant's mass that crushed the mouse, it is the elephant. The elephant's mass is just an observer relative number that could be described as kilograms or pounds". In other words, you are confusing the algorithm itself from the code used to represent the algorithm in some particular computer language.

The algorithm is most certainly not just a symbolic description of a series of states of the computer process. The algorithm is more like a structural property of the process. The algorithm is a pattern instantiated by the process, and any process that has that pattern will (in my view) have the same intentionality, just as any massive object can crush a mouse. The algorithm is therefore as crucial to consciousness as mass is to crushing.

Computer code may be a way of conveniently capturing codes about how to rewire a computer, but computer code is not an algorithm. Even a microchip implements algorithms. Here, there need be no code, no software, no symbols at all, only raw electronics doing what physics dictates. But there is still an algorithm, because an algorithm is not code but a procedure for doing something. Euclid's algorithm existed before any computer language, and it is the same algorithm whether expressed in ancient greek, English, Java or C++. This is why talk of utterances is so misguided.


Reply

Inti22 August 2014 at 16:04
I do realise that a real “brain program would have to be very complex, as I’m sure Searle is. But I’ve never understood that point of the many “too slow, too simple” objections to Searle. However many layers you add, it seems to me that the fundamental conceptual issues remain the same.

I’m still a bit puzzled about how the point that Searle might infer some simple rules moves your argument forward. If I grant it for the sake of argument, so what? He can consciously infer a few relationships from observing regularities. How does that affect the argument? Searle and both know both people and programs can do this. Remember, he first came up with the Chinese room in response to Schrank et al’s system that could answer questions about stories, inferring points that were not explicitly stated. Another example is theorem proving software. That was exactly the point of the Chinese Room; that to be able to that kind of inference is no guarantee of intentionality or consciousness.
“Searle seems to think that just because he is not aware of this physical, mechanistic information processing means that it is not happening at all, or perhaps that the effortless intuitive ease with which we do many routine mental tasks cannot be reconciled with the laborious information processing it seems to be when we understand it in detail.”
No, that simply isn’t what he says. He understands very well that, however the brain makes mind happen, there must be processes involved which are not accessible to the person’s consciousness. He simply sets out to show that that process can’t be computational, in the commonplace sense, for the reasons I’ve mentioned before. That plays no part in his argument. In a sense, computationalists are like the drunk in the joke, looking for his lost keys over here under the street light, even though he dropped them over there; “because the light’s better over here”. The computational theory of mind seemed like a marvellous breakthrough, because it offers a naturalistic solution to the mind-body problem. But not only is it a false friend, leading to a degenerating research program in Lakotos’ sense, but it isn’t even necessary.

You often hear computationalists argue that only way we can solve the problem of other minds is by examining the behaviour of other people, or even animals. Searle say no, as well as behaviour, we know quite a lot about how their nervous systems work, and how they evolved, which give him as reasons as good as we are likely to get to suspect that neighbour or even his dog is also conscious work, What’s more computationalist explanations are superfluous.
If a full, naturalistic description of how consciousness arises becomes available, through neuroscience and related disciplines, then a computational description over the top of that is unnecessary and overdetermined. (And if such a naturalistic biological description is never available, we won’t be able to use what we learn from that to validate the claims to consciousness of any supposedly conscious AI.)

Reply
Replies

Disagreeable Me25 August 2014 at 16:27
Searle acknowledges that an AI algorithm would be very complex, but then he goes and uses unrealistically simplistic examples to supposedly show how computers are not like people -- but in my view the differences he illustrates are usually an artefact of the simplicity. They are driving intuitions in the wrong direction. It's like a creationist arguing against evolution by saying that his grandfather was no more a chimpanzee than he is. It demonstrates a failure to imagine how an intuitive understanding of simple, familiar cases may not extrapolate to very extreme ones. And this is the problem, because Searle never manages to demonstrate any fundamental objections to computationalism: ultimately all his arguments bottom out at the fact that computer consciousness is unintuitive.

The point about simple inferences being possible from purely symbolic processing was to establish the point that there is no insuperable gulf between symbolic processing and meaning. If simple inferences are possible, then sophisticated inferences may be possible, and if this is the case then it seems to me that meaning is possible, and this is perhaps most clearly the case where there are causal links to objects to the external world. Even in cases where the objects are abstract, as in theorem proving software, I would not agree that there is no intentionality. Indeed I would think it reasonable to describe theorem-proving software as being intentional, so it cannot be used as an uncontroversially self-evident example of an unintentional computer system.

As with simplicity, though Searle acknowledges that there are processes which are not accessible to consciousness, he often seems to forget this when illustrating his intuitions. It doesn't feel to him like he is thinking algorithmically, but this doesn't at all mean that algorithms are not ultimately responsible for his thoughts.

For me, the appeal of computationalism is not that it offers a naturalistic solution to the mind-body problem, but that it simply has to be correct. Given how I perceive the issues, it is simply unthinkable that it is wrong. Searle's biological naturalism is incoherent in my view.

Reply

Inti22 August 2014 at 16:04
So why is has computationalism been so popular? You mention materialism as another tenable computational position (of course computationalism is only the most popular subset of materialist positions). Well, computationalism seems to solve problems that materialism poses for itself, within a pleasingly materialistic framework.
In the first two chapters of The Rediscovery of the Mind, Searle shows why materialism (in this sense) isn’t tenable. He calls the second chapter “The recent history of materialism: the same mistake over and over”. . (I must emphasis that Searle is using the term materialism in a strict and limited sense, not as a synonym for naturalism, anti-dualism etc, as it’s sometimes used. I can tell you know that, DM, but sometimes it’s misunderstood.).
He sets out the propositions which make the materialist tradition. I won’t try to go over all of these mistakes here, but the critical ones are “The only things that exist are ultimately physical, as the physical is traditionally conceived, that is, as opposed to the mental” and (my paraphrase for brevity) reality is objective and so we must study the mind only through objective or third person points of view.
This is wrong because not all of reality is objective; some of it is subjective. There’s a common confusion between epistemic objectivity, which scientists amongst others rightly seek, avoiding the use personal prejudices, emotional a biases, opinions and limited perspective, for example, and the ontological subjectivity of things which actually exist but are subjective by their nature. The prime example of this is consciousness; the others derive from consciousness through culture and ascribed intentionality; for instance, money is subjective when it comes down to it. And the mental is not opposed to the physical, it is part of it.
So not only are we entitled to take into account the irreducibly first person nature of consciousness, if we don’t, we will certainly go astray. And that’s why Searle emphasises that we have to consider the question of consciousness in a way that takes account of its first-person nature, rather than trying to explain it away as materialist explanations do. It’s not that he appeals to intuitions, as so many claim; it’s this hard-headed argument.

I’ll have to think a bit more about genetic algorithms. Fromm what I know about them, they are not as “hands-off” or independent of human intervention as you suggest, but I’ll have to leave that for another time.
I’m sorry, I don’t feel I can take on your mathematical Platonism. I don’t know what I think about that view, but I’m pretty sure that you are stretching beyond the breaking point here. I remember Massimo Pigliucci saying to you in the Scientia Salon comments “You do realise that are practically an idealist here, don’t you?” and I suspect this was what he was talking about. Still, maybe you are happy with that. Anyway, it seems too left-field to be useful here, I’m afraid.

Reply
Replies

Disagreeable Me25 August 2014 at 16:36
Well, I'm not a materialist, so at least we can agree on that.

Genetic algorithms can be as hands off as you like. We don't need to find examples (although they do exist). We can use thought experiments which while practically infeasible make the point of principle crystal clear. We can for example imagine a physical simulation of a whole universe in which beings such as us happen to evolve. The only intentionality ascribed to such a system is the symbols used to represent quarks, photons, etc, and yet beings with mental representations just as sophisticated (and just as apparently intentional) as our own can evolve. They can represent concepts such as love, loyalty, mathematical abstractions and so on, as well as referring to other objects within the simulation, objects that were not conceived of by the programmers but that emerged as the physical simulation developed.

With regard to Platonism, I'm not sure where I'm stretching beyond the breaking point. What is it I'm stretching? The definition of Platonism? Computationalism? What?

Reply

Inti22 August 2014 at 16:05
I know what you mean about preferring lectures; I first heard about the Chinese room on a program on BBC Radio 3, when I was just a simple naive computationalist myself ;-) Later I listened to Searle’s lectures on Philosophy of Mind from The Teaching Company. There are his Reith lectures (quite old now, but a decent abbreviated version, and free on the BBC site). But I find that his books and papers provide a lot more detail, and responses to many objections that won’t fit in a lecture format. I wasn’t nagging you to read them, but I mentioned them for anyone reading this, who might not know much more than cut down versions of the Chinese Room arguments. I would certainly recommend at least reading Critique of Cognitive Reason and Why I am Not a Property Dualist, and if possible the book The Rediscovery of the Mind; perhaps also The Mystery of Consciousness, though some of the contents of that is available for free online, for instance the debate with Dennett is on the New York Review of Books site, along with Searle vs. the Churchlands.
By the way, the Searle position doesn’t precude there being some processes in the brain that could be best described in computational terms. For example, he did write somewhere, I forget where, about the imaginary case where the function of parts of a persons’ neurons, say the dendrites, start to fail, and are replaced by a computational device within the neuron. This case wouldn’t be invalidated by his arguments, and he believes that the person would remain conscious, nor would it undermine his position, assuming that it is the interaction of the network of neurons that gives rise to consciousness. The “computerised” element would be wholly contained within the black box of the neoron, and to the rest of the system, the inputs and outputs would be the same, so overall, the conscious system would remain non-computational. (Of course, we know too little about the brain to be dogmatic about the precise interactions that lead to consciousness)
So my position would not preclude some computational elements, It only denies that computation is sufficient for consciousness, not that it plays no part at all.

Reply
Replies

Disagreeable Me25 August 2014 at 16:42
I've listened to the Reith lectures and many others, and I'm always aggravated. I would love to be able to have a discussion with him. Not that I would get anywhere, but the points he makes are often so obviously wrong (from my perspective) that's hard to endure him with equanimity when I don't have a chance to answer him back.

Which is why it's nice to have such a chat with a Searle apologist!

On the neurons and the black box, what if instead of replacing one neuron we replace two neighbouring neurons, keeping the external interface the same and accurately simulating the connections between the two?

What if we replace three? Or 400, or four million?

At what point do we lose concsciousness? Would we notice? It would seem that we would not, for if all inputs and outputs are kept the same, all external behaviour must be kept the same time the whole time. If we could imagine some Star Trek-era teleporter technology gradually teleporting neurons out and teleporting ever-larger black boxes in, it would seem that you could not possibly notice anything was happening because if you did you would be able to report it, which would constitute a change in outputs. Far from bolstering his case, considering such thought experiments only shows how it makes no sense.


Inti25 August 2014 at 22:35
The case I’ve set out wouldn’t change even if every neuron was altered internally, in any way you like, because each one remains a black box, and how it functions internal makes no difference to the function of the brain at an inter-neuronal level, provided that all the inputs and outputs remain the same. This is assuming for the sake of argument that the processes that lead to consciousness operate at the level of interaction between assemblages of neurons, and/or of larger assemblies made up of neurons. So it isn’t a process where changes within the neurons have any additive or multiplying effect.After all, how could the rest of the brain, outside the neurons, even “know” that anything had changed within?

This is really not a defense of Seale’s arguments as such. It simply shows how there could be strongly computational elements (elements where computation is unambiguously the best description,) within parts of the brain, without the system as a whole being computational at all. Off the cuff, here’s an anology. When I go into a shop, a computational device runs the automatic door opener. That doesn’t make the shop in any sense a computational device, because the computation is “sealed off” within one small part of the system. I’m not suggesting that neurons really play such a tiny part in consciousness, of course, just illustrating the principle.


Disagreeable Me29 August 2014 at 21:34
You didn't answer the question I asked, Inti, which is why must we always draw the borders of the black box around the periphery of single neurons? Could we not also draw the borders around pairs of neurons? It seems to me that we could take any two adjacent neurons and replace the two simultaneously by a single device that simulates any behaviour between the two of them and perfectly emulates their combined interface to the rest of the brain. Now extend that logic outwards and where does it lead you?


Inti30 August 2014 at 11:41
This comment has been removed by the author.


Inti30 August 2014 at 11:43
I’m going to try to answer this without addressing your main “systems reply” based position, but agreeing to differ on that as far as possible.

I see your line of thought; though there are two interpretations of it.

1. if we could replace ALL the neurons with computational substitutes, you might believe that we would have a completely computational device, and if one neuron would act as a black box, then they all would, in which case there is no reason to assume that the brain would lose consciousness. So the brain would be completely composed of computational units, and so would be one itself.

2. The second variant is a bit like Dennett’s view described from bottom-up rather than top-down; we could keep on replacing larger assemblies of neurons with single unit computational black boxes with the same function, until we have one unit, the whole brain. (I think this is your point).
QED for computationalism;? Or perhaps not.
Version 1 would miss the point that the two levels, inside and outside the neurons, are sealed off from on another, and how one works has no implication for the other.

My scenario is based on the “for the sake of argument” assumption that conscious arises ultimately from the interaction of neurons in a neural net, and that such interaction is a sufficient cause for consciousness. (In fact we don’t know that for sure; the neural net may be necessary but not sufficient, and it’s even conceivable, however unlikely, that Roger Penrose is right and neurons are not the critical components at all.)

If that is the case, I’d contend for other reasons we’ve discussed previously, that this neural network is not in any strong or fundamental sense a computational device, in the usual sense of the word.

(Again, it could be described metaphorically in computational terms. It could be simulated computationally, but that leaves open the central question as to whether this would be a true reproduction.)

We can continue to view this as a physical, neurobiological system that operates in a qualitatively different manner to a computer. And this is perfectly consistent with the possibility that the neurons become internally computational, provided the input and output behaviour does not change. In this case, the only conceivable evidence of change within neurons would be a variation in input or output, and by stipulation that doesn’t occur, so the other parts of the brain cannot “see” into any one or more neurons.

To put it another way, if neurons are completely multiply realisable, it doesn’t matter how they work internally for the working of the brain at a neural net level. They could be driven by steam for all we know or care. (The same argument applies at a deeper level ,; imagine that we gain a better and somewhat altered understanding of quantum-level processes in matter. Unless that implied different behaviour of this matter, it would be irrelevant to the behaviour or organisational nature of any system at a more gross level) So version 1 provides no new support for computationalism.

And Version 2? This would only work if we presume that computationalism is true and my stipulated biological version is false. Version 2 describes the changes as crossing into and breaking up/ altering the process at the network level, which I stipulated was the level which gave rise to consciousness. Obviously you could change that stipulation, in which case we are no longer discussing my original setup, and you have simply reasserted to truth of computationalism, not demonstrated it.


Disagreeable Me30 August 2014 at 17:45
Hi Inti,

Version 2 is indeed the scenario I am trying to discuss. I may seem to be begging the question, but the same logic which forces you to accept version 1 (if the interface is the same then nothing has changed) would seem also to apply to version 2. If you reject this logic, as you might, then I don't see why you should be so confident that version 1 would be conscious. You're assuming it's physical connections between neurons that give rise to consciousness, but you have no reason I can see for preferring this assumption to the hypothesis that neurons need to be entirely biological.

I agree that consciousness arises ultimately from interactions between neurons in a neural net. I just think it is very naive to think it matters whether the connections between neurons are implemented in software or in hardware. From a computer science perspective, software and hardware are equivalent, the only difference being in terms of performance. Furthermore, it doesn't matter if there is one computational device or billions. There is no information processing task that many computers working in parallel can do which cannot be performed by a single computer working step by step.

But, you may say, this assumes that consciousness is an information processing task. True. But even if we assume that something beyond information processing happens when we have some large number of interconnected computing devices, we have a problem. It's not always clear whether we should say two devices are distinct or two parts of the same whole. An individual computer has many data buses, transmitting information from one register to another. Does that mean that each register is a separate interconnected device or is it one whole, one computer? Is the brain one device or billions? The computationalist answer is that it doesn't matter -- drawing distinctions between devices is arbitrary and can be of no consequence.

Anyway, you didn't fully answer my initial question. Your position would appear to be that a wholesale replacement of your neurons by a black box would render you a philosophical zombie. Fine. So what if we just replace one hemisphere? Or one lobe? What if the process is very gradual, taking over your biological brain bit by bit? What would that feel like?

This example to me illustrates the absurdity of Searle's position. By the terms of the thought experiment, you could not possibly notice anything, because noticing would involve a change in behaviour which we stipulated does not happen. This means that it is possible for you to gradually change into a philosophical zombie over the course of an hour or so without noticing that anything is happening. It also means we have to consider entities that are somehow half conscious though they appear normal and report feeling normal. If that doesn't give you pause I don't know what will, for both ideas seem profoundly absurd to me and prompt me to reject the distinction between the conscious states of a normal personal and a presumed philosophical zombie.


Inti31 August 2014 at 22:00
This comment has been removed by the author.


Inti31 August 2014 at 22:03
Starting with your question in paragraph four; “What would it feel like?” Searle seems to assume a slow decline, but not to believe that much depends on the question.. I would say we can’t tell; we don’t know enough. It might be gradual, like falling asleep or being anaesthetised. It might be a sudden lights’ out when some critical brain area was altered. It might involve loss of sight or sound, or the capacity to move, before the loss of consciousness. We can’t say, at present. (See my postebelow for an interesting sidelight on this, the “on-off switch article from New Scientist.)
“Your position would appear to be that a wholesale replacement of your neurons by a black box would render you a philosophical zombie”. Not exactly. I think it’s just possible that we could replace every neuron by a computational black box substitute, without affecting consciousness. Doesn’t that imply computationalism? Not at all, provided that it is the noncomputational interaction of the neurons that gives rise to consciousness. If so, how things happen inside the black box neurons is irrelevant; it doesn’t alter the noncomputational processes in any way!

But suppose we expand the boundaries of the elements that are replaced with computational substitutes? Well, perhaps on a very small scale the effect might be negligible. And it might be that there are some other elements that we could treat as black boxes, too. However, you can’t propose to extend the black box to encompass major parts of the brain, because if you do, you are removing –knocking out -the areas where the biologically causal non-computational interactions are taking place, and substituting non-causal computational elements instead. You were not doing that in the case of single neurons, because the critical causal interactions aren’t taking place within the neurons, but outside in the interactions amongst them. So” siliconising” outside the neurons would break the critical non-computational network. And you can’t just claim that the extra-neuron changes would still work in place of that network; this is precisely what is at issue; that claim would simply reassert computationalism’s basic claim in different words.
In the first paragraph you say; “You're assuming it's physical connections between neurons that give rise to consciousness, but you have no reason I can see for preferring this assumption to the hypothesis that neurons need to be entirely biological.”
To a large extent that’s an unresolved empirical question about how the brain works. I just made that assumption for the sake of argument. It does seem quite likely at first sight; we have good reasons to believe that electrochemical signalling between neurons in a complex network is at least a major part of the answer.
Agreed, we can’t be sure that there aren’t other hidden interactions involved, between different neurons and/or between neurons and other brain elements. If that was the case, the neurons wouldn’t be black boxes after all, and the premises I’ve stipulated for the scenario change-, and my argument no longer applied.
But then, I only claimed that it would apply IF the black box conditions held. Of course, we might go on to find that there are other sub-elements of the brain that really are black boxes, which would reinstate the same situation in a slightly different form. Or perhaps not, perhaps all elements are so intertwined that no black box is possible. In which case, that simply implies that there could not be any computational substitution anywhere without breaking the conscious system.


Inti31 August 2014 at 22:04
In case this is still unclear, let me put it a different way., My basic point is that, (a) if you can enclose a certain part of the brain that is not critical to the physical neurobiological process that produce consciousness, then you can make any changes you like within that element, provided (b) they maintain the same pattern of interactions with the outside of the enclosed part. On the other hand, this will not work under two conditions;

1. The altered black box does not actually maintain the same interactions, so it doesn’t meet the condition (b) – that seems to be what you are suggesting at the end of your first paragraph (quoted above);
2. If the enclosed part of the brain grows so large that the encroach into the parts where the core complex of physical neurobiological process are operating, in which case condition (a) above doesn’t apply.
Conclusion: if conditions (a) and (b) do apply then it is possible to have islands of computational functioning within a larger non-computational brain. That’s all I set out to show.
It doesn’t strengthen or weak computationalism at all, and so my assuming that computationalism is false in this black-box argument is not question-begging, since computation’s truth isn’t at issue here.
All it might show is that if, hypothetically we found some indisputably, intrinsically computational (whatever that would mean!) sub-component in the brain, we would still need to prove that it’s function impinged on the core processes that produce conscious, and are not either secondary elements (“peripherals”?) or units sealed within a black box.

And once more, if we violate (a) then we are just describing a fundamentally computational conscious brain, and thus returning to the main point at issues, leaving this black box discussion behind.

I know I haven’t addressed some core issues in your previous posts, but I’m going to try to pull them together in one reply – when time permits.


Inti31 August 2014 at 22:05
Here’s an edited version of an interesting report in New Scientist

http://www.newscientist.com/article/mg22329762.700-consciousness-onoff-switch-discovered-deep-in-brain.html?full=true (subscription needed for full text, but I can send it to you if you like.)

Consciousness on-off switch discovered deep in brain

 02 July 2014 by Helen Thomson, Magazine issue 2976

...most agree that consciousness has to involve the integration of activity from several brain networks, allowing us to perceive our surroundings as one single unifying experience rather than isolated sensory perceptions.

One proponent of this idea was Francis Crick, a pioneering neuroscientist who earlier in his career had identified the structure of DNA. Just days before he died in July 2004, Crick was working on a paper that suggested our consciousness needs something akin to an orchestra conductor to bind all of our different external and internal perceptions together.

With his colleague Christof Koch, at the Allen Institute for Brain Science in Seattle, he hypothesised that this conductor would need to rapidly integrate information across distinct regions of the brain and bind together information arriving at different times

...

The pair suggested that the claustrum – a thin, sheet-like structure that lies hidden deep inside the brain – is perfectly suited to this job (Philosophical Transactions of The Royal Society B, doi.org/djjw5m).

...

Anil Seth, who studies consciousness at the University of Sussex, UK, warns that we have to be cautious when interpreting behaviour from a single case study. The woman was missing part of her hippocampus, which was removed to treat her epilepsy, so she doesn't represent a "normal" brain, he says.

However, he points out that the interesting thing about this study is that the person was still awake. "Normally when we look at conscious states we are looking at awake versus sleep, or coma versus vegetative state, or anaesthesia." Most of these involve changes of wakefulness as well as consciousness but not this time, says Seth. "So even though it's a single case study, it's potentially quite informative about what's happening when you selectively modulate consciousness alone."

This seems to be an example of a thought experiment becoming a real experiment. In Rediscovery of the Mind, Ch3 pp 74-5, Searle wrote: Suppose we had about such the possibility that neurophysiology might find a precise, neurophysiological isolable cause of consciousness in human beings, such that the presence of the relevant neurophysiological phenomenon was both necessary and sufficient for consciousness. He went on to discuss how this could be used to settle questions about the presence of consciousness in other animals at different phylogenetic levels.

I’d point to this as an example of a more promising way forward in understanding consciousness.


Inti31 August 2014 at 22:06
A couple more relevant quotations from Searle:

“Given a choice between the facts as we know them—consciousness exists, it is caused by neuronal processes, it exists in the brain, and it has causal functions in the life of the organism—and various philosophical theories, I will take the facts any time. Furthermore, I am confident that in the long run, the facts will prevail over the theories which will come to seem more and more obsolete. It is worth pointing out that practicing neurobiologists of my acquaintance, such as Francis Crick, Gerald Edelman and Cristof Koch, implicitly or explicitly accept a version of what I have been calling biological naturalism. They look to the operations of the brain to find an explanation of consciousness. It will probably take a long time before Biological Naturalism is generally accepted by the academic profession because we follow a long tradition of teaching our students the mistaken view that this is a philosophical problem of impossible difficulty.”
J Searle, Biological Naturalism, 2004,
http://socrates.berkeley.edu/~jsearle/BiologicalNaturalismOct04.doc

If nothing else, this shows that Searle’s view is not a way-out peripheral approach, or on that is vague, ill-defined or sterile.


Inti31 August 2014 at 22:10
Now from Faigenbaum, Gustavo (2005-07-09). Conversations with John Searle (sorry about the odd fragmented references, Kindle won’t let me copy across page breaks)

G. F.: How do you see the field of cognitive science today? Where is it going? What’s your assessment?

John Searle:

As you know, originally, cognitive science was founded on a mistake. It was founded on the mistake of supposing that the brain is a digital computer and the mind is a computer program. I’ve never believed that. The view that mental states could be defined in terms of their causal relations to input-stimuli, to other mental states, and to output-behavior, was an extension of functionalism in the philosophy of mind. I think that now the computational model of the mind in cognitive science is gradually fading out, and is being replaced by something I welcome, namely cognitive neuroscience. I think we’re now beginning, with the invention of fMRI (functional magnetic resonance imaging) techniques and other techniques, to get an understanding of the brain and how it functions in cognition, which we’ve never been able to understand before. So, I think cognitive science is more exciting than ever, but that’s because it failed. The original project failed. The project of a computational science of cognition was bound to fail. But we are making progress with brain science. (Kindle Locations 774-785)

G. F.: Is there any positive content left to be rescued from that failure?

John Searle:

Sure. First of all, cognitive science overcame “behaviorism,” which had been the orthodoxy in psychology. “And the science of psychology on the behaviorist model was you were going to correlate these stimulus inputs with the behavioral outputs....the other view I call weak A.I. That is the view that you can use the computer to study the mind as you can use it to study anything. And we have a lot of useful computational studies of cognitive processes. You can do useful computational simulations and models of cognitive processes as you can do computational (Kindle Locations 786-804).)



Inti31 August 2014 at 22:10
G. F.: If you are right about the importance of these breakthroughs in neurophysiology, and we eventually come to know how the brain really works, does that mean that scholars who work on the philosophy of mind will become unemployed?

John Searle:

In fact , changes that are now going on in cognitive science make the philosophy of mind more interesting. Cognitive science was founded on the mistake that the brain is a digital computer and the mind is a computer program. It’s not the first time in history that a science was founded on a mistake. Chemistry was founded on alchemy, and cultural anthropology was founded on the false idea that people are infinitely malleable and variable. I don’t think that this mistake is fatal, but I do think that we are now moving into a new paradigm, away from computational cognitive science, to cognitive neuroscience. And I welcome that. Now we are investigating how the brain functions. And as we find out more about how the brain functions, we’re going to have a much more rigorous cognitive science. That seems to me not to put an end to philosophical discussion, but to create all sorts of philosophical problems that we didn’t have before. For example, I’ve just published an article in the Annual Review of Neuroscience, where I claim that a lot of the neuroscientific research is based on a false premise. It’s based on the premise that in order to understand how the brain causes consciousness, we should try to find out how it causes a single building-block of consciousness, such as the experience of red or a certain kind of sound. And I argue that the subject can only experience red if he or she is already conscious. He can only hear a sound if he’s already conscious. So I oppose what I call a building-block theory to the unified field theory. And I make a claim that the unified field of consciousness is what we should be investigating. And to investigate that, we have to investigate what’s the difference between the conscious brain and the unconscious brain. (Kindle Locations 809-823).

Reply

Inti22 August 2014 at 16:05
In the light of that, I was interested to read the Introduction to Jerry Fodor’s The Mind Doesn’t Work That Way: The Scope and Limits of Computational Psychology,.MIT Press, 2001. Fodor is generally seen as a paragon of computationalism, but he writes;
“Over the years, I've written a number of books in praise of the Computational Theory of Mind (CTM often hereinafter). It is, in my view, by far the best theory of cognition that we've got; indeed, the only one we've got that's worth the bother of a serious discussion. There are facts about the mind that it accounts for and that we would be utterly at a loss to explain without it; and its central idea-that intentional processes are syntactic operations defined on mental representations-is strikingly elegant. There is, in short, every reason to suppose that the Computational Theory is part of the truth about cognition.' But it hadn't occurred to me that anyone could think that it's a very large part of the truth; still less that it's within miles of being the whole story about how the mind works. (Practitioners of artificial intelligence have sometimes said things that suggest they harbor such convictions. But, even by its own account, AI was generally supposed to be about engineering, not about science; and certainly not about philosophy.) So, then, when I wrote books about what a fine thing CTM is, I generally made it a point to include a section saying that I don't suppose that it could comprise more than a fragment of a full and satisfactory cognitive psychology; and that the most interesting-certainly the hardest-problems about thinking are unlikely to be much illuminated by any kind of computational theory we are now able to imagine. I guess I sort of took it for granted that even us ardent admirers of computational psychology were more or less agreed on that.”
I’ll be interested to read his argument in full when I have time.

Now to look at your latest comment under “Strong AI: The Story So Far”. I’ll be back…

Reply
Replies

Disagreeable Me25 August 2014 at 16:48
I can sympathise with this to an extent. If I convince you that computationalism is correct (of course I won't -- nobody ever changes their minds in discussions like these!), then I won't have solved the problem of mind for you. Computationalism is just an attitude about the fundamental metaphysics of mind. It doesn't tell you how exactly concepts are represented and it doesn't tell you exactly how the brain processes information. Indeed, generic computationalism doesn't even approximately answer these questions (though subcategories of computationalism such as connectionism may be more specific). There are a huge number of empirical questions to be worked out, but those are largely the domain of psychology, neurology and computer science. The whole story of how the mind works will involve a greater understanding in all these areas, but the strictly philosophical problem seems to me to be largely answered by computationalism.

Reply

Disagreeable Me25 August 2014 at 21:59
Hi Inti.

"It’s striking that each of us thinks the other is begging the question."

I think that's a very common situation when discussing this particular issue. I agree with you that we are operating with very different intuitions and presuppositions.

On the particular charge that I'm begging the question when I assume that qualia can be embedded in a concept map, I have to agree with you. I got ahead of myself. That is of course not at all persuasive to a non-computationalist. I was just describing how I see it without justifying my description. I can still make my point however using a less controversial statement. The issue was whether mental representations are like utterances, and I can still show that they may be quite unlike utterances by analogy to other forms of representation, such as relational databases etc, where we don't only have sequences of symbols but we have causal associations between them. The representation is a dynamic structure, and the shape of this structure imposes a kind of order absent in mere utterances. There is more than syntax at play. It is more like a mechanism than like words on a page. That said, please don't take from this that I think of mental representation as a relational database. It is much more complex, dynamic and sophisticated than this, while also being less robust.

"including its critical failure to understand that all computational representations must be observer relative ... all representations are symbolic statements TO a conscious representational mind."

So certain neural firing patterns in your brain are symbolic statements TO the conscious representational mind that is you and certain patterns of execution in an electronic artificial intelligence are symbolic statements TO a conscious representational mind that is an electronic artificial intelligence. I'm not failing to understand this at all. The problem is I think an AI can be conscious and you think it cannot. Both views are consistent as far as your statement goes.

"Great, I agree with almost all of that description, but there is no necessary computational element in your description; it’s a biological, neurological process, through and through."

In my view that's missing the point. Firstly, almost any process can be construed as computation, which Searle often gleefully points out. So any process at all is computational from a certain point of view, including this one. What is in question is whether it is useful to see it as computation, which is usually determined by whether the phenomenon will be reproduced if we implement it on another substrate. Photosynthesis is not, because simulated photosynthesis does not produce glucose. But what this example was discussing is the chain of cause and effect that links my mental representation of a car with an actual car, and the kind of thing I described can be reproduced in computers and robots, lending weight to the view that computer representations can actually have intrinsic meaning in the same way that neurological representations do. Where you say there is no need for computation, I say there is no need for biology.


Reply
Replies

Disagreeable Me25 August 2014 at 22:34
"The point is that nothing that is physically causal can ever be reproduced in computers, by any program no matter how sophisticated."

Put it this way. If I see a car, I can say "I see a car". My seeing of the car causes me to say "I see a car" via a circuitous physical chain of cause and effect. But a computer can do just the same thing. A computer, connected to a camera and a speaker, can see a car and say "I see a car" via a circuitous chain of cause and effect. Assuming this computer has some sophisticated image processing and identification software to match images to classes of object, that computer has a representation of some sort of the concept "car". My point is, since this representation is causally connected to physical cars in just the same way as a mental representation, it is plausible to say that it intrinsically means "car". Furthermore, that representation was not necessarily put there by a human programmer. The system could have been trained to represent cars as a novel concept. It could easily be that no human would be able to look at its internals and figure out which bits represent which concepts (depending of course on how it is designed).

"It is the representational structure that makes it computational, and representation is never a physical phenomenon."

There is cause and effect at both hardware and software levels. These are two complementary ways of describing the same thing. The hardware level describes the physical chain of cause and effect that you're talking about, but the software describes the pattern of this sequence. Substrate independence or multiple realisability means that any physical system with such a pattern of cause and effect (including cogwheels or whatever) will be conscious in the same way. I don't see this as coming back to bite computationalism. Far from it.

"To put it very crudely, causal brain processes actually push matter about, computation doesn’t, ever."

Of course it does! Computers push electrons about. Cogwheels turn. Pipes push water about. Computation can be seen as a level of description above the physical. Every part of the computation has both an algorithmic and a physical cause, and there is no conflict here because they are perfectly complementary, like describing the decision-making of a person as a product of atoms bumping, neurons firing, or a desire to eat cheesecake. The question is whether consciousness depends on the particular substances pushing matter about or on a higher level account of the structure of the chain of cause and effect. The claim that computation doesn't push matter about is strange because most computationalists do indeed imagine that matter must be pushed about to achieve consciousness.

"Associations, in your sense, are representations"

Well, no, not really. They are more than this. They are causal links.

"At best, representations can describe or depict processes."

If I have a light and a button, where pressing the button toggles the state of the light, then the two are associated and I can even describe this association as an algorithm. This association is different from that between a switch and a light, where the state of the switch is associated with a state of the light.

But the association between a toggle button and a fan is the much the same as the association between a toggle button and a light, so it is substrate independent. We can abstract away the structure of this association from a particular physical realisation and say that "toggle-control" is instantiated wherever such a causal association is implemented. Computationalism is just the view that consciousness is like "toggle-control" in this respect. It is instantiated wherever certain patterns of cause and effect are implemented, be they in brains or computers.


Disagreeable Me26 August 2014 at 00:25
"It’s the underlying machine substrate that has even that power, and the program instruction only denotes that. Any error in this is all mine, not Searle’s, though"

OK, well I won't blame Searle for that then. You're thinking of computer programs in terms of symbols and instructions, but I think of them as patterns of cause and effect. The symbols and instructions of computer code are just a means to the end of instantiating that pattern of cause and effect.

"And the Churchlands, like you, claimed that Searle was looking at the wrong layer, or at the wrong scale, and they too missed the point that it is a qualitative difference, not one of scale or complexity, all the way through."

This would be a fair comment if Searle had conclusively demonstrated a qualitative difference, but he hasn't. The virtual minds reply dismantles the Chinese room argument completely. All that is left to him are intuitions based on failing to imagine the quantitative differences between the simple systems he is capable of imagining and those that we would actually call conscious.

"Only the history of how they came about would provide evidence."

I strongly disagree with this. The genesis of a physical object cannot, except in a practical sense, determine what it is capable of. All that matters is how it is constituted. This idea that computers can be conscious if they evolved but not if they were programmed is very alien to my way of thinking. I understand your argument and Searle's that the intentionality we see in computers is only there because we perceive it to be, but I cannot understand the view that ascribing intentionality somehow prevents the manifestation of intrinsic intentionality.

"I see that you think that philosophical zombies are logically impossible, assuming your strong version of computationalism. I think your are write, given your premise, but your premise is wrong."

That's perfectly fair. So it is my view that philosophical zombies are useful as a way of articulating the difference in intuitions. I have often said to Massimo Pigliucci on Scientia Salon and previously on Rationally Speaking that I regard his rejection of both computationalism and the philosophical zombie argument to be inconsistent. David Chalmers seems to me to be the natural ally of the biological naturalists, though they often don't feel the same way.

Cheers for the discussion, Inti. If all the typing and back and forth is too much I'd be up for a Skype conversation some time. You can contact me via the email form if you like.


Inti27 August 2014 at 13:27
"Only the history of how they came about would provide evidence."

I strongly disagree with this….. the manifestation of intrinsic intentionality.


I’m going to reply just to two points. I want to give the other more time. So I may have to go quiet for a while, but “I’ll be back”.

Maybe Skype later, but at the moment, asynchronous discussion is easier., thanks.


Obviously I wasn’t clear in my SF story. I’m not perpetrating the genetic fallacy. I think it’s quite reasonable to say that there is a crucial difference between a system, of any type, that came about without any intelligent design, on the one hand, and one which can produce the property of appearing to be conscious precisely because it was designed to produce that appearance. Even if a chatbot could perform faultlessly in the Turing test, I’d still be justified in assuming that it was using tricks, which allow it to manipulate text rather than to understand the content of the interchange. In that case, I’d say that it’s legitimate to say that knowledge of how the system came to exist is perfectly relevant

So I’m not proposing that “that computers can be conscious if they evolved but not if they were programmed”. I’m saying that if I knew that it had evolved, I would know that there was no underhand trickery of the Eliza-but-much-more-sophisticated kind. I’d know that nobody was rigging the game. In that case, I would have empirical evidence that Searle is wrong, which was what I was discussing.

I do realise that the kind of AI system you are defending is much more, in your view, than a super-Eliza chatbot.


On ‘Intuitions’: This is one of those irregular verbs, isn’t it: I have rational insights, you have irrational intuitions!

It’s popular with Searle’s opponents to say that he is only invoking intuitions. I don’t believe that’s true, but for now, I’ll just say that the reverse seems to be true for people who think like me; it is computationalism that is underpinned by intuitions and assumptions. I remember when the popular media often called computers “electronic brains”, because they were the first devices that could reproduce the powers of calculating and processing data, the exclusive preserve of human brains up to that point. How natural to assume that this could be extrapolated to every power of the brain. This dovetailed neatly with the behaviourist errors that dominated philosophy of mind and psychology at the time. Later, computationalists often resented themselves as paragons of hard-headed materialist reason as opposed to the “touchy-feely nonsense’ about the intrinsic importance of the first person viewpoint.

I’m not claiming that this is the only basis of computationalism, of course; just that it is one cultural bias that is part of the basis of intuition that makes computationalist perspective seem credible.

Here’s one example of a statement, by Jack Copeland in this case, that I’ve often seen or heard in computationalist attacks on Searle:

“Few accept Searle's objection, but there is little agreement as to exactly what is wrong with it. “

That ‘few accept’ is a biased perspective from within computationalist circles, of course, but I wouldn ’t deny that for many years that perspective dominated, rightly or wrongly.

But doesn’t that sound like a classic case of intuition-based, motivated reasoning: “we know Searle is wrong, now let’s find some arguments that show what we already know”?

http://www.alanturing.net/turing_archive/pages/reference%20articles/what_is_AI/What%20is%20AI14.html

Incidently, when I first heard about the Turing Test, I did feel intuitively that it was wrong, despite being basically a computationalist at that point.. I couldn’t pin it down rationally, though. I was attracted to Searle’s CR room argument precisely because it goes beyond intuition.


Disagreeable Me29 August 2014 at 21:38
I'm not going to deny that computationalists have intuitions too. Indeed there is an essay you may want to read coming out on Scientia Salon next Monday to that effect.

Reply

Disagreeable Me2 September 2014 at 16:54
Hi Inti,

Once again, and please don't lose sight of this, I'm talking about replacing ever-larger numbers of neurons with a single black box device which simulates all replaced neurons while preserving behaviour at the boundary of the device. I am not talking about replacing individual neurons with individual devices. From the premise of the Chinese Room, we can assume it is possible to simulate the behaviour of an entire brain, and so presumably smaller areas of a brain too. The question is not whether this is possible but whether doing so achieves consciousness.

"Searle seems to assume a slow decline"

If that is so, then Searle hasn't understood the question. If we stipulate that the interactions of the black box are identical to those of the replaced brain matter, then it is not possible that there could be any difference in reported experience.

"on a very small scale the effect might be negligible"
"You can’t propose to extend the black box to encompass major parts of the brain, because if you do, you are removing –knocking out -the areas where the biologically causal non-computational interactions are taking place, and substituting non-causal computational elements instead"

Right. So if 2 neurons are replaced only, then we can assume the impact on consciousness is negligible. If a whole hemisphere is replaced, we can assume that the impact on consciousness is severe. Perhaps we have created a philosophical zombie? It would at least behave normally on the outside -- we have stipulated all behaviour at the interface of the black box is the same. But we would have replaced major parts of the brain responsible for consciousness. So does this mean we have created a philosophical zombie or a half-zombie? Is there a point of sharp transition, where we switch from being fully conscious to fully unconscious as the black box grows outwards -- a straw that breaks the camel's back?

I imagine you don't know, and that is to be expected. But this is not a simple empirical question. The larger point is that this transition appears to be meaningless. There is no observer who could see the difference between a sharp transition and a gradual decline, not even the subject of the experiment. It is therefore clear to me that this is a distinction without a difference.

"And you can’t just claim that the extra-neuron changes would still work in place of that network; this is precisely what is at issue; that claim would simply reassert computationalism’s basic claim in different words."

Remember that on the Chinese Room, Searle is conceding the point that we might be able to make a computer system which gives the same inputs and outputs as a biological brain. If that is true for the brain, then I think it is not too controversial to propose that it is true for a portion of the brain. So to argue that it is not possible to make a black box which gives the same inputs and putputs as a continuous region of brain matter is to make an entirely different argument against not only strong AI but weak AI, and one that would seem to need some argument to back it up.

I really think this neuron thought experiment is a vital one, and I do really want to know how you answer it because I want to know how Searle would and you seem to think alike. Unfortunately, you seem so far to have missed the point of it somewhat. Again, I'd suggest Skype at some point as a conversation might be more productive if we continue to talk past each other.

Reply
Replies

Inti3 September 2014 at 14:07
It seems neither of us seems to be making sense of the other’s point.
Mea culpa. I think I see why that is and what’s leading you astray and it’s at least partly my fault, in two ways. First, I hoped we could temporarily set aside some important disagreements like the system reply and the simulation vs reproduction argument, and still discuss this topic
Maybe I made a related mistake in thinking I could use computationally loaded terms like input and output innocently, because I’ d hoped I’d already explained why, in the brain, you have to use terms “input” output” and “information” inside the same quote marks, indicating a useful fiction, like saying that evolution ” wants” to do this or that, or “has goals”. Those are loose shorthand, and if we took them literally, we’d be lead astray, That’s what does does happen with information processing talk in the context of mind and brain. I now see that I invited this misunderstanding.
I hope I’ve made myself clearer, and brought out the misunderstandings here, You may disagree, please go beyond restating that t Searle is wrong and computationalism right; you need to show why my train of thought here is inconsistent. This doesn’t prove Searle’s perspective, it merely sets it out, but it does show why, within that framework, the neuron argument has no implications for Strong AI or Searle’s position.
1. I’ve discussed the neuron as a black box. The point was to show that how it works internally has no implications for or against the computational or the biological naturalist theories. Many different realisations of a neuron might fit into either of these theories.

2. The reason for this is that the way changes in the neuron appear to elements outside it remain the same whatever the internal implementation. I’ve stopped using the terms input and output, because I realise they carry computationalist assumptions that confuse the issue- see below.


3. We already know from neurobiology that some parts of the brain appear to be implicated in consciousness. Let’s call those parts the “brain box” that’s just a conceptual schematic name.

4. You argue that we could expand the black box arbitrarily to enclose any portion of the brain. In itself, that’s probably true. So you mistake my point when you say; “So to argue that it is not possible to make a black box which gives the same inputs and outputs as a continuous region of brain matter is to make an entirely different argument against not only strong AI but weak AI, and one that would seem to need some argument to back it up.” .That’s not what I’ve said.

Reply

Inti3 September 2014 at 14:08
5. Now let’s focus on what happens inside the brain box. Here, biological processes combine to give rise to consciousness. We do not know that because we examine inputs and outputs, but because we are aware of it in our own minds, we have conscious thoughts, emotions, pains, tickles and so on. We are conscious, and it is a first person, subjective experience. We can’t coherently deny that exists, though we may be mistaken about the details; we feel as if we have a continuous visual field, like a movie screen, though we don’t; we may believe that we are in love, then realise that we weren’t. The one thing that we can’t deny or reduce away is the fact that we are conscious.


6. I do not need any “inputs” or “outputs” from the brain box to know that I am conscious. I’m directly aware of it precisely because it is “me”; there is no gap over which communication needs to take place between my “brain box” and me; they are the same thing under different aspects.

7. I will of course need some processes that could be called “inputs and outputs” to allow my senses to work and to move my body. Conceptually, these are interactions with other systems at the boundaries of the brain box. In the precise detail, it’s likely to be less sharply divided, but that’s not the point.


8. Notice the qualitative difference between this brain box setup and that of the neuron’s black box. As far as we know at present, it’s the interaction with other neurons that are central to the neuron’s role In the case of the brain box, it is not interactions with other elements that define it, but the single process with two aspects, first-person and biological machinery, that goes on within it. That’s why expanding the black box doesn’t have the implications you claim.

9. I’m puzzled by your claim that “The larger point is that this transition appears to be meaningless. There is no observer who could see the difference between a sharp transition and a gradual decline, not even the subject of the experiment. It is therefore clear to me that this is a distinction without a difference.” Of course there is an observer; the conscious individual concerned, at least until that consciousness is extinguished. I don’t understand how you can deny that. The whole nature of what we are discussing in this part of the discussion was; what is the first person experience?


10. If we were to replace the working elements of the brain box with computational devices, we would kill its consciousnesss. We can’t be sure how that would seem to the victim, but it follows from biological naturalism.You can of course claim that the processes within the brain box are really computational ; that they are made up of computational elements, representations, inputs and outputs, information. All these are loaded terms, and if Searle is right, they all describe observer–relative features, and so can’t be claimed to give rise to consciousness without a vicious homunculus fallacy. Of course he might be wrong, but that’s not proved by asserting computationalism. They are at best convenient metaphors, like saying that evolution ”wants” to do something; useful as long as we remember that they are only metaphors. Many neurobiologists use seemingly computational language in that way, as a handy shorthand, while frankly admitting that that’s all they think they are. Cognitive scientists mistake the metaphor for descriptions of what actually happens.

Reply
Replies

Inti3 September 2014 at 14:10
11. Computationalism isn’t a very useful way to explain the mind. At first, it seemed to solve all sorts of philosophical difficulties; but Searle has shown that it is not necessary to solve them. Biological naturalism does the same task, more parsimoniously and with more connection to the results of neurobiology. Computer functionalism/computationalism/cognitivism is overdetemined and redundant.
Your error about the nature of “input” and output is the same error that Searle complains of repeatedly, and the reason that computationalists just don’t get in his experience. They think “there must be some information in there!” No; information is a metaphor for the physical patterns themselves. { Improve this] So the biological naturalist approach doesn’t rely on input-output processing. Maybe I made a mistake in thinking I could use computationally loaded terms like input and output innocently, because I’ d hoped I’d already explained why, in the brain, you have to use terms “input” output” and “information” inside the same quote marks, indicating a useful fiction, like saying that evolution ” wants” to do this or that, or “has goals”. Those are loose shorthand, and if we took them literally, we’d be lead astray, That’s what does does happen with information processing talk in the context of mind and brain. I now see that I invited this misunderstanding.
I notice that in Consciousness is not like Photosynthesis you make this point, which is half wrong, half right:
“The reason is simple. The brain is an information processing machine,[Nope! See below] and so the only product the brain needs to make reliably is output. Output is not a physical product, it is information. Like a novel, which can be paperback or hardback. audio book or ebook, as long as the content is the same then nothing essential has changed.”
This makes exactly the point I made before; output is not physical, it’s computational. I know that computations work on a physical substrate as you replied when I made this point myself, but the substrate isn’t the information; it’s just a contingent means to presenting it. In the same way, the information in a book is not the ink on a page, or the bits, or whatever .It is a pattern with purely observer relative meaning.That’s a basic proposition of cognitive science, from multiply realisability. So yes RIGHT: “Output is not a physical product, it is information”. And so it is observer dependent, so it can’t be the basis of consciousness without a homunculus to observe it!

By the way, there are several reason why I prefer to discuss in written form, for now at least; asynchronous discussion being easier to maintain and fit round other commitments; I like to be able to think through and express my arguments as clearly explicitly as possible, which can be more difficult in conversation , and I kind of hope that someone else might find these discussions useful, someday! I do agree that the Blogspot format isn’t ideal; it breaks up the flow when an argument won’t fit into a single comment.
--
An example of discussion I wanted to postpone was the debate over the point that just as simulated fire won’t burm, simulated mind won’t be conscious. IIf that’s right it undermines computationalist assumptions. In reply you made the quite popular that, while fire and computer models of fire are made of very different stuff, the mind and a model of the mind are made of the same stuff; such as computional structures or information processing.. I now see that these contrary positions have come back to bite us, The meanings we are each assuming for some of our key concepts are radically different, I thought I’d already posted the first extract, but I can’t see it or find it by search so. I’ve posted it after this comment. with another quotation from Searle that I think is to the point.


Inti3 September 2014 at 14:10
This is from Critique of Cognitive Reason, which is chapter 9 of Rediscovery of the Mind, but is also online in an earlier form at http://www.federaljack.com/ebooks/Consciousness%20Books%20Collection/John%20R.%20Searle%20-%20The%20Critique%20of%20Cognitive%20Reason.pdf
Further Difficulty : The Brain Does Not Do Information Processing In this section I turn finally to what I think is, in some ways, the central issue in all of this, the issue of information processing. Many people in the " cognitive science" scientific paradigm will feel that much of my discussion is simply irrelevant , and they will argue against it as follows :
“There is a difference between the brain and all of the other systems you have been describing, and this difference explains why a computational simulation in the case of the other systems is a mere simulation , whereas in the case of the brain a computational simulation is actually duplicating and not merely modeling the functional properties of the brain . The reason is that the brain, unlike these other systems, is an information processings ystem. And this fact about the brain is, in your words , intrinsic ." It is just a fact about biology that the brain functions to process information , and as we can also process the same information computationally , computational models of brain processes have a different role altogether from computational models of, for example, the weather .
So there is a well -defined research question : Are the computational procedures by which the brain processes information the same as the procedures by which computers process the same information?”
What I just imagined an opponent saying embodies one of the worst mistakes in cognitive science. The mistake is to suppose that in the sense in which computers are used to process information , brains also process information .
To see that that is a mistake contrast what goes on in the computer with what goes on in the brain . In the case of the computer, an outside agent encodes some information in a form that can be processed by the circuitry of the computer. That is, he or she provides a syntactical realization of the information that the computer can implement in , for example, different voltage levels. The computer then goes through a series of electrical stages that the outside agent can interpret both syntactically and semantically even though , of course, the hardware has no intrinsic syntax or semantics: It is all in the eye of the beholder . And the physics does not matter, provided only that you can get it to implement the algorithm . Finally, an output is produced in the form of physical phenomena, for example, a printout , which an observer can interpret as symbols with a syntax and a semantics. But now contrast that with the brain . In the case of the brain, none of the relevant neurobiological processes are observer relative (though of course, like anything they can be described from an observer-relative point of view ), and the specificity of the neurophysiology matters desperately

Reply

Inti3 September 2014 at 14:11
(continuing Searle extract) To make this difference clear, let us go through an example. Suppose I see a car coming toward me. A standard computational model of vision will take in information about the visual array on my retina and eventually print out the sentence,"There is a car coming toward me."
[DM I know you’ve said that it is not about sentences, though computationalist like Fodor claim that it is; Still, the principle is the same in ANY computational system – it’s a purely symbolic response so Searle point still applies. Inti]
But that is not what happens in the actual biology. In the biology a concrete and specific series of electrochemical reactions are set up by the assault of the photons on the photo receptor cells of my retina, and this entire process eventually results in a concrete visual experience. The biological reality is not that of a bunch of words or symbols being produced by the visual system; rather, it is a matter of a concrete specific conscious visual event- this very visual experience. That concrete visual event is as specific and as concrete as a hurricane or the digestion of a meal. We can, with the computer, make an information processing model of that event or of its production , as we can make an information processing model of the weather, digestion , or any other phenomenon , but the phenomena themselves are not thereby information processing systems.
In short, the sense of information processing that is used in cognitive science is at much too high a level of abstraction to capture the concrete biological reality of intrinsic intentionality . The "information " in the brain is always specific to some modality or other. It is specific to thought , or vision , or hearing, or touch, for example. The level of information processing described in the cognitive science computational models of cognition , on the other hand, is simply a matter of getting a set of symbols as output in response to a set of symbols as input . We are blinded to this difference by the fact that the sentence, "I see a car coming toward me," can be used to record both the visual inten - tionality and the output of the computational model of vision . But this should not obscure the fact that the visual experience is a concrete conscious event and is produced in the brain by specific electrochemical biological processes. To confuse these events and processes with formal symbol manipulation is to confuse the reality with the model . The upshot of this part of the discussion is that in the sense of "information " used in cognitive science, it is simply false to say that the brain is an information processing device. . . .

Reply
Replies

Inti3 September 2014 at 14:12
And from Faigenbaum, Gustavo (2005-07-09). Conversations with John Searle (Kindle Locations 949-966).
John Searle: There is something, though, that I’m not completely satisfied about my own views . We need to think very seriously about the notion of information, and information processing. I argue in various places that it’s a mistake to think that the unconscious level of information processing that goes on the brain is literally information processing. That’s a metaphorical attribution to the steps the brain goes through, and whereby it produces intrinsic information in the form of thoughts and perceptions in the agent. But in the stages in between, though they impact in such a way as to cause for example a conscious perception, there’s literally no information. There is just a sequence of causes and effects. I think that’s right, but I think we need to say some more about it. I mean, why is it that the information processing terminology is so useful to us? Why does it seem to be so functional and so important when you’re describing how the brain is able to produce a visual experience, for example? Now, the visual experience, in my view, has intrinsic information because it’s got intentionality . It has intentional content. But the actual sequence of neurological events that lead from the retina to the visual experience doesn’t have any information. Those events are just blind processes that we can describe as if they had some mental content or intentional content. But they’re not literally that way. However, I think we need to know more . The fact is that the information processing terminology is very useful. G. F.: In developmental cognitive psychology, information processing is a very basic assumption. Yet it is not clear for me whether cognitive psychologists are referring to a neurophysiological, hard-wired process, or to an intermediate level of information processing, between the level of neurophysiology and the level of conscious, intentional experience.



Inti3 September 2014 at 14:13
Faigenbaum, Gustavo (2005-07-09). Conversations with John Searle (Kindle Locations 929-947). LibrosEnRed. Kindle Edition.
John Searle; Well, originally cognitive science was based on the assumption that there was an intermediate level between the level of common sense intentionality and the level of neurobiology. And that was the level of the computer program. Now that’s false. I think I refuted that view. There is no such level. However, the fact remains that it is useful to describe a lot of the processes that go on at the neurobiological level as if they were thinking, as if they were doing information processing. Now I say that you need to distinguish between those features of the world that exist only relative to us, the observer-relative, and the observer-independent features. Now if the book is on the table, and I see the book is on the table, that’s observer independent. It doesn’t matter what other people think. But when people say that information about the book was already present in the lateral geniculate nucleus, that’s observer relative. They’re saying there’s some neuron firing in the lateral geniculate nucleus, which later on produces such and such information in the experience. I think that’s a harmless way of speaking provided you don’t think that there’s literally some intentionality going on at this lower level. There isn’t. Most people in cognitive science just don’t get it. They don’t understand what I’m talking about. They think it’s obvious there is information there. Don’t you see? The guy gets photons on a retina and later on he sees a book. There must be some information in between. It doesn’t follow, of course, but it’s very hard to get people to see that it doesn’t.
G. F.: Therefore, your argument basically consists in a conceptual distinction.
John Searle: Yes. If you’re going to do science, you have to understand the distinction between the features of reality that are observer independent and those that are observer relative. You can’t begin to do science without that.
(My emphases)

I really will have to give more time to other projects for a while, so I’ll have to try to resist the temptation to come back in the next couple of weeks. I suspect from a quick skim that I might want to what spare time I have to replying to the latest post on Scientia Salon, too. See you here later, though.

Reply

Disagreeable Me9 September 2014 at 22:07
Hi Inti,

OK, I understand that we're just going to assume that what happens within a neuron is not important for consciousness but what happens between neurons is. On this assumption combined with the rest of Searle's view then I agree that replacing all the neurons individually does not have implications for consciousness but replacing the whole lot does.

So I can accept points 1-8 without too much complaint. Point 9 is where I think Searle's view falls apart.

"Of course there is an observer; the conscious individual concerned, at least until that consciousness is extinguished. I don’t understand how you can deny that."

Let me try to explain.

We're stipulating that all the behaviour of the black box region is perfectly like a real brain. So, as the black box begins to encroach into the centers of consciousness (the brain box, as you call it), Searle's view suggests that consciousness begins to diminish in some way. The problem is that if this were happening to you, you could not possibly know it, because if you could know it you could report the change, and if you could report the change then your brain is behaving differently, which we stipulated is not the case.

So, on Searle's view, you can transition smoothly from 100% conscious to 100% unconscious without ever noticing that anything is happening to you. If you can't notice, neither can anybody else. There is no observer who can detect the difference.

I'm going to leave all the other points alone, for now, but I will just reassert my view that the brain is doing information processing, that it does accept input and produce output, and that computation is not observer relative on mathematical Platonism. I can see how some of Searle's arguments may be problematic for some computationalists, but not for my own idiosyncratic version which I feel is correct.

Reply
Replies

Inti10 September 2014 at 12:19
Hi DM

I think I’m getting the point, but I’m still not sure. Are you still thinking about the case when you expand beyond the black box neuron and start to encroach on the brain box level? In that case, I think you have missed part of my point. We have definitely not stipulated that the behaviour of the brain would remain unchanged. That stipulation only applied to the neuron.

If I understand you, you are thinking that you could expand the black box to encroach in ever more of the brain box, and the stipulation about “inputs” and “outputs” would still apply to the expanded boundary, at whatever scale. But the point of my steps 5, 6 and 8 was to show why that’s not the case.
The key point is this, for the neuronal black box, the critical factor is its interaction with other neurons, all of which extend outwards from the BB. For the brain box, the crucial factor for consciousness lies entirely within it. As you expand the black boxes’ boundary (BB for short) into the brain box, you are cutting off elements of the brain box’s internal process and hiding them from the remainder of the black box within the BB; they become inaccessible to the brainbox, sealed off within the BB.

I can anticipate you saying “why should that be the case. How could the remaining parts of the brain box know that anything has changed?” The answer is, because we are now removing the biological level of interaction at the level where it is crucial to consciousness. You are replacing them with computational processes, which are only symbolic descriptions of what previously happened on a physical level. To allow the BB to extend to the level of the brain box without effect would be to grant computationalism as a premise.

I suppose the most concise way to make the same point is; in the case of the black box, consciousness is on the outside looking in (or failing to); in the case of the brain box, consciousness is on the inside looking out.

I think that’s right. However, I want to remind you that this is only a side issue. Nothing in the idea that neurons could be treated as black boxes is vital to biological naturalism. You could persuade me that it doesn’t work, in which case I’d say fine, then it has to be biological causation inside the neuron, as well. You could disagree, obviously, but then we are just returning to the core argument.


Inti10 September 2014 at 12:20

I have a genuine question to ask you. First, to clarify why I’m asking; I appreciate that you have an idiosyncratic twist on computationalism. However, leaving that aside for a moment, I had planned to ask you about your sympathy for connectionism in Scientia Salon.

I have a feeling that there is a sharp divide down the middle of connectionism; on one side, it is still a variant of computationalism, it doesn’t matter how you arrange the computational elements as long as they are still computation i.e. algorithmic symbol processing and it’s not immune to Searle’s arguments.

On the other side of the divide, the strongly “hardware based” version, it may be that it stops being computational at all, so that Searle’s response to the Many Mansion applies; fine, that might give consciousness, because it doesn’t assume that computation is a sufficient cause in itself. It might prove to have the same physical causal powers as biological systems.

You suggest that Searle’s concept of computation is too narrow. I disagree: of course you could redefine the term to include the hypothetical “hard” second form of connectionism, but that would be changing the subject, rather like the move you called Coel on, in Scientia Salon. Short of that, I think that Searle’s various argument about syntax, thesymbols and the observer relative still apply to any form of digital data processing, which includes computational simulated connectionist machines.

What I wanted to ask you was, which particular connectionist projects do you feel are promising. Can you give me some references on them, because I’d like to look at them in more detail, and see for myself whether they really do escape Searle’s criticism.

I know there is another difference between us that I still need to investigate in more detail, at a later moment; you don’t accept the claimed distinction between the causal, physical nature of processes in the brain, and the non-physical, symbolic nature of computation. You think that must be wrong, if only because any kind of computer must have a physical substrate. I think that’s not the point, precisely because computationalism depends on multiple realisabilty and a functional, not a physical view of what computation is. If you are right, the argument that “simulation does not equal reproduction” probably fails in the case of the computer/mind. If I’m right, I have a feeling that the implications extend out into many of the attempted replies to the Chinese Room , and further.


Disagreeable Me10 September 2014 at 17:29
Hi Inti,

I can't tell you how much I value this conversation. I've been frustrated for a long time trying to discuss Searle's view with people who don't really get it, including people who call themselves biological naturalists.

"Are you still thinking about the case when you expand beyond the black box neuron and start to encroach on the brain box level?"

Yes. I'm talking about where the black box, a single device, begins to emulate and replace a greater and greater number of neurons, eventually taking over the whole brain. I am aware that this is not the same thought experiment Searle was discussing. It is a different one, and one much more challenging to biological naturalism.

If there is a discrete part of the brain responsible for consciousness (and I'm far from certain of this), the black box will eventually encroach on this area and consciousness will start to fade (on your view at least). Even if there isn't a brain box, we're still looking at a smooth transition between consciousness and unconsciousness.

"We have definitely not stipulated that the behaviour of the brain would remain unchanged."

I am stipulating it.

If you object to this stipulation, then you have a problem not only with strong AI but with even with weak AI. If the behaviour of the brain is thought of as either the signals it emits in response to sensory signals, or as macroscopic behaviour of an organism, weak AI claims that it should be possible to build a computational device which behaves exactly like the brain. Strong AI claims it is conscious.

So I'm imagining biological neurons wired into an ever-growing black box. Everything within the black box is simulated perfectly so that from the point of view of any of the biological neurons nothing has changed. By the time the black box has engulfed the whole brain, you're left with a person that has a computer in their head but is outwardly normal. They have become in effect a philosophical zombie (on your view).

"To allow the BB to extend to the level of the brain box without effect would be to grant computationalism as a premise."

No. It would be to grant weak AI as a premise. Computationalism assumes that the black box would be conscious, but I'm only assuming that it behaves like a brain from the perspective of an external observer. So I'm making claims only about behaviour, not about consciousness. That's why I'm going along with your view that consciousness would be diminished.

But it turns out that this view is absurd, and I think it is fatal for biological naturalism.


Disagreeable Me10 September 2014 at 17:33
I imagine the divide you perceive in connectionism is only on the biological naturalist side. I doubt there are any serious AI researchers who think it makes any difference besides cost and efficiency whether neurons are implemented in software or hardware.

I am not an expert on connectionist projects. I don't think there are any promising projects right now (i.e. there are no current research projects that look like they are going to produce a conscious computer). But there are lots of practical applications of connectionist approaches to performing tasks that brains are stereotypically good at but hand-designed algorithms are not.

But I'm not an expert in the field, and I'm certainly not current on bleeding edge stuff.

Reply



-------------



The Chinese Room Argument
First published Fri Mar 19, 2004; substantive revision Wed Oct 23, 2024
The argument and thought-experiment now generally known as the Chinese Room Argument was first published in a 1980 article by American philosopher John Searle (1932– ). It has become one of the best-known arguments in recent philosophy. Searle imagines himself alone in a room following a computer program for responding to Chinese characters slipped under the door. Searle understands nothing of Chinese, and yet, by following the program for manipulating symbols and numerals just as a computer does, he sends appropriate strings of Chinese characters back out under the door, and this leads those outside to mistakenly suppose there is a Chinese speaker in the room.

The narrow conclusion Searle draws from the argument is that programming a digital computer may make it appear to understand language but could not produce real understanding. Hence the “Turing Test” is inadequate. Searle argues that the thought experiment underscores the fact that computers merely use syntactic rules to manipulate symbol strings, but have no understanding of meaning or semantics. The broader conclusion of the argument is that the theory that human minds are computer-like computational or information processing systems is refuted. Instead minds must result from biological processes; computers can at best simulate these biological processes. Thus the argument has large implications for semantics, philosophy of language and mind, theories of consciousness, computer science, and cognitive science generally. As a result, there have been many critical replies to the argument.

1. Overview
2. Historical Background
2.1 Leibniz’ Mill
2.2 Turing’s Paper Machine
2.3 The Chinese Nation
3. The Chinese Room Argument
4. Replies to the Chinese Room Argument
4.1 The Systems Reply
4.2 The Robot Reply
4.3 The Brain Simulator Reply
4.4 The Other Minds Reply
4.5 The Intuition Reply
4.6 Advances in Artificial intelligence
5. The Larger Philosophical Issues
5.1 Syntax and Semantics
5.2 Intentionality
5.3 Mind and Body
5.4 Simulation, duplication and evolution
Conclusion
Bibliography
Academic Tools
Other Internet Resources
Related Entries
1. Overview
Work in Artificial Intelligence (AI) has produced computer programs that can beat the world chess champion, control autonomous vehicles, and defeat the best human players on the television quiz show Jeopardy. By 2022 AI had evolved from personal digital assistants (Alexa, Siri, Google Assistant) translating and answering questions to using Large Language Models (LLMs) that could write poems, college level essays, and computer programs, and could pass exams designed to screen the entrants into graduate schools, the study and practice of Law, and other “learned professions”. Our experience shows that playing chess or Jeopardy, writing essays, passing difficult exams, and carrying on a conversation, are activities that require understanding and intelligence. Does computer prowess at conversation, writing essays, and passing difficult examinations then show that computers can understand language and be intelligent? Will further development result in digital computers that fully match or even exceed human intelligence?

Alan Turing (1950), one of the pioneer theoreticians of computing, believed the answer to these questions was “yes”. Turing proposed what is now known as ‘The Turing Test’: if a computer can pass for human in online chat, we should grant that it is intelligent. By the late 1970s some AI researchers claimed that computers already understood at least some natural language. In 1980 U.C. Berkeley philosopher John Searle introduced a short and widely-discussed argument intended to show conclusively that it is impossible for digital computers to understand language or think, now or in the future

Searle argues that a good way to test a theory of mind, say a theory that holds that understanding can be created by doing such and such, is to imagine what it would be like to actually do what the theory says will create understanding. Searle (1999) summarized his Chinese Room Argument (hereinafter, CRA) concisely:

Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese.
Searle goes on to say, “The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have.”

Thirty years after introducing the CRA Searle 2010 describes the conclusion in terms of consciousness and intentionality:

I demonstrated years ago with the so-called Chinese Room Argument that the implementation of the computer program is not by itself sufficient for consciousness or intentionality (Searle 1980). Computation is defined purely formally or syntactically, whereas minds have actual mental or semantic contents, and we cannot get from syntactical to the semantic just by having the syntactical operations and nothing else. To put this point slightly more technically, the notion “same implemented program” defines an equivalence class that is specified independently of any specific physical realization. But such a specification necessarily leaves out the biologically specific powers of the brain to cause cognitive processes. A system, me, for example, would not acquire an understanding of Chinese just by going through the steps of a computer program that simulated the behavior of a Chinese speaker (p.17).
“Intentionality” is a technical term for a feature of mental and certain other things, namely being about something. Thus a desire for a piece of chocolate as well as thoughts about real-world Manhattan or fictional Harry Potter all display intentionality, as will be discussed in more detail in section 5.2 below.

Searle’s shift from machine understanding to consciousness and intentionality is not directly supported by the original 1980 argument. However the re-description of the conclusion indicates the close connection between understanding and consciousness in Searle’s later accounts of meaning and intentionality. Those who don’t accept Searle’s linking of understanding and consciousness might hold that running a program can create understanding without necessarily creating consciousness, and conversely a fancy robot might have dog level consciousness, desires, and beliefs, without necessarily understanding natural language.

In moving to discussion of intentionality Searle seeks to develop the broader implications of his argument. It aims to refute the functionalist approach to understanding minds, that is, the approach that holds that mental states are defined by their causal roles, not by the stuff (neurons, transistors) that plays those roles. The argument counts especially against that form of functionalism known as the Computational Theory of Mind that treats minds as information processing systems. As a result of its scope, as well as Searle’s clear and forceful writing style, the Chinese Room argument has probably been the most widely discussed philosophical argument in cognitive science to appear since the Turing Test. By 1991 computer scientist Pat Hayes had defined Cognitive Science as the ongoing research project of refuting Searle’s argument. Cognitive psychologist Steven Pinker (1997) pointed out that by the mid-1990s well over 100 articles had been published on Searle’s thought experiment – and that discussion of it was so pervasive on the Internet that Pinker found it a compelling reason to remove his name from all Internet discussion lists.

This interest has not subsided, and the range of connections with the argument has broadened. A search on Google Scholar for “Chinese Room Argument” produces thousands of results, including papers making connections between the argument and topics ranging from embodied cognition to theater to talk psychotherapy to postmodern views of truth and “our post-human future” – as well as discussions of group or collective minds, and discussions of the role of intuitions in philosophy. In 2007 a UK game company took the name “The Chinese Room” in joking honor of “...Searle’s critique of AI – that you could create a system that gave the impression of intelligence without any actual internal smarts.” This wide-range of discussion and implications is a tribute to the argument’s simple clarity and centrality.

2. Historical Background
2.1 Leibniz’ Mill
Searle’s argument has four important antecedents. The first of these is an argument set out by the philosopher and mathematician Gottfried Leibniz (1646–1716). This argument, often known as “Leibniz’ Mill”, appears as section 17 of Leibniz’ Monadology. Like Searle’s argument, Leibniz’ argument takes the form of a thought experiment. Leibniz asks us to imagine a physical system, a machine, that behaves in such a way that it supposedly thinks and has experiences (“perception”).

17. Moreover, it must be confessed that perception and that which depends upon it are inexplicable on mechanical grounds, that is to say, by means of figures and motions. And supposing there were a machine, so constructed as to think, feel, and have perception, it might be conceived as increased in size, while keeping the same proportions, so that one might go into it as into a mill. That being so, we should, on examining its interior, find only parts which work one upon another, and never anything by which to explain a perception. Thus it is in a simple substance, and not in a compound or in a machine, that perception must be sought for. [Robert Latta translation]
Notice that Leibniz’s strategy here is to contrast the overt behavior of the machine, which might appear to be the product of conscious thought, with the way the machine operates internally. He points out that these internal mechanical operations are just parts moving from point to point, hence there is nothing that is conscious or that can explain thinking, feeling or perceiving. For Leibniz physical states are not sufficient for, nor constitutive of, mental states.

To this day the mystery of consciousness remains; one can still follow Leibniz’ suggestion and imagine a brain made so huge that one could walk between the neurons, and all one would see is, at best, squirts of neurotransmitters, and nothing to explain conscious experience, including the experience of understanding language. Leibniz’ argument, that no matter what a physical system does, there would be no consciousness (and so materialism is refuted), is parallel to Searle’s claim that no matter what syntactic processing there is, there would be no understanding of meaning (and so strong AI claims are refuted).

2.2 Turing’s Paper Machine
A second antecedent to the Chinese Room argument is the idea of a paper machine, a computer implemented by a human. This idea is found in the work of Alan Turing, for example in “Intelligent Machinery” (1948). Turing writes there that he wrote a program for a “paper machine” to play chess. A paper machine is a kind of program, a series of simple steps like a computer program, but written in natural language (e.g., English), and implemented by a human. The human operator of the paper chess-playing machine need not (otherwise) know how to play chess. All the operator does is follow the instructions for generating moves on the chess board. In fact, the operator need not even know that he or she is involved in playing chess – the input and output strings, such as “N–QB7” need mean nothing to the operator of the paper machine.

As part of the WWII project to decipher German military encryption, Turing had written English-language programs for human “computers”, as these specialized workers were then known, and these human computers did not need to know what the programs that they implemented were doing.

One reason the idea of a human-plus-paper machine is important is that it already raises questions about agency and understanding similar to those in the CRA. Suppose I am alone in a closed room and follow an instruction book for manipulating strings of symbols. I thereby implement a paper machine that generates symbol strings such as “N-KB3” that I write on pieces of paper and slip under the door to someone ouside the room. Suppose further that prior to going into the room I don’t know how to play chess, or even that there is such a game. However, unbeknownst to me, in the room I am running Turing’s chess program and the symbol strings I generate are chess notation and are taken as chess moves by those outside the room. They reply by sliding the symbols for their own moves back under the door into the room. If all you see is the resulting sequence of moves displayed on a chess board outside the room, you might think that someone in the room knows how to play chess very well. Do I now know how to play chess? Or is it the system (consisting of me, the manuals, and the paper on which I manipulate strings of symbols) that is playing chess? If I memorize the program and do the symbol manipulations inside my head, do I then know how to play chess, albeit with an odd phenomenology? Do someone’s conscious states matter for whether or not they know how to play chess? If a digital computer implements the same program, does the computer (or program or computer plus program) then play chess, or merely simulate this?

By mid-century Turing was optimistic that the newly developed electronic computers themselves would soon be able to exhibit apparently intelligent behavior, answering questions posed in English and carrying on conversations. Turing (1950) proposed what is now known as the Turing Test: if a computer could pass for human in on-line chat, it should be counted as intelligent.

A third antecedent of Searle’s argument was the work of Searle’s colleague at Berkeley, Hubert Dreyfus. Dreyfus was an early critic of the optimistic claims made by AI researchers. In 1965, when Dreyfus was at MIT, he published a circa hundred page report titled “Alchemy and Artificial Intelligence”. Dreyfus argued that key features of human mental life could not be captured by formal rules for manipulating symbols. Dreyfus moved to Berkeley in 1968 and in 1972 published his extended critique, “What Computers Can’t Do”. Dreyfus’ primary research interests were in Continental philosophy, with its focus on consciousness, intentionality, and the role of intuition and the inarticulated background in shaping our understandings. Dreyfus identified several problematic assumptions in AI, including the view that brains are like digital computers, and, again, the assumption that understanding can be codified as explicit rules.

However by the late 1970s, as computers became faster and less expensive, some in the burgeoning AI community started to claim that their programs could understand English sentences, using a database of background information. The work of one of these, Yale researcher Roger Schank (Schank & Abelson 1977) came to Searle’s attention. Schank’s team developed a technique called “conceptual representation” that used “scripts” to represent conceptual relations (related to Conceptual Role Semantics). Searle’s argument was originally presented in 1980 specifically as a response to the claim that AI programs such as Schank’s literally understand the sentences that they respond to.

2.3 The Chinese Nation
A fourth antecedent to the Chinese Room argument are thought experiments involving myriad humans acting as a computer. In 1961 Anatoly Mickevich (pseudonym A. Dneprov) published “The Game”, a story in which a stadium full of 1400 math students are arranged to function as a digital computer (see Dneprov 1961 and the English translation listed at Mickevich 1961, Other Internet Resources). For 4 hours each student repeatedly does a bit of calculation on binary numbers received from someone near them, then passes the binary result onto someone nearby. They learn the next day that they collectively translated a sentence from Portuguese into their native Russian. Mickevich’s protagonist concludes “We’ve proven that even the most perfect simulation of machine thinking is not the thinking process itself, which is a higher form of motion of living matter.”

Apparently independently, a similar consideration emerged in early discussion of functionalist theories of minds and cognition (see further discussion in section 5.3 below), Functionalists hold that mental states are defined by the causal role they play in a system (just as being a door stop is defined by what it does, not by what it is made out of). Critics of functionalism were quick to turn its proclaimed virtue of multiple realizability against it.

By emphasizing causal or information processing roles as the essence of mental states, functionalism allowed us to understand creatures with different physiology, for example extraterrestrials, to have the same types of mental states as humans – pains, for example. But it was pointed out that if extraterrestrial aliens, with some other complex system in place of brains, could realize the functional properties that constituted mental states, then, presumably so could systems even less like human brains. The computational form of functionalism, which holds that the defining role of each mental state is its role in information processing or computation, is particularly vulnerable to this maneuver, since a wide variety of systems with simple components are computationally equivalent (see e.g., Maudlin 1989 for discussion of a computer built from buckets of water). Critics asked if it was really plausible that these inorganic systems could have mental states or feel pain.

Daniel Dennett (1978) reports that in 1974 Lawrence Davis gave a colloquium at MIT in which he presented one such unorthodox implementation. Dennett summarizes Davis’ thought experiment as follows:

Let a functionalist theory of pain (whatever its details) be instantiated by a system the subassemblies of which are not such things as C-fibers and reticular systems but telephone lines and offices staffed by people. Perhaps it is a giant robot controlled by an army of human beings that inhabit it. When the theory’s functionally characterized conditions for pain are now met we must say, if the theory is true, that the robot is in pain. That is, real pain, as real as our own, would exist in virtue of the perhaps disinterested and businesslike activities of these bureaucratic teams, executing their proper functions.
In “Troubles with Functionalism”, also published in 1978, Ned Block envisions the entire population of China implementing the functions of neurons in the brain. This scenario has subsequently been called “The Chinese Nation” or “The Chinese Gym”. We can suppose that every Chinese citizen would be given a call-list of phone numbers, and at a preset time on implementation day, designated “input” citizens would initiate the process by calling those on their call-list. When any citizen’s phone rang, he or she would then phone those on his or her list, who would in turn contact yet others. No phone message need be exchanged; all that is required is the pattern of calling. The call-lists would be constructed in such a way that the patterns of calls implemented the same patterns of activation that occur between neurons in someone’s brain when that person is in a mental state – pain, for example. The phone calls play the same functional role as neurons causing one another to fire. Block was primarily interested in qualia, and in particular, whether it is plausible to hold that the population of China might collectively be in pain, while no individual member of the population experienced any pain, but the thought experiment applies to any mental states and operations, including understanding language.

Thus Block’s thought experiment, as with those of Davis and Dennett, is a system of many humans rather than one. The focus is on consciousness, but to the extent that Searle’s argument also involves consciousness, the thought experiment is closely related to Searle’s. Cole (1984) tries to pump intuitions in the reverse direction by setting out a thought experiment in which each of his neurons is itself conscious, and fully aware of its actions including being doused with neurotransmitters, undergoing action potentials, and squirting neurotransmitters at its neighbors. Cole argues that his conscious neurons would find it implausible that their collective activity produced a consciousness and other cognitive competences, including understanding English, that the neurons lack. That is, the mental states achieved by the activity of my neurons are my mental states, not those of any of my neurons – so if my neurons thought in Chinese (only), that would not show that they don’t collectively produce someone –me– who understands English but not Chinese.) Cole suggests that the intuitions of implementing systems are not to be trusted.

3. The Chinese Room Argument
In 1980 John Searle published “Minds, Brains and Programs” in the journal The Behavioral and Brain Sciences. In this article, Searle sets out the argument, and then replies to the half-dozen main objections that had been raised during his earlier presentations at various university campuses (see next section). In addition, Searle’s article in BBS was published along with comments and criticisms by 27 cognitive science researchers. These 27 comments were followed by Searle’s replies to his critics.

In the decades following its publication, the Chinese Room argument was the subject of very many discussions. By 1984, Searle presented the Chinese Room argument in a book, Minds, Brains and Science. In January 1990, the popular periodical Scientific American took the debate to a general scientific audience. Searle included the Chinese Room Argument in his contribution, “Is the Brain’s Mind a Computer Program?”, and Searle’s piece was followed by a responding article, “Could a Machine Think?”, written by philosophers Paul and Patricia Churchland. Soon thereafter Searle had a published exchange about the Chinese Room with another leading philosopher, Jerry Fodor (in Rosenthal (ed.) 1991).

The heart of the argument is Searle imagining himself following a symbol-processing program written in English (which is what Turing called “a paper machine”). The English speaker (Searle) sitting in the room follows English instructions for manipulating Chinese symbols, whereas a computer “follows” (in some sense) a program written in a computing language. The human produces the appearance of understanding Chinese by following the symbol manipulating instructions, but does not thereby come to understand Chinese. Since a computer just does what the human does – manipulate symbols on the basis of their syntax alone – no computer, merely by following a program, comes to genuinely understand Chinese.

This narrow argument, based closely on the Chinese Room scenario, is specifically directed at a position Searle calls “Strong AI”. Strong AI is the view that suitably programmed computers (or the programs themselves) can understand natural language and actually have other mental capabilities similar to the humans whose behavior they mimic. According to Strong AI, these computers really play chess intelligently, make clever moves, or understand language. By contrast, “weak AI” is the much more modest claim that computers are merely useful in psychology, linguistics, and other areas, in part because they can simulate mental abilities. But weak AI makes no claim that computers actually understand or are intelligent. The Chinese Room argument is not directed at weak AI, nor does it purport to show that no machine can think – Searle says that brains are machines, and brains think. The argument is directed at the view that formal computations on symbols can produce thought.

We might summarize the narrow argument as a reductio ad absurdum against Strong AI as follows. Let L be a natural language, and let us say that a “program for L” is a program for conversing fluently in L. A computing system is any system, human or otherwise, that can run a program.

If Strong AI is true, then there is a program for Chinese, C, such that if any computing system runs C, that system thereby comes to understand Chinese.
I could run C without thereby coming to understand Chinese.
Therefore Strong AI is false.
The first premise elucidates the claim of Strong AI. The second premise is supported by the Chinese Room thought experiment. The conclusion of this narrow argument is that running a program cannot endow the system with language understanding. (There are other ways of understanding the structure of the argument. It may be relevant to understand some of the claims as counterfactual: e.g. “there is a program” in premise 1 as meaning there could be a program, etc. On this construal the argument involves modal logic, the logic of possibility and necessity (see Damper 2006 for the CRA reconstructed as a modal 5 step reductio and Shaffer 2009 in response)).

It is also worth noting that the claim made by Strong AI in the first premise above attributes understanding to “the system”. Exactly what Strong-AI supposes will acquire understanding when the program runs is crucial to the success or failure of the CRA. Schank 1978 has a title that claims their group’s computer, a physical device, understands, but in the body of the paper he claims that the program [“SAM”] is doing the understanding: SAM, Schank says “...understands stories about domains about which it has knowledge” (p. 133). As we will see in the next section (4), these issues about the identity of the understander (the cpu? the program? the system? something else?) quickly came to the fore for critics of the CRA. Searle’s wider argument includes the claim that the thought experiment shows more generally that one cannot get semantics (meaning) from syntax (formal symbol manipulation). That larger claim and related issues are discussed in section 5: The Larger Philosophical Issues.

4. Replies to the Chinese Room Argument
Criticisms of the narrow Chinese Room argument against Strong AI have often followed three main lines, which can be distinguished by how much they concede:

(1) Some critics concede that the man in the room doesn’t understand Chinese, but hold that nevertheless running the program may create comprehension of Chinese by something other than the room operator. These critics object to the inference from the claim that the man in the room does not understand Chinese to the conclusion that no understanding has been created. There might be understanding by a larger, smaller, or different, entity than the man rustling papers in the room. This is the strategy of The Systems Reply and the Virtual Mind Reply. These replies hold that the output of the room might reflect real understanding of Chinese, but the understanding would not be that of the room operator. Thus Searle’s claim that he doesn’t understand Chinese while running the room is conceded, but his claim that there is no understanding of the questions in Chinese, and that computationalism is false, is denied.

(2) Other critics concede Searle’s claim that just running a natural language processing program as described in the CR scenario does not create any understanding, whether by a human or a computer system. But these critics hold that a variation on the computer system could understand. The variant might be a computer embedded in a robotic body, having interaction with the physical world via sensors and motors (“The Robot Reply”), or it might be a system that simulated the detailed operation of an entire human brain, neuron by neuron (“the Brain Simulator Reply”).

(3) Finally, some critics do not concede even the narrow point against AI. These critics hold that the man in the original Chinese Room scenario might understand Chinese, despite Searle’s denials, or that the scenario is impossible. For example, critics have argued that our intuitions in such cases are unreliable. Other critics have held that it all depends on what one means by “understand” – points discussed in the section on The Intuition Reply. Others (e.g. Sprevak 2007) object to the assumption that any system (e.g. Searle in the room) can run any computer program. And finally some have argued that if it is not reasonable to attribute understanding on the basis of the behavior exhibited by the Chinese Room, then it would not be reasonable to attribute understanding to humans on the basis of similar behavioral evidence (Searle calls this last the “Other Minds Reply”). This objection to the CRA is that we should be willing to attribute understanding in the Chinese Room on the basis of the overt behavior, just as we do with other humans (and some animals), and as we would do with extra-terrestrial aliens (or burning bushes or angels) that spoke our language. This position is close to Turing’s own, when he proposed his behavioral test for machine intelligence.

In addition to these responses specifically to the Chinese Room scenario and the narrow argument to be discussed in this section, some critics also independently argue against Searle’s larger claim, and hold that one can get semantics (that is, meaning) from syntactic symbol manipulation, including the sort that takes place inside a digital computer, a question discussed in the section below on Syntax and Semantics.

4.1 The Systems Reply
In the original BBS article, Searle identified and discussed several responses to the argument that he had come across in giving the argument in talks at various places. As a result, these early responses have received the most attention in subsequent discussion. What Searle 1980 calls “perhaps the most common reply” is the Systems Reply.

The Systems Reply (which Searle says was originally associated with Yale, the home of Schank’s AI work) concedes that the man in the room does not understand Chinese. But, the reply continues, the man is but a part, a central processing unit (CPU), in a larger system. The larger system includes the huge database, the memory (scratchpads) containing intermediate states, and the instructions – the complete system that is required for answering the Chinese questions. So the Systems Reply is that while the man running the program does not understand Chinese, the system as a whole does.

Ned Block was one of the first to press the Systems Reply, along with many others including Jack Copeland, Daniel Dennett, Douglas Hofstadter, Jerry Fodor, John Haugeland, Ray Kurzweil and Georges Rey. Rey (1986) says the person in the room is just the CPU of the system. Kurzweil (2002) says that the human being is just an implementer and of no significance (presumably meaning that the properties of the implementer are not necessarily those of the system). Kurzweil hews to the spirit of the Turing Test and holds that if the system displays the apparent capacity to understand Chinese “it would have to, indeed, understand Chinese” – Searle is contradicting himself in saying in effect, “the machine speaks Chinese but doesn’t understand Chinese”.

Margaret Boden (1988) raises levels considerations. “Computational psychology does not credit the brain with seeing bean-sprouts or understanding English: intentional states such as these are properties of people, not of brains” (244) – a person is an agent that is not identical with a brain or a body. “In short, Searle’s description of the robot’s pseudo-brain (that is, of Searle-in-the-robot) as understanding English involves a category-mistake comparable to treating the brain as the bearer, as opposed to the causal basis, of intelligence”. Boden (1988) points out that the room operator is a conscious agent, while the CPU in a computer is not – the Chinese Room scenario asks us to take the perspective of the implementer, and not surprisingly fails to see the larger picture.

Searle’s response to the Systems Reply is simple: in principle, he could internalize the entire system, memorizing all the instructions and the database, and doing all the calculations in his head. He could then leave the room and wander outdoors, perhaps even conversing in Chinese. But he still would have no way to attach “any meaning to the formal symbols”. The man would now be the entire system, yet he still would not understand Chinese. For example, he would not know the meaning of the Chinese word for hamburger. He still cannot get semantics from syntax.

In some ways Searle’s response here anticipates later extended mind views (e.g. Clark and Chalmers 1998): if Otto, who suffers loss of memory, can regain those recall abilities by externalizing some of the information to his notebooks, then Searle arguably can do the reverse: by internalizing the instructions and notebooks he should acquire any abilities had by the extended system. And so Searle in effect concludes that since he doesn’t acquire understanding of Chinese by internalizing the external components of the entire system (e.g. he still doesn’t know what the Chinese word for hamburger means), understanding was never there in the partially externalized system of the original Chinese Room.

In his 2002 paper “The Chinese Room from a Logical Point of View”, Jack Copeland considers Searle’s response to the Systems Reply and argues that a homunculus inside Searle’s head might understand even though the room operator himself does not, just as modules in our brains solve tensor equations that enable us to catch cricket balls. Copeland then turns to consider the Chinese Gym, and again appears to endorse the Systems Reply: “…the individual players [do not] understand Chinese. But there is no entailment from this to the claim that the simulation as a whole does not come to understand Chinese. The fallacy involved in moving from part to whole is even more glaring here than in the original version of the Chinese Room Argument”. Copeland denies that connectionism implies that a room of people can simulate the brain.

Shaffer 2009 examines modal aspects of the logic of the CRA and argues that familiar versions of the System Reply are question-begging. But, Shaffer claims, a modalized version of the System Reply succeeds because there are possible worlds in which understanding is an emergent property of complex syntax manipulation. Nute 2011 is a reply to Shaffer.

Stevan Harnad has defended Searle’s argument against Systems Reply critics in two papers. In his 1989 paper, Harnad writes “Searle formulates the problem as follows: Is the mind a computer program? Or, more specifically, if a computer program simulates or imitates activities of ours that seem to require understanding (such as communicating in language), can the program itself be said to understand in so doing?” (Note the specific claim: the issue is taken to be whether the program itself understands.) Harnad concludes: “On the face of it, [the CR argument] looks valid. It certainly works against the most common rejoinder, the ‘Systems Reply’….” Harnad appears to follow Searle in linking understanding and states of consciousness: Harnad 2012 (Other Internet Resources) argues that Searle shows that the core problem of conscious “feeling” requires sensory connections to the real world. (See sections below “The Robot Reply” and “Intentionality” for discussion.)

Finally some have argued that even if the room operator memorizes the rules and does all the operations inside his head, the room operator does not become the system. Cole (1984) and Block (1998) both argue that the result would not be identity of Searle with the system but much more like a case of multiple personality – distinct persons in a single head. The Chinese responding system would not be Searle, but a sub-part of him. In the CR case, one person (Searle) is an English monoglot and the other is a Chinese monoglot. The English-speaking person’s total unawareness of the meaning of the Chinese responses does not show that they are not understood. This line, of distinct persons, leads to the Virtual Mind Reply.

4.1.1 The Virtual Mind Reply
The Virtual Mind reply concedes, as does the System Reply, that the operator of the Chinese Room does not understand Chinese merely by running the paper machine. However the Virtual Mind reply holds that what is important is whether understanding is created, not whether the Room operator is the agent that understands. Unlike the Systems Reply, the Virtual Mind reply (VMR) holds that a running system may create new, virtual, entities that are distinct from both the system as a whole, as well as from the sub-systems such as the CPU or operator. In particular, a running system might create a distinct agent that understands Chinese. This virtual agent would be distinct from both the room operator and the entire system. The psychological traits, including linguistic abilities, of any mind created by artificial intelligence will depend entirely upon the program and the Chinese database, and will not be identical with the psychological traits and abilities of a CPU or the operator of a paper machine, such as Searle in the Chinese Room scenario. According to the VMR the mistake in the Chinese Room Argument is to make the claim of strong AI to be “the computer understands Chinese” or “the System understands Chinese”. The claim at issue for AI should simply be whether “the running computer creates understanding of Chinese”.

For example, John Haugeland writes (2002) that Searle’s response to the Systems Reply is flawed: “…what he now asks is what it would be like if he, in his own mind, were consciously to implement the underlying formal structures and operations that the theory says are sufficient to implement another mind”. According to Haugeland, his failure to understand Chinese is irrelevant: he is just the implementer. The implemented mind would understand – there is a level-of-description fallacy.

A familiar model of virtual agents are characters in computer or video games, as well as generative AIs such as ChatGPT. Characters in video games have various abilities and personalities, and the characters are not identical with the system hardware or program that creates them. A single running system might control two distinct virtual agents, or physical robots, simultaneously, one of which converses only in Chinese and one of which can converse only in English, and which otherwise manifest very different personalities, memories, and cognitive abilities. For the Systems Reply, the system understands, whereas for the VM reply, the running system creates a new, virtual, mind that is not identical with the system or the physical implementation. Thus the VM reply asks us to distinguish between minds and their realizing systems.

Minsky (1980) and Sloman and Croucher (1980) suggested a Virtual Mind reply when the Chinese Room argument first appeared. In his widely-read 1989 paper “Computation and Consciousness”, Tim Maudlin considers minimal physical systems that might implement a computational system running a program. His discussion revolves around his imaginary Olympia machine, a system of buckets that transfer water, implementing a Turing machine. Maudlin’s main target is the computationalists’ claim that such a machine could have phenomenal consciousness. However in the course of his discussion, Maudlin considers the Chinese Room argument. Maudlin (citing Minsky, and Sloman and Croucher) points out a Virtual Mind reply that the agent that understands could be distinct from the physical system (414). Thus “Searle has done nothing to discount the possibility of simultaneously existing disjoint mentalities” (414–5).

Perlis (1992), Chalmers (1996) and Block (2002) have apparently endorsed versions of a Virtual Mind reply as well, as has Richard Hanley in The Metaphysics of Star Trek (1997). Penrose (2002) is a critic of this strategy, and Stevan Harnad scornfully dismisses such heroic resorts to metaphysics. Harnad defended Searle’s position in a “Virtual Symposium on Virtual Minds” (1992) against Patrick Hayes and Don Perlis. Perlis pressed a virtual minds argument derived, he says, from Maudlin. Chalmers (1996) notes that the room operator is just a causal facilitator, a “demon”, so that his states of consciousness are irrelevant to the properties of the system as a whole. Like Maudlin, Chalmers raises issues of personal identity – we might regard the Chinese Room as “two mental systems realized within the same physical space. The organization that gives rise to the Chinese experiences is quite distinct from the organization that gives rise to the demon’s [= room operator’s] experiences”(326).

Cole (1991, 1994) develops the reply and argues as follows: Searle’s argument requires that the agent of understanding be the computer itself or, in the Chinese Room parallel, the person in the room. However Searle’s failure to understand Chinese in the room does not show that there is no understanding being created. One of the key considerations is that in Searle’s discussion the actual conversation with the Chinese Room is always seriously under specified. Searle was considering Schank’s programs, which can only respond to a few questions about what happened in a restaurant, all in third person. But Searle wishes his conclusions to apply to any AI-produced responses, including those that would pass the toughest unrestricted Turing Test, i.e. they would be just the sort of conversations real people have with each other. If we flesh out the conversation in the original CR scenario to include questions in Chinese such as “How tall are you?”, “Where do you live?”, “What did you have for breakfast?”, “What is your attitude toward Mao?”, and so forth, it immediately becomes clear that the answers in Chinese are not Searle’s answers. Searle is not the author of the answers, and his beliefs and desires, memories and personality traits (apart from his industriousness!) are not reflected in the answers and in general Searle’s traits are causally inert in producing the answers to the Chinese questions. This suggests the following conditional is true: if there is understanding of Chinese created by running the program, the mind understanding the Chinese would not be the computer, whether the computer is human or electronic. The person understanding the Chinese would be a distinct person from the room operator, with beliefs and desires bestowed by the program and its database. Hence Searle’s failure to understand Chinese while operating the room does not show that understanding is not being created.

Cole (1991) offers an additional argument that the mind doing the understanding is neither the mind of the room operator nor the system consisting of the operator and the program: running a suitably structured computer program might produce answers submitted in Chinese and also answers to questions submitted in Korean. Yet the Chinese answers might apparently display completely different knowledge and memories, beliefs and desires than the answers to the Korean questions – along with a denial that the Chinese answerer knows any Korean, and vice versa. Thus the behavioral evidence would be that there were two non-identical minds (one understanding Chinese only, and one understanding Korean only). Since these might have mutually exclusive properties, they cannot be identical, and ipso facto, cannot be identical with the mind of the implementer in the room. Alternatively, we can flesh out Searle’s scenario by supposing those outside the room not only submit questions in Chinese, but also in English. The result would appear to be that there are two individuals in the Room – Searle answering questions about himself and what he believes to be the case, and a Chinese speaker with a different personal history and knowledge of the world. Analogously, a video game might include a (virtual) character with one set of cognitive abilities (smart, understands Chinese) as well as another virtual character with an incompatible set (stupid, English monoglot). These inconsistent cognitive traits cannot be traits of the XBOX system that realizes them. Cole argues that the implication is that minds and persons generally are more abstract than the physical systems that realize them (see Mind and Body in the Larger Philosophical Issues section).

In short, the Virtual Mind argument is that since the evidence that Searle provides that there is no understanding of Chinese was that he wouldn’t understand Chinese in the room, the Chinese Room Argument cannot refute a differently formulated equally strong AI claim, asserting the possibility of using a programmed digital computer to create a distinct mind that understands a natural language. Maudlin (1989) says that Searle has not adequately responded to this criticism.

Others however have replied to the VMR, including Stevan Harnad and mathematical physicist Roger Penrose. Penrose is generally sympathetic to the points Searle raises with the Chinese Room argument, and has argued against the Virtual Mind reply. Penrose does not believe that computational processes can account for consciousness, both on Chinese Room grounds, as well as because of limitations on formal systems revealed by Kurt Gödel’s incompleteness proof. (Penrose has two books on mind and consciousness; Chalmers and others have responded to Penrose’s appeals to Gödel.) In his 2002 article “Consciousness, Computation, and the Chinese Room” that specifically addresses the Chinese Room argument, Penrose argues that the Chinese Gym variation – with a room expanded to the size of India, with Indians doing the processing – shows it is very implausible to hold there is “some kind of disembodied ‘understanding’ associated with the person’s carrying out of that algorithm, and whose presence does not impinge in any way upon his own consciousness” (230–1). Penrose concludes the Chinese Room argument refutes Strong AI. Christian Kaernbach (2005) reports that he subjected the virtual mind theory to an empirical test, with negative results.

4.2 The Robot Reply
The Robot Reply concedes Searle is right about the Chinese Room scenario: it shows that a computer trapped in a computer room cannot understand language, or know what words mean. The Robot reply is responsive to the problem of knowing the meaning of the Chinese word for hamburger – Searle’s example of something the room operator would not know. It seems reasonable to hold that most of us know what a hamburger is because we have seen one, and perhaps even made one, or tasted one, or at least heard people talk about hamburgers and understood what they are by relating them to things we do know by seeing, making, and tasting. Given this is how one might come to know what hamburgers are, the Robot Reply suggests that we put a digital computer in a robot body, with sensors, such as video cameras and microphones, and add effectors, such as wheels to move around with, and arms with which to manipulate things in the world. Such a robot – a computer with a body – might do what a child does, learn by seeing and doing. The Robot Reply holds that such a digital computer in a robot body, freed from the room, could attach meanings to symbols and actually understand natural language. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec and Georges Rey are among those who have endorsed versions of this reply at one time or another. The Robot Reply in effect appeals to “wide content” or “externalist semantics”. This can agree with Searle that syntax and internal connections in isolation from the world are insufficient for semantics, while holding that suitable causal connections with the world can provide content to the internal symbols.

About the time Searle was pressing the CRA, many in philosophy of language and mind were recognizing the importance of causal connections to the world as the source of meaning or reference for words and concepts. Hilary Putnam 1981 argued that a Brain in a Vat, isolated from the world but with neurons connected to a computer that generated a virtual world, might speak or think in a language that sounded like English, but it would not be English – hence a brain in a vat could not wonder if it was a brain in a vat (because of its sensory isolation, its words “brain” and “vat” do not refer to brains or vats). The view that meaning was determined by connections with the world became widespread. Searle however resisted this turn outward and continued to think of meaning as subjective and connected with consciousness.

A related view that minds are best understood as embodied or embedded in the world has gained many supporters since the 1990s, contra Cartesian solipsistic intuitions. Organisms rely on environmental features for the success of their behavior. So whether one takes a mind to be a symbol processing system, with the symbols getting their content from sensory connections with the world, or a non-symbolic system that succeeds by being embedded in a particular environment, the importance of things outside the head have come to the fore. Hence many are sympathetic to some form of the Robot Reply: a computational system might understand, provided it is acting in the world. For example, Carter 2007 in a textbook on philosophy and AI concludes “The lesson to draw from the Chinese Room thought experiment is that embodied experience is necessary for the development of semantics.”

However Searle does not think that the Robot Reply to the Chinese Room argument is any stronger than the Systems Reply. All the sensors can do is provide additional input to the computer – and it will be just syntactic input. We can see this by making a parallel change to the Chinese Room scenario. Suppose the man in the Chinese Room receives, in addition to the Chinese characters slipped under the door, a stream of binary digits that appear, say, on a ticker tape in a corner of the room. The instruction books are augmented to use the numerals from the tape as input, along with the Chinese characters. Unbeknownst to the man in the room, the symbols on the tape are the digitized output of a video camera (and possibly other sensors). Searle argues that additional syntactic inputs will do nothing to allow the man to associate meanings with the Chinese characters. It is just more work for the man in the room.

Jerry Fodor, Hilary Putnam, and David Lewis, were principal architects of the computational theory of mind that Searle’s wider argument attacks. In his original 1980 reply to Searle, Fodor allows Searle is certainly right that “instantiating the same program as the brain does is not, in and of itself, sufficient for having those propositional attitudes, e.g. beliefs, characteristic of the organism that has the brain.” But Fodor holds that Searle is wrong about the robot reply. A computer might have beliefs about, and knowledge of, the world if it has the right causal connections to the world – but those are not ones mediated by a man sitting in the head of the robot. We don’t know what the right causal connections are. Searle commits the fallacy of inferring from “the little man is not the right causal connection” to conclude that no causal linkage would succeed. There is considerable empirical evidence that mental processes involve “manipulation of symbols”; Searle gives us no alternative explanation (this is sometimes called Fodor’s “Only Game in Town” argument for computational approaches). In the 1980s and 1990s Fodor wrote extensively on what the connections must be between a brain state and the world for the state to have intentional (representational) properties, while coming to emphasize that computationalism has limits because the computations are intrinsically local and so cannot account for abductive reasoning, that is inference to the best explanation.

In a later piece, “Yin and Yang in the Chinese Room” (in Rosenthal 1991 pp.524–525), Fodor substantially revises his 1980 view. He distances himself from his earlier version of the robot reply, and holds instead that “instantiation” should be defined in such a way that the symbol must be the proximate cause of the effect – no intervening guys in a room. So Searle in the room is not an instantiation of a Turing Machine, and “Searle’s setup does not instantiate the machine that the brain instantiates.” He concludes: “…Searle’s setup is irrelevant to the claim that strong equivalence to a Chinese speaker’s brain is ipso facto sufficient for speaking Chinese.” Searle says of Fodor’s move, “Of all the zillions of criticisms of the Chinese Room argument, Fodor’s is perhaps the most desperate. He claims that precisely because the man in the Chinese room sets out to implement the steps in the computer program, he is not implementing the steps in the computer program. He offers no argument for this extraordinary claim.” (in Rosenthal 1991, p. 525)

In a 1986 paper, Georges Rey advocated a combination of the system and robot reply, after noting that the original Turing Test is insufficient as a test of intelligence and understanding, and that the isolated system Searle describes in the room is certainly not functionally equivalent to a real Chinese speaker sensing and acting in the world. In a 2002 second look, “Searle’s Misunderstandings of Functionalism and Strong AI”, Rey again defends functionalism against Searle, and in the particular form Rey calls the “computational-representational theory of thought – CRTT”. CRTT is not committed to attributing thought to just any system that passes the Turing Test (like the Chinese Room). Nor is it committed to a conversation manual model of understanding natural language. Rather, CRTT is concerned with intentionality, natural and artificial (the representations in the system are semantically evaluable – they are true or false, hence have aboutness). Searle saddles functionalism with the “blackbox” character of behaviorism, but functionalism cares how things are done. Rey sketches “a modest mind” – a CRTT system that has perception, can make deductive and inductive inferences, makes decisions on basis of goals and representations of how the world is, and can process natural language by converting to and from its native representations. To explain the behavior of such a system we would need to use the same attributions needed to explain the behavior of a normal Chinese speaker.

If we flesh out the Chinese conversation in the context of the Robot Reply, just as with the Virtual Mind Reply, we may again see evidence that the entity that understands is not the operator inside the room. Suppose we ask the robot system using the Chinese translation of “what do you see?”, we might get the answer “My old friend Shakey”, or “I see you!”. Whereas if we phone Searle in the room and ask the same questions in English we might get “These same four walls” or “these damn endless instruction books and notebooks.” Again this is evidence that we have distinct responders here, an English speaker and a Chinese speaker, who see and do quite different things. If the giant robot goes on a rampage and smashes much of Tokyo, and all the while oblivious Searle is just following the program in his notebooks in the room, Searle is not guilty of homicide and mayhem, because he is not the agent committing the acts.

Tim Crane discusses the Chinese Room argument in his 1991 book, The Mechanical Mind. He cites the Churchlands’ 1990 luminous room analogy, but then goes on to argue that in the course of operating the room, Searle would learn the meaning of the Chinese: “…if Searle had not just memorized the rules and the data, but also started acting in the world of Chinese people, then it is plausible that he would before too long come to realize what these symbols mean.”(127). (Rapaport 2006 presses an analogy between Helen Keller and the Chinese Room.) Crane appears to end with a version of the Robot Reply: “Searle’s argument itself begs the question by (in effect) just denying the central thesis of AI – that thinking is formal symbol manipulation. But Searle’s assumption, none the less, seems to me correct … the proper response to Searle’s argument is: sure, Searle-in-the-room, or the room alone, cannot understand Chinese. But if you let the outside world have some impact on the room, meaning or ‘semantics’ might begin to get a foothold. But of course, this concedes that thinking cannot be simply symbol manipulation.” (129) The idea that learning grounds understanding has led to work in developmental robotics (a.k.a. epigenetic robotics). This AI research area seeks to replicate key human learning abilities, such as robots that are shown an object from several angles while being told in natural language the name of the object.

Margaret Boden 1988 also argues that Searle mistakenly supposes programs are pure syntax. But programs bring about the activity of certain machines: “The inherent procedural consequences of any computer program give it a toehold in semantics, where the semantics in question is not denotational, but causal.” (250) Thus a robot might have causal powers that enable it to refer to a hamburger.

Stevan Harnad also finds our sensory and motor capabilities to be important: “Who is to say that the Turing Test, whether conducted in Chinese or in any other language, could be successfully passed without operations that draw on our sensory, motor, and other higher cognitive capacities as well? Where does the capacity to comprehend Chinese begin and the rest of our mental competence leave off?” Harnad believes that symbolic functions must be grounded in “robotic” functions that connect a system with the world. And he thinks this counts against symbolic accounts of mentality, such as Jerry Fodor’s, and, one suspects, the approach of Roger Schank that was Searle’s original target. Harnad 2012 (Other Internet Resources) argues that the CRA shows that even with a robot with symbols grounded in the external world, there is still something missing: feeling, such as the feeling of understanding.

However Ziemke 2016 argues a robotic embodiment with layered systems of bodily regulation may ground emotion and meaning, and Seligman 2019 argues that “perceptually grounded” approaches to natural language processing (NLP) have the “potential to display intentionality, and thus after all to foster a truly meaningful semantics that, in the view of Searle and other skeptics, is intrinsically beyond computers’ capacity.”

4.3 The Brain Simulator Reply
Consider a computer that operates in quite a different manner than an AI program with scripts and operations on sentence-like strings of symbols. The Brain Simulator reply asks us to suppose instead the program parallels the actual sequence of nerve firings that occur in the brain of a native Chinese language speaker when that person understands Chinese – every nerve, every firing. Since the computer then works the very same way as the brain of a native Chinese speaker, processing information in just the same way, it will understand Chinese. Paul and Patricia Churchland have set out a reply along these lines, discussed below.

In response to this, Searle argues that it makes no difference. He suggests a variation on the brain simulator scenario: suppose that in the room the man has a huge set of valves and water pipes, in the same arrangement as the neurons in a native Chinese speaker’s brain. The program now tells the man which valves to open in response to input. Searle claims that it is obvious that there would be no understanding of Chinese. (Note however that the basis for this claim is no longer simply that Searle himself wouldn’t understand Chinese – it seems clear that now he is just facilitating the causal operation of the system and so we rely on our Leibnizian intuition that water-works don’t understand (see also Maudlin 1989).) Searle concludes that a simulation of brain activity is not the real thing.

However, following Pylyshyn 1980, Cole and Foelber 1984, and Chalmers 1996, we might wonder about gradually transitioning cyborg systems. Pylyshyn writes:

If more and more of the cells in your brain were to be replaced by integrated circuit chips, programmed in such a way as to keep the input-output function each unit identical to that of the unit being replaced, you would in all likelihood just keep right on speaking exactly as you are doing now except that you would eventually stop meaning anything by it. What we outside observers might take to be words would become for you just certain noises that circuits caused you to make.
These cyborgization thought experiments can be linked to the Chinese Room. Suppose Otto has a neural disease that causes one of the neurons in his brain to fail, but surgeons install a tiny remotely controlled artificial neuron, a synron, alongside his disabled neuron. The control of Otto’s artificial neuron is by John Searle in the Chinese Room, unbeknownst to both Searle and Otto. Tiny wires connect the artificial neuron to the synapses on the cell-body of his disabled neuron. When his artificial neuron is stimulated by neurons that synapse on his disabled neuron, a light goes on in the Chinese Room. Searle then manipulates some valves and switches in accord with a program. That, via the radio link, causes Otto’s artificial neuron to release neuro-transmitters from its tiny artificial vesicles. If Searle’s programmed activity causes Otto’s artificial neuron to behave just as his disabled natural neuron once did, the behavior of the rest of his nervous system will be unchanged. Alas, Otto’s disease progresses; more neurons are replaced by synrons controlled by Searle. Ex hypothesi the rest of the world will not notice the difference; will Otto? If so, when? And why?

Under the rubric “The Combination Reply”, Searle also considers a system with the features of all three of the preceding: a robot with a digital brain simulating computer in its aluminum cranium, such that the system as a whole behaves indistinguishably from a human. Since the normal input to the brain is from sense organs, it is natural to suppose that most advocates of the Brain Simulator Reply have in mind such a combination of brain simulation, Robot, and Systems or Virtual Mind Reply. Some (e.g. Rey 1986) argue it is reasonable to attribute intentionality to such a system as a whole. Searle agrees that it would indeed be reasonable to attribute understanding to such an android system – but only as long as you don’t know how it works. As soon as you know the truth – it is a computer, uncomprehendingly manipulating symbols on the basis of syntax, not meaning – you would cease to attribute intentionality to it.

(One assumes this would be true even if it were one’s spouse, with whom one had built a life-long relationship, that was revealed to hide a silicon secret. Science fiction stories, including episodes of Rod Serling’s television series The Twilight Zone, have been based on such possibilities (the face of the beloved peels away to reveal the awful android truth); however, Steven Pinker (1997) mentions one episode in which the android’s secret was known from the start, but the protagonist still developed a romantic relationship with the android.)

On its tenth anniversary the Chinese Room argument was featured in the general science periodical Scientific American. Leading the opposition to Searle’s lead article in that issue were philosophers Paul and Patricia Churchland. The Churchlands agree with Searle that the Chinese Room does not understand Chinese, but hold that the argument itself exploits our ignorance of cognitive and semantic phenomena. They raise a parallel case of “The Luminous Room” where someone waves a magnet and argues that the absence of resulting visible light shows that Maxwell’s electromagnetic theory is false. The Churchlands advocate a view of the brain as a connectionist system, a vector transformer, not a system manipulating symbols according to syntax-sensitive rules. The system in the Chinese Room uses the wrong computational strategies. Thus they agree with Searle against traditional AI, but they presumably would endorse what Searle calls “the Brain Simulator Reply”, arguing that, as with the Luminous Room, our intuitions fail us when considering such a complex system, and it is a fallacy to move from part to whole: “… no neuron in my brain understands English, although my whole brain does.”

In his 1991 book, Microcognition. Andy Clark holds that Searle is right that a computer running Schank’s program does not know anything about restaurants, “at least if by ‘know’ we mean anything like ‘understand’”. But Searle thinks that this would apply to any computational model, while Clark, like the Churchlands, holds that Searle is wrong about connectionist models. Clark’s interest is thus in the brain-simulator reply. The brain thinks in virtue of its physical properties. What physical properties of the brain are important? Clark answers that what is important about brains are “variable and flexible substructures” which syntactic, rule-based systems like Schank’s (‘GOFAI’, or Good Old-Fashioned AI) lack. But that doesn’t mean computationalism or functionalism is false. It depends on what level you take the functional units to be. Clark defends “microfunctionalism” – one should look to a fine-grained functional description, e.g. neural net level. Clark cites William Lycan approvingly contra Block’s absent qualia objection – yes, there can be absent qualia, if the functional units are made large. But that does not constitute a refutation of functionalism generally. So Clark’s views are not unlike the Churchlands’, conceding that Searle is right about Schank and symbolic-level processing systems, but holding that he is mistaken about connectionist systems.

Similarly Ray Kurzweil (2002) argues that Searle’s argument could be turned around to show that human brains cannot understand – the brain succeeds by manipulating neurotransmitter concentrations and other mechanisms that are in themselves meaningless. In criticism of Searle’s response to the Brain Simulator Reply, Kurzweil says: “So if we scale up Searle’s Chinese Room to be the rather massive ‘room’ it needs to be, who’s to say that the entire system of a hundred trillion people simulating a Chinese Brain that knows Chinese isn’t conscious? Certainly, it would be correct to say that such a system knows Chinese. And we can’t say that it is not conscious anymore than we can say that about any other process. We can’t know the subjective experience of another entity….”

4.4 The Other Minds Reply
Related to the preceding is The Other Minds Reply: “How do you know that other people understand Chinese or anything else? Only by their behavior. Now the computer can pass the behavioral tests as well as they can (in principle), so if you are going to attribute cognition to other people you must in principle also attribute it to computers.”

Searle’s (1980) reply to this is very short:

The problem in this discussion is not about how I know that other people have cognitive states, but rather what it is that I am attributing to them when I attribute cognitive states to them. The thrust of the argument is that it couldn’t be just computational processes and their output because the computational processes and their output can exist without the cognitive state. It is no answer to this argument to feign anesthesia. In ‘cognitive sciences’ one presupposes the reality and knowability of the mental in the same way that in physical sciences one has to presuppose the reality and knowability of physical objects.
Critics of Searle’s claim here argue that if the evidence we have that humans understand is the same as the evidence we might have that a visiting extra-terrestrial alien understands, which is the same as the evidence that a robot understands, the presuppositions we may make in the case of our own species are not relevant, for presuppositions are sometimes false. For similar reasons, Turing, in proposing the Turing Test, is specifically worried about our presuppositions and chauvinism. If the reasons for the presuppositions regarding humans are pragmatic, in that they enable us to predict the behavior of humans and to interact effectively with them, perhaps the presupposition could apply equally to computers (similar considerations are pressed by Dennett, in his discussions of what he calls the Intentional Stance).

Searle raises the question of just what we are attributing in attributing understanding to other minds, saying that it is more than complex behavioral dispositions. For Searle, understanding appears to involve states of consciousness, as is seen in his 2010 summary of the CRA conclusions. Terry Horgan (2013) endorses this claim: “the real moral of Searle’s Chinese room thought experiment is that genuine original intentionality requires the presence of internal states with intrinsic phenomenal character that is inherently intentional…” But this tying of understanding to phenomenal consciousness raises a host of issues.

We attribute limited understanding of language to toddlers, dogs, and other animals, but it is not clear that we are ipso facto attributing unseen states of subjective consciousness – what do we know of the hidden states of exotic creatures? Ludwig Wittgenstein (the Private Language Argument) and his followers pressed similar points. Altered qualia possibilities, analogous to the inverted spectrum, arise: suppose I ask “what’s the sum of 5 and 7” and you respond “the sum of 5 and 7 is 12”, but as you heard my question you had the conscious experience of hearing and understanding “what is the sum of 10 and 14”, though you were in the computational states appropriate for producing the correct sum and so said “12”. Are there certain conscious states that are “correct” for certain functional states? Wittgenstein’s considerations appear to be that the subjective state is irrelevant, at best epiphenomenal, if a language user displays appropriate linguistic behavior. Afterall, we are taught language on the basis of our overt responses, not our qualia or states of consciousness. The mathematical savant Daniel Tammet reports that when he generates the decimal expansion of pi to thousands of digits he experiences colors that reveal the next digit, but even here it may be that Tennant’s performance is likely not produced by the colors he experiences, but rather by unconscious neural computation that produces both the correct answer and the color he experiences. The possible importance of subjective states is further considered in the section on Intentionality, below.

Since the CRA there has been philosophical interest in another other-minds problem, namely the possibility of zombies – creatures that look like and behave just as normal humans, including linguistic behavior, yet have no subjective consciousness. A difficulty for claiming that subjective states of consciousness are crucial for understanding meaning will arise in these cases of absent qualia: we can’t tell the difference between zombies and non-zombies, and so on Searle’s account we can’t tell the difference between those that really understand English and those that don’t. And if you and I can’t tell the difference between those who understand language and Zombies who behave like they do but don’t really, than neither can any selection factor in the history of human evolution – for predators, mates, fellow tribe members, zombies and true understanders, with the “right” conscious experience, have been indistinguishable. But then there appears to be a distinction without a difference. In any case, Searle’s short reply to the Other Minds Reply may be too short.

Descartes famously argued that speech was sufficient for attributing minds and consciousness to others, and infamously argued that it was necessary. Turing was in effect endorsing Descartes’ sufficiency condition, at least for intelligence, while substituting written for oral linguistic behavior. Since most of us use dialog as a sufficient condition for attributing understanding, Searle’s argument, which holds that speech is a sufficient condition for attributing understanding to humans but not for anything that doesn’t share our biology, an account would appear to be required of what additionally is being attributed, and what can justify the additional attribution. Further, if being con-specific is key on Searle’s account, a natural question arises as to what circumstances would justify us in attributing understanding (or consciousness) to extra-terrestrial aliens who do not share our biology? Offending ET’s by withholding attributions of understanding until after doing a brain scan or post-mortem may be risky.

Hans Moravec, director of the Robotics laboratory at Carnegie Mellon University, and author of Robot: Mere Machine to Transcendent Mind, argues that Searle’s position merely reflects intuitions from traditional philosophy of mind that are out of step with the new cognitive science. Moravec endorses a version of the Other Minds reply. It makes sense to attribute intentionality to machines for the same reasons it makes sense to attribute them to humans; his “interpretative position” is similar to Dennett’s view. Moravec goes on to note that one of the things we attribute to others is the ability to make attributions of intentionality, and then we make such attributions to ourselves. He holds that such self-representation is at the heart of consciousness. These capacities appear to be implementation independent, and hence possible for aliens and suitably programmed computers.

As we have seen, the reason that Searle thinks we can disregard the behavioral evidence in the case of robots and computers is that we know that their processing is syntactic, and this fact trumps all other considerations. Indeed, Searle believes this is the larger point that the Chinese Room merely illustrates. This larger point is addressed in the Syntax and Semantics section below.

4.5 The Intuition Reply
Many responses to the Chinese Room argument have noted that, as with Leibniz’ Mill, the argument appears to be based on intuition: the intuition that a computer (or the man in the room) cannot think or have understanding. For example, Ned Block (1980) in his original BBS commentary says “Searle’s argument depends for its force on intuitions that certain entities do not think.” But, Block argues, (1) intuitions sometimes can and should be trumped and (2) perhaps we need to bring our concept of understanding in line with a reality in which certain computer robots belong to the same natural kind as humans. Similarly Margaret Boden (1988) points out that we can’t trust our untutored intuitions about how mind depends on matter; developments in science may change our intuitions. Indeed, elimination of bias in our intuitions was precisely what motivated Turing (1950) to propose the Turing Test, a test that was blind to the physical character of the system replying to questions. Some of Searle’s critics in effect argue that he has merely pushed the reliance on intuition back, into the room.

For example, one can hold that despite Searle’s intuition that he would not understand Chinese while in the room, perhaps he is mistaken and does, albeit unconsciously. Hauser (2002) accuses Searle of Cartesian bias in his inference from “it seems to me quite obvious that I understand nothing” to the conclusion that I really understand nothing. (From “I can really clearly imagine myself existing without my body”, Descartes unsoundly inferred “I can exist without my body.”) Normally, if one understands English or Chinese, one knows that one does – but not necessarily. The man in the Chinese Room might lack the normal introspective awareness of understanding – but this, while abnormal, does not support the conclusion that he does not understand.

Critics of the CRA note that our intuitions about intelligence, understanding and meaning may all be unreliable. With regard to meaning, Wakefield 2003, following Block 1998, defends what Wakefield calls “the essentialist objection” to the CRA, namely that a computational account of meaning is not analysis of ordinary concepts and their related intuitions. Rather we are building a scientific theory of meaning that may require revising our intuitions. As a theory, it gets its evidence from its explanatory power, not its accord with pre-theoretic intuitions (however Wakefield himself argues that computational accounts of meaning are afflicted by a pernicious indeterminacy (pp. 308ff)).

Other critics focusing on the role of intuitions in the CRA argue that our intuitions regarding both intelligence and understanding may also be unreliable, and perhaps incompatible even with current science. With regard to understanding, Steven Pinker, in How the Mind Works (1997), holds that “… Searle is merely exploring facts about the English word understand…. People are reluctant to use the word unless certain stereotypical conditions apply…” But, Pinker claims, nothing scientifically speaking is at stake. Pinker objects to Searle’s appeal to the “causal powers of the brain” by noting that the apparent locus of the causal powers is the “patterns of interconnectivity that carry out the right information processing”. Pinker ends his discussion by citing a science fiction story in which Aliens, anatomically quite unlike humans, cannot believe that humans can really think once they discover that our heads are filled with meat. The Aliens’ intuitions are unreliable – presumably ours may be so as well.

Clearly the CRA turns on what is required to understand language. Schank 1978 clarifies his claim about what he thinks his programs can do: “By ‘understand’, we mean SAM [one of his programs] can create a linked causal chain of conceptualizations that represent what took place in each story.” This is a nuanced understanding of “understanding”, whereas the Chinese Room thought experiment does not turn on a technical understanding of “understanding”, but rather intuitions about our ordinary competence when we understand a word like “hamburger”. Indeed by 2015 Schank distances himself from weak senses of “understand”, holding that no computer can “understand when you tell it something”, and that IBM’s WATSON “doesn’t know what it is saying”. Schank’s program may get links right, but arguably does not know what the linked entities are. Whether it does or not depends on what concepts are, see section 5.1. Furthermore it is possible that when it comes to attributing understanding of language we have different standards for different things – more relaxed for dogs and toddlers. Some things understand a language “un poco”. Searle (1980)concedes that there are degrees of understanding, but says that all that matters that there are clear cases of no understanding, and AI programs are an example: “The computer understanding is not just (like my understanding of German) partial or incomplete; it is zero.”

Some defenders of AI are also concerned with how our understanding of understanding bears on the Chinese Room argument. In their paper “A Chinese Room that Understands” AI researchers Simon and Eisenstadt (2002) argue that whereas Searle refutes “logical strong AI”, the thesis that a program that passes the Turing Test will necessarily understand, Searle’s argument does not impugn “Empirical Strong AI” – the thesis that it is possible to program a computer that convincingly satisfies ordinary criteria of understanding. They hold however that it is impossible to settle these questions “without employing a definition of the term ‘understand’ that can provide a test for judging whether the hypothesis is true or false”. They cite W.V.O. Quine’s Word and Object as showing that there is always empirical uncertainty in attributing understanding to humans. The Chinese Room is a Clever Hans trick (Clever Hans was a horse who appeared to clomp out the answers to simple arithmetic questions, but it was discovered that Hans could detect unconscious cues from his trainer). Similarly, the man in the room doesn’t understand Chinese, and could be exposed by watching him closely. (Simon and Eisenstadt do not explain just how this would be done, or how it would affect the argument.) Citing the work of Rudolf Carnap, Simon and Eisenstadt argue that to understand is not just to exhibit certain behavior, but to use “intensions” that determine extensions, and that one can see in actual programs that they do use appropriate intensions. They discuss three actual AI programs, and defend various attributions of mentality to them, including understanding, and conclude that computers understand; they learn “intensions by associating words and other linguistic structure with their denotations, as detected through sensory stimuli”. And since we can see exactly how the machines work, “it is, in fact, easier to establish that a machine exhibits understanding that to establish that a human exhibits understanding….” Thus, they conclude, the evidence for empirical strong AI is overwhelming.

Similarly, Daniel Dennett in his original 1980 response to Searle’s argument called it “an intuition pump”, a term he came up with in discussing the CRA with Douglas Hofstader. Sharvy 1983 echoes the complaint. Dennett’s considered view (2013) is that the CRA is “clearly a fallacious and misleading argument ….” (p. 320). Paul Thagard (2013) proposes that for every thought experiment in philosophy there is an equal and opposite thought experiment. Thagard holds that intuitions are unreliable, and the CRA is an example (and that in fact the CRA has now been refuted by the technology of autonomous robotic cars). Dennett has elaborated on concerns about our intuitions regarding intelligence. Dennett 1987 (“Fast Thinking”) expressed concerns about the slow speed at which the Chinese Room would operate, and he has been joined by several other commentators, including Tim Maudlin, David Chalmers, and Steven Pinker. The operator of the Chinese Room may eventually produce appropriate answers to Chinese questions. But slow thinkers are stupid, not intelligent – and in the wild, they may well end up dead. Dennett argues that “speed … is ‘of the essence’ for intelligence. If you can’t figure out the relevant portions of the changing environment fast enough to fend for yourself, you are not practically intelligent, however complex you are” (326). Thus Dennett relativizes intelligence to processing speed relative to current environment.

Tim Maudlin (1989) disagrees. Maudlin considers the time-scale problem pointed to by other writers, and concludes, contra Dennett, that the extreme slowness of a computational system does not violate any necessary conditions on thinking or consciousness. Furthermore, Searle’s main claim is about understanding, not intelligence or being quick-witted. If we were to encounter extra-terrestrials that could process information a thousand times more quickly than we do, it seems that would show nothing about our own slow-poke ability to understand the languages we speak.

Steven Pinker (1997) also holds that Searle relies on untutored intuitions. Pinker endorses the Churchlands’ (1990) counterexample of an analogous thought experiment of waving a magnet and not generating light, noting that this outcome would not disprove Maxwell’s theory that light consists of electromagnetic waves. Pinker holds that the key issue is speed: “The thought experiment slows down the waves to a range to which we humans no longer see them as light. By trusting our intuitions in the thought experiment, we falsely conclude that rapid waves cannot be light either. Similarly, Searle has slowed down the mental computations to a range in which we humans no longer think of it as understanding (since understanding is ordinarily much faster)” (94–95). Howard Gardiner, a supporter of Searle’s conclusions regarding the room, makes a similar point about understanding. Gardiner addresses the Chinese Room argument in his book The Mind’s New Science (1985, 171–177). Gardiner considers all the standard replies to the Chinese Room argument and concludes that Searle is correct about the room: “…the word understand has been unduly stretched in the case of the Chinese room ….” (175).

Thus several in this group of critics argue that speed affects our willingness to attribute intelligence and understanding to a slow system, such as that in the Chinese Room. The result may simply be that our intuitions regarding the Chinese Room are unreliable, and thus the man in the room, in implementing the program, may understand Chinese despite intuitions to the contrary (Maudlin and Pinker). Or it may be that the slowness marks a crucial difference between the simulation in the room and what a fast computer does, such that the man is not intelligent while the computer system is (Dennett).

4.6 Advances in Artificial Intelligence
Even as late as 2001, Robert Damper [2001, Other Internet Resources) dismissed the CRA as useless, and possibly harmful, because “What Searle and others seem ready blithely to assume – the existence of a Chinese ‘understanding’ program able to pass the Turing test … – is so far beyond the current capabilities of AI and computer technology as to amount to science fiction. What could we possibly learn from such a fanciful conception? There is no realistic way of resolving any paradoxes which arise, save appeals to common sense, and we know from the example of quantum mechanics how fallible this is.” And in 2015 Steven Pinker remarked “Human-level AI is still the standard 15-to-25 years away, just as it always has been….”

Searle’s argument was developed in the late 1970s, little more than 20 years after transistorized computers were introduced, as well as the first AI conference (1956). In the many decades since then, there have been enormous advances in areas relevant to the CRA and many of the replies: computing speed and power, robotics, artificial intelligence, neural networks, and to the point, natural language processing.

In late 2022, AI systems based on large language models (LLMs) received wide attention, from academics to their essay-writing students, as well as many other professions in which language proficiency was important. Whereas Shank’s program and database were hand-built, so that, (once debugged!) their highly limited output of a sentence or two about restaurants could rarely, if ever, surprise the programmers, LLM systems crawl the world wide web and can generate paragraph after paragraph that may be all news to their coders.

Does this make any difference to the CRA and its replies? Sabine Hossenfelder (2023) argues that these chatbots understand some of what they say, namely they understand in the same sense that humans understand quantum mechanics. We can understand the equations well enough to make predictions, but we do not have a deep understanding of why the equations are what they are. Jensen Huang (2024 [Other Internet Resources]), CEO of AI chip maker Nvidia, see no such limits: “Generative AI is the most impactful invention of our time, and as with electricity and the internet, it impacts everyone and every industry. … LLMs, learned to understand human language, prior knowledge, and even some common sense from massive amounts of text.”

There is no doubt that AI agents now have a way with words, and can present useful information. If those models can diagnose disease better than diagnosticians, write essays better than essayists, argue law better than lawyers, and answer questions better than teachers, then it appears to be sad sour grapes to say “but they don’t understand anything”. So has AI progress refuted Searle? Asked in 2024 “Do you understand English words”, ChatGPT itself responds “Yes, I understand English words and can process and respond to them. If you have any questions or need help with something specific, just let me know!” It is loquacious if asked what a hamburger is, Searle’s example of something a natural language program cannot understand. It readily explains the difference between a hamburger and the word “hamburger”. But when pressed, and asked HOW it understands English, ChatGPT responds “I understand English semantics through a combination of patterns and context.” When asked directly, “John Searle argues you cannot get semantics from syntax – is he correct?” the response is “Searle’s argument highlights a crucial limitation: syntax alone is insufficient for fully capturing semantics. While syntactic rules provide a framework for constructing sentences, understanding meaning involves additional layers of context, interpretation, and often real-world knowledge.” And to “Doesn’t Searle’s argument apply to ChatGPT?” we get “Yes, John Searle’s argument does apply to systems like ChatGPT. … ChatGPT uses patterns and rules from the vast amounts of text it has been trained on to generate responses. It operates based on statistical correlations between words and phrases, mimicking syntactic structures. … ChatGPT doesn’t truly understand meaning in the human sense. It generates text that appears to be meaningful based on the patterns it has learned, but it lacks genuine comprehension of the content or context beyond what it has been trained to predict.”

So, paradoxically, the system appears to understand that it doesn’t understand. However it fails to notice that these claims are inconsistent with its output made just minutes before. So we are not there yet. A system that answers questions with carefully argued denials that it can understand anything is not going to pass the Turing Test. However this may get fixed in the future, such that LLM systems will avoid what has been called “locutionary suicide” (e.g. “Here’s the answer to your question, but I don’t understand your question”). The consequence may be that if systems that use language just as proficiently as humans in all contexts are argued by a thought experiment to not really understand, then we can’t test for “real” understanding, and it adds nothing to the purposes for which we use language. “Really” understanding becomes epiphenomenal: undetectable, causally inert, and irrelevant.

5. The Larger Philosophical Issues
5.1 Syntax and Semantics
Searle believes the Chinese Room thought experiment supports a larger point, which explains the failure of the Chinese Room to produce understanding. Searle argued that programs implemented by computers are just syntactical. Computer operations are “formal” in that they respond only to the physical form of the strings of symbols, not to the meaning of the symbols. Minds on the other hand have states with meaning, mental contents. We associate meanings with the words or signs in language. We respond to signs because of their meaning, not just their physical appearance. In short, we understand. But, and according to Searle this is the key point, “Syntax is not by itself sufficient for, nor constitutive of, semantics.” So although computers may be able to manipulate syntax to produce appropriate responses to natural language input, they do not understand the sentences they receive or output, for they cannot associate meanings with the words.

Searle (1984) presents a three premise argument that because syntax is not sufficient for semantics, programs cannot produce minds.

Programs are purely formal (syntactic).
Human minds have mental contents (semantics).
Syntax by itself is neither constitutive of, nor sufficient for, semantic content.
Therefore, programs by themselves are not constitutive of nor sufficient for minds.
The Chinese Room thought experiment itself is the support for the third premise. The claim that syntactic manipulation is not sufficient for meaning or thought is a significant issue, with wider implications than AI, or attributions of understanding. Prominent theories of mind hold that human cognition generally is computational. In one form, it is held that thought involves operations on symbols in virtue of their physical properties. On an alternative connectionist account, the computations are on “subsymbolic” states. If Searle is right, not only Strong AI but also these main approaches to understanding human cognition are misguided.

As we have seen, Searle holds that the Chinese Room scenario shows that one cannot get semantics from syntax alone. In a symbolic logic system, a kind of artificial language, rules are given for syntax. A semantics, if any, comes later. The logician first specifies the basic symbol set and some rules for manipulating strings to produce new ones (“well-formed fomulas”). These rules are purely syntactic – they are applied to strings of symbols solely in virtue of their syntax or form. A semantics, if any, for the symbol system must be provided separately. And if one wishes to show that interesting additional relationships hold between the syntactic operations and semantics, such as that the symbol manipulations preserve truth, one must provide somewhat complex meta-proofs to show this. So on the face of it, semantics is quite independent of syntax for artificial languages, and one cannot get semantics from syntax alone. “Formal symbols by themselves can never be enough for mental contents, because the symbols, by definition, have no meaning (or interpretation, or semantics) except insofar as someone outside the system gives it to them” (Searle 1989, 45).

Searle’s identification of meaning with interpretation in this passage is important. Searle’s point is clearly true of the causally inert formal systems of logicians. A semantic interpretation has to be given to those symbols by a logician. When we move from formal systems to computational systems, the situation is more complex. As many of Searle’s critics (e.g. Cole 1984, Dennett 1987, Boden 1988, and Chalmers 1996) have noted, a computer running a program is not the same as “syntax alone”. A computer is an enormously complex electronic causal system (some now have transistor counts that are comparable to the number of neurons in a human brain). State changes in the system are physical. One can interpret the physical states, e.g. voltages, as syntactic 1’s and 0’s, but the intrinsic reality is electronic and syntax is “derived”, a product of someone else’s interpretation. The states are syntactically specified by programmers, but when implemented in a running machine they are electronic states of a complex causal system directly or indirectly embedded in the real world. This is quite different from the abstract formal systems that logicians study. Dennett notes that no “computer program by itself” (Searle’s language) – e.g. a program lying on a shelf – can cause anything, even simple addition, let alone mental states. The program must be running. Chalmers (1996) offers a parody in which it is reasoned that recipes are syntactic, syntax is not sufficient for crumbliness, cakes are crumbly, so implementation of a recipe is not sufficient for making a cake. Implementation makes all the difference; an abstract entity (recipe, program) determines the causal powers of a physical system embedded in the larger causal nexus of the world.

Dennett (1987) sums up the issue: “Searle’s view, then, comes to this: take a material object (any material object) that does not have the power of causing mental phenomena; you cannot turn it in to an object that does have the power of producing mental phenomena simply by programming it – reorganizing the conditional dependencies of transitions between its states.” Dennett’s view is the opposite: programming “is precisely what could give something a mind”. But Dennett claims that in fact it is “empirically unlikely that the right sorts of programs can be run on anything but organic, human brains” (325–6).

A computer does not recognize that its binary data strings have a certain form, and thus that certain syntactic rules may be applied to them, unlike the man inside the Chinese Room. Inside a computer, there is nothing that literally reads input data, or that “knows” what symbols are. Instead, there are millions of transistors that change states. A sequence of voltages causes operations to be performed. We humans may choose to interpret these voltages as binary numerals and the voltage changes as syntactic operations, but a computer does not interpret its operations as syntactic or any other way. So perhaps a computer does not need to make the move from syntax to semantics that Searle objects to; it needs to move from complex causal connections to semantics. Furthermore, perhaps any causal system is describable as performing syntactic operations – if we interpret a light square as logical “0” and a dark square as logical “1”, then a kitchen toaster may be described as a device that rewrites logical “0”s as logical “1”s. But there is no philosophical problem about getting from syntax to breakfast.

In the 1990s, Searle began to use considerations related to these to argue that computational views are not just false, but lack a clear sense. Computation, or syntax, is “observer-relative”, not an intrinsic feature of reality: “…you can assign a computational interpretation to anything” (Searle 2002b, p. 17), even the molecules in the paint on the wall. Since nothing is intrinsically computational, one cannot have a scientific theory that reduces the mental, which is not observer-relative, to computation, which is. “Computation exists only relative to some agent or observer who imposes a computational interpretation on some phenomenon. This is an obvious point. I should have seen it ten years ago, but I did not.” (Searle 2002b, p.17, originally published 1993).

Critics note that walls are not computers; unlike a wall, a computer goes through state-transitions that are counterfactually described by a program (Chalmers 1996, Block 2002, Haugeland 2002). In his 2002 paper, Block addresses the question of whether a wall is a computer (in reply to Searle’s charge that anything that maps onto a formal system is a formal system, whereas minds are quite different). Block denies that whether or not something is a computer depends entirely on our interpretation. Block notes that Searle ignores the counterfactuals that must be true of an implementing system. Haugeland (2002) makes the similar point that an implementation will be a causal process that reliably carries out the operations – and they must be the right causal powers. Block concludes that Searle’s arguments fail, but he concedes that they “do succeed in sharpening our understanding of the nature of intentionality and its relation to computation and representation” (78).

Rey (2002) also addresses Searle’s arguments that syntax and symbols are observer-relative properties, not physical. Searle infers this from the fact that syntactic properties (e.g. being a logical “1”) are not defined in physics; however Rey holds that it does not follow that they are observer-relative. Rey argues that Searle also misunderstands what it is to realize a program. Rey endorses Chalmers’ reply to Putnam: a realization is not just a structural mapping, but involves causation, supporting counterfactuals. “This point is missed so often, it bears repeating: the syntactically specifiable objects over which computations are defined can and standardly do possess a semantics; it’s just that the semantics is not involved in the specification.” States of a person have their semantics in virtue of computational organization and their causal relations to the world. Rey concludes: Searle “simply does not consider the substantial resources of functionalism and Strong AI.” (222) A plausibly detailed story would defuse negative conclusions drawn from the superficial sketch of the system in the Chinese Room.

John Haugeland (2002) argues that there is a sense in which a processor must intrinsically understand the commands in the programs it runs: it executes them in accord with the specifications. “The only way that we can make sense of a computer as executing a program is by understanding its processor as responding to the program prescriptions as meaningful” (385). Thus operation symbols have meaning to a system. Haugeland goes on to draw a distinction between narrow and wide system. He argues that data can have semantics in the wide system that includes representations of external objects produced by transducers. In passing, Haugeland makes the unusual claim, argued for elsewhere, that genuine intelligence and semantics presuppose “the capacity for a kind of commitment in how one lives” which is non-propositional – that is, love (compare Steven Spielberg’s 2001 film Artificial Intelligence: AI).

To Searle’s claim that syntax is observer-relative, that the molecules in a wall might be interpreted as implementing the Wordstar program (an early word processing program) because “there is some pattern in the molecule movements which is isomorphic with the formal structure of Wordstar” (Searle 1990b, p. 27), Haugeland counters that “the very idea of a complex syntactical token … presupposes specified processes of writing and reading….” The tokens must be systematically producible and retrievable. So no random isomorphism or pattern somewhere (e.g. on some wall) is going to count, and hence syntax is not observer-relative.

With regard to the question of whether one can get semantics from syntax, William Rapaport has for many years argued for “syntactic semantics”, a view in which understanding is a special form of syntactic structure in which symbols (such as Chinese words) are linked to concepts, themselves represented syntactically. Others believe we are not there yet. AI futurist (The Age of Spiritual Machines) Ray Kurzweil holds in a 2002 follow-up book that it is red herring to focus on traditional symbol-manipulating computers. Kurzweil agrees with Searle that existent computers do not understand language – as evidenced by the fact that they can’t engage in convincing dialog. But that failure does not bear on the capacity of future computers based on different technology. Kurzweil claims that Searle fails to understand that future machines will use “chaotic emergent methods that are massively parallel”. This claim appears to be similar to that of connectionists, such as Andy Clark, and the position taken by the Churchlands in their 1990 Scientific American article.

Apart from Haugeland’s claim that processors understand program instructions, Searle’s critics can agree that computers no more understand syntax than they understand semantics, although, like all causal engines, a computer has syntactic descriptions. And while it is often useful to programmers to treat the machine as if it performed syntactic operations, it is not always so: sometimes the characters programmers use are just switches that make the machine do something, for example, make a given pixel on the computer display turn red, or make a car transmission shift gears. Thus it is not clear that Searle is correct when he says a digital computer is just “a device which manipulates symbols”. Computers are complex causal engines, and syntactic descriptions are useful in order to structure the causal interconnections in the machine. AI programmers face many tough problems, but one can hold that they do not have to get semantics from syntax. If they are to get semantics, they must get it from causality.

Two main approaches have developed that explain meaning in terms of causal connections. The internalist approaches, such as Schank’s and Rapaport’s conceptual representation approaches, and also Conceptual Role Semantics, hold that a state of a physical system gets its semantics from causal connections to other states of the same system. Thus a state of a computer might represent “kiwi” because it is connected to “bird” and “flightless” nodes, and perhaps also to images of prototypical kiwis. The state that represents the property of being “flightless” might get its content from a Negation-operator modifying a representation of “capable of airborne self-propulsion”, and so forth, to form a vast connected conceptual network, a kind of mental dictionary.

Externalist approaches developed by Dennis Stampe, Fred Dretske, Hilary Putnam, Jerry Fodor, Ruth Millikan, and others, hold that states of a physical system get their content through causal connections to the external reality they represent. Thus, roughly, a system with a KIWI concept is a system that has a state it uses to represent the presence of kiwis in the external environment. This kiwi-representing state will be a state that is appropriately causally connected to the presence of kiwis. Depending on the system, the kiwi representing state could be a state of a brain, or of an electrical device such as a computer, or even of a hydraulic system. The internal representing state can in turn play a causal role in determining the behavior of the system. For example, Rey (1986) endorses an indicator semantics along the lines of the work of Dennis Stampe (1977) and Fodor’s Psychosemantics. These semantic theories that locate content or meaning in appropriate causal relations to the world fit well with the Robot Reply. A computer in a robot body might have just the causal connections that could allow its inner syntactic states to have the semantic property of representing states of things in its environment.

Thus there are at least two families of theories (and marriages of the two, as in Block 1986) about how semantics might depend upon causal connections. Both of these attempt to provide accounts that are implementation neutral: states of suitably organized causal systems can have content, no matter what the systems are made of. On these theories a computer could have states that have meaning. It is not necessary that the computer be aware of its own states and know that they have meaning, nor that any outsider appreciate the meaning of the states. On either of these accounts meaning depends upon the (possibly complex) causal connections, and digital computers are systems designed to have states that have just such complex causal dependencies. It should be noted that Searle does not subscribe to these theories of semantics. Instead, Searle’s discussions of linguistic meaning have often centered on the notion of intentionality.

5.2 Intentionality
Intentionality is the property of being about something, having content. In the 19th Century, psychologist Franz Brentano re-introduced this term from Medieval philosophy and held that intentionality was the “mark of the mental”. Beliefs and desires are intentional states: they have propositional content (a person never just believes or desires, they believe that p, or desire that p, where sentences or clauses that represent propositions substitute for “p”). Searle’s views regarding intentionality are complex; of relevance here is that he makes a distinction between the original or intrinsic intentionality of genuine mental states, and the derived intentionality of language. A written or spoken sentence only has intentionality, namely derived intentionality, insofar as it is interpreted by someone. It appears that on Searle’s view, original intentionality must at least potentially be conscious. Searle then argues that the distinction between original and derived intentionality applies to computers. We can interpret the states of a computer as having content, but the states themselves do not have original intentionality. Many philosophers endorse this intentionality dualism, including Sayre (1986) and even Fodor (2009), despite Fodor’s many differences with Searle.

In a section of her 1988 book, Computer Models of the Mind, Margaret Boden notes that intentionality is not well-understood – reason to not put too much weight on arguments that turn on intentionality. Furthermore, insofar as we understand the brain, we focus on informational functions, not unspecified causal powers of the brain: “…from the psychological point of view, it is not the biochemistry as such which matters but the information-bearing functions grounded in it.” (241) Searle sees intentionality as a causal power of the brain, uniquely produced by biological processes. Dale Jacquette 1989 argues against a reduction of intentionality – intentionality, he says, is an “ineliminable, irreducible primitive concept.” However, most AI sympathizers have seen intentionality, aboutness, as bound up with information, and non-biological states can carry information just as well as can brain states. Hence many responders to Searle have argued that he displays substance chauvinism, in holding that brains understand but systems made of silicon with comparable information processing capabilities cannot, even in principle. Papers on both sides of the issue appeared, such as J. Maloney’s 1987 paper “The Right Stuff”, defending Searle, and R. Sharvy’s 1983 critique, “It Ain’t the Meat, it’s the Motion”. AI proponents such as Kurzweil (1999, see also Richards 2002) have continued to hold that AI systems can potentially have such mental properties as understanding, intelligence, consciousness and intentionality, and will exceed human abilities in these areas.

Other critics of Searle’s position take intentionality more seriously than Boden does, but deny his dualistic distinction between original and derived intentionality. Dennett (1987, e.g.) argues that all intentionality is derived, in that attributions of intentionality – to animals, other people, and even ourselves – are purely instrumental and allow us to predict behavior, but they are not descriptions of intrinsic properties. As we have seen, Dennett is concerned about the slow speed of things in the Chinese Room, but he argues that once a system is working up to speed, it has all that is needed for a mind with derived intentionality – and derived intentionality is the only kind that there is, according to Dennett. A machine can be an intentional system because intentional explanations work in predicting the machine’s behavior. Dennett also suggests that Searle conflates intentionality with awareness of intentionality. In his syntax-semantic arguments, “Searle has apparently confused a claim about the underivability of semantics from syntax with a claim about the underivability of the consciousness of semantics from syntax” (336). The emphasis on consciousness forces us to think about things from a first-person point of view, but Dennett 2017 continues to press the claim that this is a fundamental mistake if we want to understand the mental.

We might also worry that Searle conflates meaning and interpretation, and that Searle’s original or underived intentionality is just second-order intentionality, a representation of what an intentional object represents or means. Dretske and others have seen intentionality as information-based. One state of the world, including a state in a computer, may carry information about other states in the world, and this informational aboutness is a mind-independent feature of states. Hence it is a mistake to hold that conscious attributions of meaning are the source of intentionality.

Others have noted that Searle’s discussion has shown a shift over time from issues of intentionality and understanding to issues of consciousness. Searle links intentionality to awareness of intentionality, in holding that intentional states are at least potentially conscious. In his 1996 book, The Conscious Mind, David Chalmers notes that although Searle originally directs his argument against machine intentionality, it is clear from later writings that the real issue is consciousness, which Searle holds is a necessary condition of intentionality. It is consciousness that is lacking in digital computers. Chalmers uses thought experiments to argue that it is implausible that one system has some basic mental property (such as having qualia) that another system lacks, if it is possible to imagine transforming one system into the other, either gradually (as replacing neurons one at a time by digital circuits), or all at once, switching back and forth between flesh and silicon (see the brief discussion of cyborgization in section 4.3 above).

A second strategy regarding the attribution of intentionality is taken by critics who in effect argue that intentionality is an intrinsic feature of states of physical systems that are causally connected with the world in the right way, independently of interpretation (see the preceding Syntax and Semantics section). For example, a photo of Turing has intentionality: it has content about something, namely Turing. This form of intentionality is independent of interpretation – someone can look at a photo of Turing and think it is a photo of someone else. The same would presumably be the case with a sentence generated by a robot such as “I am now in the clock-room”. That sentence is about a specific robot in virtue of causal connection between the generation of the sentence and the location of the robot. When the robot generates that sentence, it means that the robot is in a room it calls “clock-room”. But someone might interpret and assign the wrong intentionality to it (e.g. they might think it is about some other robot than it actually is about). On this way of thinking about it, intentionality is one thing, and interpretation is something else, namely interpretation is a theory or hypothesis about something’s intentionality. The intentionality of the sentence or photo is its relation to the world; the interpretation of a sentence is second-order intentionality, namely it is about the sentence and its intentionality.

Fodor’s semantic externalism is influenced by Fred Dretske, but they come to different conclusions with regard to the semantics of states of computers. Over a period of years, Dretske developed an historical account of meaning or mental content that would preclude attributing beliefs and understanding to most machines. Dretske (1985) agrees with Searle that adding machines don’t literally add; we do the adding, using the machines. Dretske emphasizes the crucial role of natural selection and learning in producing states that have genuine content. Human built systems will be, at best, like Swampmen (beings that result from a lightning strike in a swamp and by chance happen to be a molecule by molecule copy of some human being, say, you) – they appear to have intentionality or mental states, but do not, because such states require the right history. AI states will generally be counterfeits of real mental states; like counterfeit money, they may appear perfectly identical but lack the right pedigree. But Dretske’s account of belief appears to make it distinct from conscious awareness of the belief or intentional state (if that is taken to require a higher order thought), and so would apparently allow attribution of intentionality to artificial systems that can get the right history by learning.

Howard Gardiner endorses Zenon Pylyshyn’s criticisms of Searle’s view of the relation of brain and intentionality, as supposing that intentionality is somehow a stuff “secreted by the brain”, and Pylyshyn’s own counter-thought experiment in which one’s neurons are replaced one by one with integrated circuit workalikes (see also Cole and Foelber (1984) and Chalmers (1996) for exploration of neuron replacement scenarios). Gardiner holds that Searle owes us a more precise account of intentionality than Searle has given so far, and until then it is an open question whether AI can produce it, or whether it is beyond its scope. Gardiner concludes with the possibility that the dispute between Searle and his critics is not scientific, but (quasi?) religious.

5.3 Mind and Body
Several critics have noted that there are metaphysical issues at stake in the original argument. The Systems Reply draws attention to the metaphysical problem of the relation of mind to body. It does this in holding that understanding is a property of the system as a whole, not the physical implementer. The Virtual Mind Reply holds that minds or persons – the entities that understand and are conscious – are more abstract than any physical system, and that there could be a many-to-one relation between minds and physical systems. (Even if everything is physical, in principle a single body could be shared by multiple minds, and a single mind could have a sequence of bodies over time.) Thus larger issues about personal identity and the relation of mind and body are in play in the debate between Searle and some of his critics.

Searle’s view is that the problem of the relation of mind and body “has a rather simple solution. Here it is: Conscious states are caused by lower level neurobiological processes in the brain and are themselves higher level features of the brain” (Searle 2002b, p. 9). In his early discussion of the CRA, Searle spoke of the causal powers of the brain. Thus his view appears to be that brain states cause consciousness and understanding, and “consciousness is just a feature of the brain” (ibid). However, as we have seen, even if this is true it begs the question of just whose consciousness a brain creates. Roger Sperry’s split-brain experiments suggest that perhaps there can be two centers of consciousness, and so in that sense two minds, implemented by a single brain. While both display at least some language comprehension, only one (typically created by the left hemisphere) controls language production. Thus many current approaches to understanding the relation of brain and consciousness emphasize connectedness and information flow (see e.g. Dehaene 2014).

Consciousness and understanding are features of persons, so it appears that Searle accepts a metaphysics in which I, my conscious self, am identical with my brain – a form of mind-brain identity theory. This very concrete metaphysics is reflected in Searle’s original presentation of the CR argument, in which Strong AI was described by him as the claim that “the appropriately programmed computer really is a mind” (Searle 1980). This is an identity claim, and has odd consequences. If A and B are identical, any property of A is a property of B. Computers are physical objects. Some computers weigh 6 lbs and have stereo speakers. So the claim that Searle called Strong AI would entail that some minds weigh 6 lbs and have stereo speakers. However it seems to be clear that while humans may weigh 150 pounds; human minds do not weigh 150 pounds. This suggests that neither bodies nor machines can literally be minds. Such considerations support the view that minds are more abstract than brains, and if so that at least one version of the claim that Searle calls Strong AI, the version that says that computers literally are minds, is metaphysically untenable on the face of it, apart from any thought-experiments.

If minds are not physical objects this inability of a computer to be a mind does not show that running an AI program cannot produce understanding of natural language, by something other than the computer (See section 4.1 above.)

Functionalism is a theory of the relation of minds to bodies that was developed in the two decades prior to Searle’s CRA. Functionalism is an alternative to the identity theory that is implicit in much of Searle’s discussion, as well as to the dominant behaviorism of the mid-twentieth Century. If functionalism is correct, there appears to be no intrinsic reason why a computer couldn’t have mental states. Hence the CRA’s conclusion that a computer is intrinsically incapable of mental states is an important consideration against functionalism. Julian Baggini (2009, 37) writes that Searle “came up with perhaps the most famous counter-example in history – the Chinese room argument – and in one intellectual punch inflicted so much damage on the then dominant theory of functionalism that many would argue it has never recovered.”

Functionalists hold that a mental state is what a mental state does – the causal (or “functional”) role that the state plays determines what state it is. A functionalist might hold that pain, for example, is a state that is typically caused by damage to the body, is located in a body-image, and is aversive. Functionalists distance themselves both from behaviorists and identity theorists. In contrast with the former, functionalists hold that the internal causal processes are important for the possession of mental states. Thus functionalists may agree with Searle in rejecting the Turing Test as too behavioristic. In contrast with identity theorists (who might e.g. hold “pain is identical with C-fiber firing”), functionalists hold that mental states might be had by a variety of physical systems (or non-physical, as in Cole and Foelber 1984, in which a mind changes from a material to an immaterial implementation, neuron by neuron). Thus while an identity theorist will identify pain with certain neuron firings, a functionalist will identify pain with something more abstract and higher level, a functional role that might be had by many different types of underlying system.

Functionalists accuse identity theorists of substance chauvinism. However, functionalism remains controversial: functionalism is vulnerable to the Chinese Nation type objections discussed above, and functionalists notoriously have trouble explaining qualia, a problem highlighted by the apparent possibility of an inverted spectrum, where qualitatively different states might have the same functional role (e.g. Block 1978, Maudlin 1989, Cole 1990).

Computationalism is the sub-species of functionalism that holds that the important causal role of brain processes is information processing. Milkowski 2017 notes that computational approaches have been fruitful in cognitive science; he surveys objections to computationalism and concludes that the majority target a strawman version. However Jerry Fodor, an early proponent of computational approaches, argues in Fodor 2005 that key mental processes, such as inference to the best explanation, which depend on non-local properties of representations, cannot be explained by computational modules in the brain. If Fodor is right, understanding language and interpretation appear to involve global considerations such as linguistic and non-linguistic context and theory of mind and so might resist computational explanation. If so, we reach Searle’s conclusion on the basis of different considerations.

Searle’s 2010 statement of the conclusion of the CRA has it showing that computational accounts cannot explain consciousness. There has been considerable interest in the decades since 1980 in determining what does explain consciousness, and this has been an extremely active research area across disciplines. One interest has been in the neural correlates of consciousness. This bears directly on Searle’s claim that consciousness is intrinsically biological and not computational or information processing. There is no definitive answer yet, though some recent work on anesthesia suggests that consciousness is lost when cortical (and cortico-thalamic) connections and information flow are disrupted (e.g., Hudetz 2012, a review article).

In general, if the basis of consciousness is confirmed to be at the relatively abstract level of information flow through neural networks, it will be friendly to functionalism, and if it is turns out to be lower and more biological (or sub-neuronal), it will be friendly to Searle’s account.

5.4 Simulation, duplication and evolution
In discussing the CRA, Searle argues that there is an important distinction between simulation and duplication. No one would mistake a computer simulation of the weather for weather, or a computer simulation of digestion for real digestion. Searle concludes that it is just as serious a mistake to confuse a computer simulation of understanding with understanding.

On the face of it, there is generally an important distinction between a simulation and the real thing. But two problems emerge. It is not clear that the distinction can always be made. Hearts are biological if anything is. Are artificial hearts simulations of hearts? Or are they functional duplicates of hearts, hearts made from different materials? Walking is normally a biological phenomenon performed using limbs. Do those with artificial limbs walk? Or do they simulate walking? Do robots walk? If the properties that are needed to be a certain kind of thing are high-level properties, anything sharing those properties will be a thing of that kind, even if it differs in its lower level properties. Chalmers (1996) offers a principle governing when simulation is replication. Chalmers suggests that, contra Searle and Harnad (1989), a simulation of X can be an X, namely when the property of being an X is an organizational invariant, a property that depends only on the functional organization of the underlying system, and not on any other details.

Copeland (2002) argues that the Church-Turing thesis does not entail that the brain (or every machine) can be simulated by a universal Turing machine, for the brain (or other machine) might have primitive operations that are not simple clerical routines that can be carried out by hand. (An example might be that human brains likely display genuine low-level randomness, whereas computers are carefully designed not to do that, and so computers resort to pseudo-random numbers when apparent randomness is needed.) Sprevak 2007 raises a related point. Turing’s 1938 Princeton thesis described such machines (“O-machines”). O-machines are machines that include functions of natural numbers that are not Turing-machine computable. If the brain is such a machine, then, says Sprevak,: “There is no possibility of Searle’s Chinese Room Argument being successfully deployed against the functionalist hypothesis that the brain instantiates an O-machine….” (120).

Copeland discusses the simulation / duplication distinction in connection with the Brain Simulator Reply. He argues that Searle correctly notes that one cannot infer from X simulates Y, and Y has property P, to the conclusion that therefore X has Y’s property P for arbitrary P. But Copeland claims that Searle himself commits the simulation fallacy in extending the CR argument from traditional AI to apply against computationalism. The contrapositive of the inference is logically equivalent – X simulates Y, X does not have P therefore Y does not – where P equals: understands Chinese. The faulty step is: the CR operator S simulates a neural net N, it is not the case that S understands Chinese, therefore it is not the case that N understands Chinese. Copeland also notes results by Siegelmann and Sontag (1994) showing that some connectionist networks cannot be simulated by a universal Turing Machine (in particular, where connection weights are real numbers).

There is another problem with the simulation-duplication distinction, arising from the process of evolution. Searle wishes to see original intentionality and genuine understanding as properties only of certain biological systems, presumably the product of evolution. Computers merely simulate these properties. At the same time, in the Chinese Room scenario, Searle maintains that a system can exhibit behavior just as complex as human behavior, simulating any degree of intelligence and language comprehension that one can imagine, and simulating any ability to deal with the world, yet not understand a thing. He also says that such behaviorally complex systems might be implemented with very ordinary materials, for example with tubes of water and valves.

This creates a biological problem, beyond the Other Minds problem noted by early critics of the CR argument. While we may presuppose that others have minds, evolution makes no such presuppositions. The selection forces that drive biological evolution select on the basis of behavior. Evolution can select for the ability to use information about the environment creatively and intelligently, as long as this is manifest in the behavior of the organism. If there is no overt difference in behavior in any set of circumstances between a system that understands and one that does not, evolution cannot select for genuine understanding. And so it seems that on Searle’s account, minds that genuinely understand meaning have no advantage over creatures that merely process information, using purely computational processes. Thus a position that implies that simulations of understanding can be just as biologically adaptive as the real thing, leaves us with a puzzle about how and why systems with “genuine” understanding could evolve. Original intentionality and genuine understanding become epiphenomenal:

Man to robot companion: “It is sad that you understand nothing”.

Robot companion: “I know, I know. An American philosopher proved ages ago that I never will, so nothing can be done about that. But let’s set that sad thought aside and return to our discussion of the unreliable narrator in Bronte’s works that we were having, and your own trip to the Yorkshire moors. There are some lovely areas there, as I can see using my remote cam. I haven’t read all her novels, but am familiar with …”.

Conclusion
As we have seen, since its appearance in 1980 the Chinese Room argument has sparked discussion across disciplines. Despite the extensive discussion there is still no consensus as to whether the argument is sound. At one end we have Julian Baggini’s (2009) assessment that Searle “came up with perhaps the most famous counter-example in history – the Chinese room argument – and in one intellectual punch inflicted so much damage on the then dominant theory of functionalism that many would argue it has never recovered.” Whereas philosopher Daniel Dennett (2013, p. 320) concludes that the Chinese Room argument is “clearly a fallacious and misleading argument”. Hence there is no consensus as to whether the argument is a proof that limits the aspirations of Artificial Intelligence or computational accounts of mind.

Meanwhile work in artificial intelligence and natural language processing has continued. The CRA led Stevan Harnad and others on a quest for “symbol grounding” in AI. Many in philosophy (Dretske, Fodor, Millikan) worked on naturalistic theories of mental content. Speculation about the nature of consciousness continues in many disciplines. And computers have moved from the lab to the pocket and the wrist.

At the time of Searle’s construction of the argument, personal computers were very limited hobbyist devices. Weizenbaum’s ‘Eliza’ and a few text ‘adventure’ games were played on DEC computers; these included limited parsers. More advanced parsing of language was limited to computer researchers such as Schank. Much changed in the next quarter century; billions now use natural language to interrogate and command virtual agents via computers they carry in their pockets. Has the Chinese Room argument moderated claims by those who produce AI and natural language systems? Some manufacturers linking devices to the “internet of things” make modest claims: appliance manufacturer LG says the second decade of the 21st century brings the “experience of conversing” with major appliances. That may or may not be the same as conversing. Apple is less cautious than LG in describing the capabilities of its “virtual personal assistant” application called ‘Siri’: Apple says of Siri that “It understands what you say. It knows what you mean.” IBM is quick to claim its much larger ‘Watson’ system is superior in language abilities to Siri. In 2011 Watson beat human champions on the television game show ‘Jeopardy’, a feat that relies heavily on language abilities and inference. IBM goes on to claim that what distinguishes Watson is that it “knows what it knows, and knows what it does not know.” This appears to be claiming a form of reflexive self-awareness or consciousness for the Watson computer system. Thus the claims of strong AI now are hardly chastened, and if anything some are stronger and more exuberant. At the same time, as we have seen, many others believe that the Chinese Room Argument showed once and for all that at best computers can simulate human cognition.

Though separated by three centuries, Leibniz and Searle had similar intuitions about the systems they consider in their respective thought experiments, Leibniz’ Mill and the Chinese Room. In both cases they consider a complex system composed of relatively simple operations, and note that it is impossible to see how understanding or consciousness could result. These simple arguments do us the service of highlighting the serious problems we face in understanding meaning and minds. The many issues raised by the Chinese Room argument may not be settled until there is a consensus about the nature of meaning, its relation to syntax, and about the biological basis of consciousness. There continues to be significant disagreement about what processes create meaning, understanding, and consciousness, as well as what can be proven a priori by thought experiments.

-----------------


STUART KAUFFMAN1,* and PHILIP CLAYTON2
1
The Institute for Biocomplexity and Informatics, The University of Calgary, Calgary, AB T2N 1N4,
Canada; 2
Philosophy, Claremont Graduate University and CST, 1325 N. College Avenue, Claremont,
CA, 91711, USA; *Author for correspondence (e-mails: skauffman@ucalgary.ca, skauff@telus.net;
phone: +403-220-8349; fax: +403-242-8771)
Received 16 July 2004; accepted in revised form 26 July 2005
Key words: Autocatalysis, Autonomous agents, Emergence, Preadaptation, Reductionism, Theory
of organization, Semiotics, Teleology, Underdetermination of biology by physics, Work cycle
Abstract. Ultimately we will only understand biological agency when we have developed a
theory of the organization of biological processes, and science is still a long way from attaining
that goal. It may be possible nonetheless to develop a list of necessary conditions for the
emergence of minimal biological agency. The authors offer a model of molecular autonomous
agents which meets the five minimal physical conditions that are necessary (and, we believe,
conjointly sufficient) for applying agential language in biology: autocatalytic reproduction; work
cycles; boundaries for reproducing individuals; self-propagating work and constraint construction; and choice and action that have evolved to respond to food or poison. When combined
with the arguments from preadaptation and multiple realizability, the existence of these agents is
sufficient to establish ontological emergence as against what one might call Weinbergian
reductionism. Minimal biological agents are emphatically not conscious agents, and accepting
their existence does not commit one to any robust theory of human agency. Nor is there
anything mystical, dualistic, or non-empirical about the emergence of agency in the biosphere.
Hence the emergence of molecular autonomous agents, and indeed ontological emergence in
general, is not a negation of or limitation on careful biological study but simply one of its
implications.
An organized being is then not a mere machine, for that has merely
moving power, but it possesses in itself formative power of a self-propagating kind which it communicates to its materials though they have it
not of themselves; it organizes them, in fact, and this cannot be explained by the mere mechanical faculty of motion. (Immanuel Kant,
Critique of Judgment [1987]: 221)
Introduction
The past decade has seen a re-emergence of interest in the concept of emergence, and not only in the pages of this Journal. Of course, the debate between
reductionist theories and those that endorse emergent properties and entities
has a long history: it bloomed briefly in the early years of the 20th century with
the British Emergentists, was dormant for some decades, and has again blossomed (cf. McLaughlin 1992; Clayton 2004: chapter 1). Clearly, though, the
Biology and Philosophy (2006) 21:501521  Springer 2005
DOI 10.1007/s10539-005-9003-9
recent discussion has produced more nuanced arguments, and a more subtle
interplay between philosophy and biology, than one finds in the earlier literature.
The aim of this article is to defend one very precise and limited form of
emergence: the emergence of biological agency. Five minimal physical conditions, we will argue, are necessary for applying teleological or agential language
in biology; and taken together, we suggest, they are sufficient: autocatalytic
reproduction, work cycles, boundaries for reproducing individuals, self-propagating work and constraint construction, and choice and action that have
evolved to respond to (e.g.) food or poison.
Although our argument is clearly relevant to the broader philosophical
question of the emergence and nature of conscious agency, we here remain
agnostic on that topic. From the standpoint of the philosophy of biology, as we
hope to show, it is much more urgent to develop an account of the organization
of biological processes and to specify the necessary and sufficient conditions for
biological agency. Science will only understand the emergence of autonomous
agents in the biosphere when it develops an adequate theory of the organization of processes, and in particular when it learns how biochemical processes
self-organize. The account of autonomous agents proposed here is at least a
first step toward comprehending biological self-organization.
The philosophical context
We assume the so-called reduction vs. emergence debate is familiar to readers
and thus provide only the briefest summary of the conceptual background presupposed by our argument. Reduction involves the assumption that one can
construct a hierarchy of scientific disciplines, and that the causal and explanatory connections move in particular directions across the hierarchy. The
causal arrow points upward from the fundamental microphysical causes.
Hence what we call chemical or biological or psychological causes are merely
more complex manifestations of the fundamental causes and are ultimately
reducible to them. Explanations map causal relations: the complex phenomena
of biology or psychology are to be explained by reducing them to their fundamental causes, i.e., microphysical particles and forces.
Among scientists, the physicist Steven Weinberg (1992) offer a classic formulation of the strongly reductionist stance. Although he is hesitant to claim
that scientists will someday be able to deduce upwards from a hoped-for final
physical theory, Weinberg does argue strongly for explanatory reduction, for
example from social behaviors to individual organisms to cells to molecules to
chemistry and ultimately to physics. Many physicists still pre-suppose some
form of classic reductive physicalism, but not all; Philip W. Anderson’s
influential ‘More is Different’ (Anderson 1972) represents a well-known
counterexample, and the Nobel laureate physicist Robert Laughlin has just
published a frontal attack on reductionism in physics (Laughlin 2005). There
502
are, it seems, as many varieties of reduction as there are of emergence, and
the number is not small. Nonetheless, we do not agree that the nuancing
of ‘reduction’ and ‘emergence’ in the recent literature erases all conceptual
difference between them or renders the distinctions otiose. To the contrary.
Phenomena that cannot be deduced from underlying laws (e.g., quantum
mechanics and deterministic chaos) are generally called epistemically emergent. Emergence in this sense presupposes the existence of levels of organization in the natural world. Wimsatt offers the classic definition: ‘By level of
organization, I will mean here compositional levels  hierarchical divisions of
stuff ... organized by partwhole relations, in which wholes at one level
function as parts at the next (and at all higher) levels ...’ (Wimsatt 1994, 222).
We take it to be uncontentious that at least some epistemically emergent
phenomena exist.
Ontological emergence, by contrast  the view that new ‘higher’ levels of
entities arise and have causal powers not possessed by the parts  is sharply
contested. Candidates for such higher-level causes include mental causes and
the emergence of causal agents in the biosphere. Rightly or wrongly, talk of
ontological emergence raises the specter of an anti-scientific dualism, especially
for philosophers of science (Clayton 2004, chapter 4). For those who
emphatically reject dualism, as we do, it will be crucial to identify potential
cases of ontological emergence prior to consciousness if this view is to be taken
seriously at all. Does life represent an emergent organization of matter, energy,
and process? And, in particular, do molecular autonomous agents  distinctively biological causal entities  exist?
Minimal holism
There is a sense of holism  call it ‘robust’ or ‘maximal holism’  that stands in
tension with causal explanations in the natural sciences. A system S is maximally holist if it does not admit of analyses in terms of the laws, regularities,
particles, or causal powers that underlie S, or if such analyses do not help to
explain the phenomena associated with S. By contrast, a system S¢ is minimally
holist if (1) S¢ is not maximally holist and (2) analyses of S¢ in terms of lowerlevel laws, causes, etc. do not fully or adequately explain the phenomena
associated with S¢. Minimal holism is not opposed to (suitably defined)
reductionist explanations; as Wimsatt notes, ‘it is possible to be a reductionist
and a holist too’ (Wimsatt 1994, 225). We suggest that functional explanations
in biology are minimally holist.
The function of the heart is, roughly, to pump blood, not to make heart
sounds. Now suppose we grant that physics might one day be able to give a
complete account of all the causal properties of the heart. That account would
nevertheless not be able to pick out the particular subset of causal consequences that constitutes the function of the heart. To find that function, it
503
would seem, requires an analysis of the organism  in its environment, through
its life cycle, and in light of its particular selective situation.
In short: biological functions, for example the functions of organs, are
subsets of their causal consequences and must be analyzed at the level of whole
organisms, their environment, and their evolutionary history. The physicist
cannot identify the particular subset of consequences that are the function of
an organ without using explanations involving selection, thereby relying on
biological theory. Hence, in at least this limited sense, biological explanations
necessarily move beyond explanations at the physical level alone.
We find the minimal holism that is inevitably entailed by functional explanations to be both non-mysterious and completely consistent with biological
science. In biology, to say that a given causal consequence of a part is the
function of that part is to say that this particular causal consequence was
selected for. Since discriminating biological function invariably includes a
reference to evolutionary history, a physicist’s account of that function would
have to incorporate an evolutionary component, which would necessarily
include reference to Darwin’s mechanisms of evolution. In short, physicists will
still have to play at the level of evolutionary explanations if they are to
discriminate the functions of parts of organisms (as they must).
Human agency and teleology
Volumes have been written on human agency and teleological explanations;
thankfully we do not here need to defend a particular position on the nature of
human agency. But the argument that follows will have to use this wellestablished way of speaking of agency as a touchstone  and, in part, as a
point of contrast  for developing a theory of biological agency. We thus note
merely that ascriptions of agency and purpose represent a standard language
game (in a loosely Wittgensteinian sense) in accounts of human behavior.
Humans regularly offer teleological or ‘means-ends’ explanations in which
reasons appear as causes of behavior (von Wright 1971; Chisholm 1976; Bishop
1983; O’Connor 1995). If human action, reasons, motives, intents, and purposes do not offer an adequate vocabulary and framework for the attribution
of teleological explanations, nothing does. And if the attribution of agency is
(as it seems) a viable language game, human action offers a vast array of
situations upon which to practice that language game.
Whatever theoretical stance one may ultimately take regarding full-blown
human action, it is a stunning fact that the universe has given rise to entities
that do, daily, modify the universe to their own ends. We shall call this capacity
agency.
We dwell no further on the human case, not because there are no important
issues here  indeed, the issues are of paramount importance  but because
there is a prior question that philosophers of biology must address before the
behaviors of homo sapiens could ever be adequately explained: what is the
504
minimal natural system to which one might attribute teleological or purposive
explanations? We shall supply a candidate answer by attempting to define
molecular autonomous agents; we then explore some of the ramifications of this
definition. We note in advance that definitions are neither true nor false, but
fruitful or barren.
Molecular autonomous agents
As noted, the goal is to identify the minimum natural system to which it makes
sense to attribute teleological explanations. Consider, then, a bacterium
swimming up a glucose gradient. This is a case in which biologists normally say
that the bacterium is ‘going to get food.’ That is, in a sense to be explicated
below, the bacterium is acting on its own behalf in an environment. We shall
call a system able to act on its own behalf an autonomous agent. By ‘autonomous’ we do not mean that the system is isolated, but only that it can act on its
own behalf. Now if the bacterium is an autonomous agent, able to act on its
own behalf, then a fortiori all free-living organisms are autonomous agents.
There is nothing mysterious or anti-scientific about this ascription of agency.
From another perspective, the bacterium is ‘just’ a physical system. But then
one wants to know: what must a physical system be such that it can act on its
own behalf?
We propose a tentative five-part definition of a minimal molecular autonomous agent: such a system should be able to reproduce with heritable variation,
should perform at least one work cycle, should have boundaries such that it
can be individuated naturally, should engage in self-propagating work and
constraint construction, and should be able to choose between at least two
alternatives.
‘Choose’ is, of course, teleological language; applied to bacteria it will have
precious few of the connotations that it has in the language game of human
agency. The term must therefore be pared down to its absolute minimum, since
we are seeking the minimal physical system to which one might apply teleological language.
These considerations lead to the idea of a hypothetical chemical autonomous
agent (see Kauffman (2000) for more detail). The first step for conceiving this
form of agency is the existence of an open, far from equilibrium, thermodynamic system. The argument then turns on a fuller understanding of what is
involved in the notion of chemical work. Consider in particular the Carnot
cycle. The Carnot cycle is an idealization that expresses the most efficient heat
engine cycle allowed for by natural law. It consists of two isothermal processes
and two adiabatic processes. To picture the cycle, consider a hot and a cool
heat reservoir, a cylinder with a piston inside, and working gas between the
piston and the head of the cylinder. During the work cycle there is a power
stroke in which the piston starts high in the cylinder, the working gas at the
high temperature of the hot reservoir. During the power stroke, the working
505
gas does work to push the piston down the cylinder in the isothermal part of
the power stroke  that is, the temperature of the gas does not fall significantly
due to contact with the hot reservoir. Next the cylinder is taken out of contact
with both reservoirs during the adiabatic part of the power stroke, when the
gas cools due to expansion. The power stroke is a spontaneous process. After
the power stroke the now cool gas needs to be recompressed to return the
piston to its initial condition high in the cylinder. The cylinder is placed in
contact with the cool reservoir and work is done on the piston, in a nonspontaneous process, to recompress the gas. The way a heat engine works
depends upon the fact that it takes less work to recompress the gas if it is cool.
Thus the contact with the cool reservoir sustains the low gas temperature
during this first, isothermal, part of the compression stroke. Then the cylinder
is brought out of contact with both reservoirs and more non-spontaneous work
is done on the piston to recompress the gas and heat it to the initial state of the
engine in the adiabatic part of the compression stroke, completing the work
cycle.
A number of features about the work cycle are important. First, the cycle
links a physically spontaneous process and a non-spontaneous process.
Second, at equilibrium, no work cycle can occur. Since we have defined a
molecular autonomous agent as a physical system that does a work cycle, it
follows that agency is a non-equilibrium concept. Third, the work cycle brings
the operating organization of the engine back to its initial arrangement of parts
ready for another cycle. Thus, the Carnot engine is an example of a cyclic
organization of process. It appears that we have no formal language to talk
about such organization. We return to this below.
In a chemical reaction system, the concept of a spontaneous process is an
exergonic chemical reaction  the approach of the system to equilibrium. A
non-spontaneous process is an endergonic process, where work is done on the
chemical system to ‘push’ it beyond equilibrium, towards the excess synthesis
of some chemical component compared to its equilibrium concentration. This
requires linking the endergonic reaction to some form of spontaneous process,
often by linking the reaction to a second exergonic reaction. With this in mind,
we can describe our hypothetical molecular autonomous agent.
The first part of the system consists in a molecular autocatalytic system in
which two DNA trimers join end to end in a proper 3¢5¢ phosphodiester bond
to form a hexamer. This reaction is catalyzed by an initial copy of the hexamer
single-stranded DNA sequence. Hence the molecular system is autocatalytic
and self-reproducing. The second part of the system is the work cycle, which is
used to drive excess synthesis of the hexamer from the trimers. More technically, exergonic chemical energy from the work cycle is used to drive the
endergonic, hence non-spontaneous, synthesis of excess hexamer compared to
its equilibrium concentration. The work cycle motor consists in coupled
reactions. Its driving force consists in pyrophosphate (PP), which is present in
excess of its equilibrium concentration. PP breaks down exergonically, that is
spontaneously with a loss of free energy, to two monophosphates, P+P. The
506
breakdown of PP to P+P is coupled to the synthesis of hexamer from the two
DNA trimers and provides the chemical energy to drive that reaction endergonically such that excess hexamer is synthesized compared to its equilibrium
concentration. Once this has occurred, PP must be restored to its initial concentration such that another cycle of work can be done. Restoring PP to its
initial concentration is accomplished by another coupled pair of reactions
where an exergonic reaction drives the non-spontaneous, hence endergonic,
resynthesis of PP. An electron in its ground state absorbs a photon and is lifted
to an excited state. Note that work has been done on the electron. As the
electron falls back spontaneously to its ground state exergonically, that energy
is coupled to the endergonic resynthesis of PP from P+P, until it reaches the
initial concentration of PP. The motor in this system is the net rotation of
monophosphates around the reaction cycle. Thus the system both reproduces
and does a work cycle.
In a heat engine, the organization of processes is accomplished by cams,
gears, and so forth. In order to coordinate the reactions of the molecular agent
such that the forward synthesis of hexamers occurs with the depletion of PP,
the PP subsequently being restored to its initial level (that is, that the two
reactions occur successively like the power and compression strokes of the
Carnot cycle), we suppose that monophosphate, P, feeds back to activate the
hexamer catalyst, while PP feeds forward to inhibit the resynthesis of PP from
monophosphate, a reaction catalyzed by one of the DNA trimers. These regulatory couplings assure that the two reactions occur reciprocally, as demonstrated by writing down the differential equations for the system and finding a
limit cycle oscillation for its dynamics (Daley et al. 2002).
We want the molecular system to have boundaries in order to be individuated. But an even deeper reason to require boundaries is that the molecules
would otherwise diffuse out of effective contact with one another. Thus we
suppose that the remaining DNA trimer is able to act as a catalyst to ligate two
molecules, X and Y, which jointly create a lipid-like molecule Z. In an aqueous
medium lipids will fall to a low energy state in which they form vesicles that are
lipid bilayers, called liposomes, which are deeply similar to cell membranes.
Thus we suppose that Z synthesis leads to the formation of a bounding
membrane containing the reaction members discussed. For the purpose of the
following discussion, let us suppose that the formation of Z requires that
chemical work be done. For example, a high energy molecule such as PP might
break down exergonically and be coupled to the ligation of X and Y to drive
the endergonic synthesis of Z.
This system is a perfectly legitimate, if hypothetical, thermodynamically
open chemical reaction network. It does not cheat the second law of thermodynamics, for its food sources are X and Y, the two DNA trimers, and photons. Indeed, we must assume that these food molecules can diffuse across the
bilipid membrane to reach the interior of the ‘cell.’
Finally, we wish our system to be able to ‘choose,’ in the sense that it can
exhibit different and appropriate behaviors in the presence of different choice
507
situations. We will therefore assume that its aqueous interior can undergo a
sol gel transition that allows the entity to move, which may require another 
here unspecified  work cycle. We further suppose that the system can synthesize receptor molecules, which bind to food and a specific toxin, and that it
can move toward the food and away from the toxin. (Obviously, this would
require substantially more molecular machinery than we have described here.)
In an appropriately minimal sense of ‘choice,’ a system of this sort would be
able to exercise choice. Real cells accomplish just such choice behavior.
A variety of points should be made about this minimal system. First, for a
work cycle to be performed, the system must be displaced from equilibrium;
hence agency is a non-equilibrium concept. Second, the system links spontaneous and non-spontaneous processes in its work cycles. This linking has led to
a biosphere in which sunlight is captured by plants and used by herbivores and
carnivores to create the complex web of linked exergonic and endergic
reactions within the global chemical reaction network that undergirds the
biosphere. Third, the system stores energy, in particular in the excess of hexamer over its equilibrium concentration. As pointed out by P.W. Anderson
(personal communication), this might later allow error correction. Fourth, the
system is a perfectly legitimate coupled non-equilibrium reaction network as
noted above. Fifth, such systems ought soon to be constructable and testable.
In fact, they may augur a technological revolution, since they can perform
work and hence build things. In sum: systems of this type are not merely
philosophical examples; they are objects of current scientific research. At the
same time, they exhibit the five principles which, we claim, are essential to a
minimal autonomous agent, that is, the sort of agent to which one can validly
apply teleological language.
Work cycles and self-organization
We turn now to some puzzles concerning work cycles, puzzles that seem to
point us towards the kind of self-propagating organization that Kant was
referring to in the initial quotation. To a physicist, work is force acting through
a distance  a scalar quantity. Yet in any concrete case of work one finds an
organization of processes that is not captured in that scalar quantity. That
there is something odd about the concept of work can be surmised from the
fact that an isolated thermodynamic system, say a gas in a bounding membrane, can do no work. By contrast, if the gas volume is partitioned by an
elastic membrane into two parts, A and B, and if the pressure in B is higher
than in A, the difference in pressure will cause the membrane to bulge into A,
with the result that B does work on A. Oddly, this seems to require that the
universe be divided into at least two parts for work to be done.
We find most congenial Atkins’ definition of work as ‘the constrained release
of energy’ (Atkins 1984). Picture a cylinder with a piston mounted inside and a
compressed working gas at the head of the cylinder. The compressed gas has
508
molecules moving in all directions with a distribution of velocities, and it exerts
pressure on the piston. The result is a spontaneous process that pushes the
piston down the cylinder. The gas does work on the piston, and energy is
released into a few degrees of freedom  the translational motion of the piston.
But this raises a new question: what are these constraints, and how do they
originate? In the case just mentioned the cylinder, the piston, and the location of
the piston inside the cylinder are the constraints. Where did they come from?
The answer is that it took work to make these constraints. So we come to a
crucial cycle: it appears to take constraints to make work, and work to make
constraints. (While this may not be necessarily true, it is typically true.) It is
critical to the entire discussion of emergence that biology presently lacks a
theory of the organization of processes in general, and of biochemical processes
in particular, and we suspect that this cycle of work constraint work is a part
of the required theory of organization. We will return to this topic in a moment.
We are now far enough that we can begin to make sense of Kant’s idea of a
formative self-propagating organization communicated by the whole to the
parts, though they have it not of themselves. The first concept for applying
Kant’s conjecture to actual biological systems is that of propagating work.
Unfortunately, there is as of yet no formal definition of the concept. But
picture a whimsical Rube Goldberg contraption: a cannon fires a cannon ball
that hits a paddle wheel, setting the wheel spinning. A rope attached to the
wheel is wound up around its axle, thereby lifting a pail of water from a well
and tipping the pail’s water into a funnel that leads to a tube. The water flows
down the tube, opens a flap valve, and waters our bean field. This Rube
Goldberg contraption accomplishes what we want to call propagating work. In
the example a series of macroscopic changes occurs in the world, occasioned by
the explosion in the cannon that sent the cannon ball flying, and the resulting
changes constitute a chain of propagating work.
In this example, as in the earlier piston example, we have ignored the work
involved in constructing the constraints that allow the whole assembly to
function. But in attempting to understand minimal autonomous agents in
biology, one cannot ignore the interrelations between work and the constraints
that make it possible. In hypothetical and real cells, there is a crucial (and
complex) cycle between work, constraint construction, and more work. It takes
the constrained release of energy to do the work to construct more constraints
to do more work  all of this depending on a controlled release of energy in a
web that closes on itself until the real cell builds a copy of itself.
Consider our minimal molecular autonomous agent with some chemicals, A
and B, in its aqueous phase. Let us suppose, as noted above, that building the
molecule Z (that is, the lipid) required chemical work: work was done to
construct the lipids, which then fell to a low energy state, the liposome. Let A
and B be capable of undergoing three different reactions, to form C and D, or
E, or F and G. Each reaction can be drawn in a Cartesian coordinate system
with reaction coordinates on the X axis and free energy on the Y axis. Typically
the substrates and products sit in their own potential wells, separated by an
509
energy barrier. Each of the three reactions has its own reaction coordinate · energy profile. Now let A and B diffuse into the bilipid membrane
bounding our agent. In the membrane the rotational, vibrational, and translational degrees of freedom are altered compared to the aqueous phase. These
alterations alter the reaction coordinate · energy profiles of the three reactions.
But this is precisely the manipulation of constraints, for the changing heights of
the energy barriers just are the constraints on the reactions. Thus our hypothetical autonomous agent has done chemical work to synthesize lipids, which
fall to a low-energy bilipid membrane state that serves to modify constraints on
chemical reactions. Suppose the barrier between A and B reactants and C and
D products is lowered significantly when A and B enter the membrane, such
that this one reaction, and not the other two (making E, or making F and G),
takes place at a high rate. As long as these differentiated relations hold,
chemical energy is released in constrained ways. It is obvious, then, that our
agent does work to construct constraints on the release of energy. That released
energy may then propagate to modify other constraints, thereby allowing more
work to be done. For example, D may diffuse to a transmembrane receptor
and, in real cells, activate a work-requiring transport mechanism that brings a
molecule across the membrane into the cell up its own gradient.
All the pieces are now in place for completing our account of the organization
of minimal autonomous agents. The example has shown that cells do selfpropagating work. This includes the construction of constraints on the release
of energy, work that then constructs still further constraints on the release of
energy, which in turn do work as well as constructing further constraints ... and
so on. The astonishing fact is that, as cells carry out this complex web of work,
constraint construction, and other construction projects (such as DNA replication and enzyme synthesis), a closure is attained in which the cell finally builds
a rough copy of itself. But this whole process is precisely the self-propagating
organization to which Kant pointed. Note that self-propagating organization in
this sense does not involve matter alone, energy alone, information alone, or
entropy alone. It is a process that involves all these  and something more as
well. It appears that this self-propagating organization, ‘communicate[d] to its
materials though they have it not of themselves,’ is a new form of energy-matter
organization in the world; it is living matter, and it is ontologically emergent.
The structural and functional features we have described meet the requirement
for ontological emergence: the whole has causal powers not possessed by the
parts. For example, in the case of cells the whole is capable of building copies of
itself, hence capable of evolution by natural selection. We need merely imagine
that mutant variants of the minimal autonomous agent, or real cells such as
bacteria, can have heritable variants and be selected.
Against our reductionist critics we want to stress the minimal holism of
autonomous agents. There is a closure of work tasks that is completed when a
cell constructs a rough copy of itself. Work tasks, like the functions of the parts
discussed earlier, are a subset of their causal consequences. And, once
again, physical descriptions are precluded in principle from identifying the
510
appropriate subset of causal consequences unless they can refer to the roles
that these consequences play in the self-reproduction (and other functional
aspects) of the autonomous agent. It’s therefore impossible to specify the tasks
and functions without discussing the entities in question at the level of autonomous agents, that is, biologically. This goal cannot be achieved at the level of
pure physical description alone.
Notice that what is needed for comprehending minimal autonomous agents
is a theory of the organism-level organization of biochemical and other processes. Unfortunately, no adequate theory of the organization of such processes currently exists in the scientific or philosophical literature, even in
outline. And yet a reproducing cell does it. Had we an adequate theory of how
organismic processes self-organize, we would be able to conclude something
more interesting about the ontological emergence of minimal autonomous
agents than the bald fact that it occurs. We return to this issue below.
Implications of the argument
One of the ways to understand and evaluate an argument is to consider what it
appears to imply. We thus pause to consider three plausible extrapolations
from the argument to this point. It goes without saying that extrapolations
from an argument are always more speculative than the core argument itself.
The underdetermination of the biological by the physical
Darwinian evolution is neutral with respect to the  possibly indefinitely many
 physical bases that might support reproduction with heritable variation and
hence evolution by natural selection. It is possible that life and evolution would
arise in a non-denumerably infinite family of universes similar to our own.1 The
possibility that evolution might run on multiple ‘platforms’ undercuts the claim
that explanatory arrows from physics are necessary and sufficient for
explaining biological phenomena. To say everything that can be physically said
about a biological process still leaves it less than fully explained.
Darwinian adaptive evolution is agnostic with respect to the precise physical
mechanisms of reproduction and heritable variation such that natural selection
can act to yield evolution. We will argue that this fact means that Darwinian
selection is not reducible to  and hence cannot be explained by  any specific
set of lower-level explanations. Indeed, the set of possible mechanisms for
reproduction and heritable variation in this specific universe is systematically
1 The ‘multiple platforms’ argument is analogous to the argument for the ‘multiple realizability’ of
mental states, although we do not here take a position on the latter. For an introduction to the
‘multiple realizability’ concept in the philosophy of mind, see Heil (2003) and Clayton (2004,
chapter 4.)
511
vague, in the sense that one cannot (even in principle) pre-list all members of
this set. Almost all organisms of all reproducing species are capable of
reproduction and heritable variation, even though we cannot pre-state all
possible modes of reproduction nor what those organisms and species will
become as evolution proceeds. Given that one cannot infer the modes of
reproduction and heritable variation from the Darwinian mechanisms of
evolution, and given the actual experimental demonstration of more than one
mode of molecular reproduction, it follows that some unknown  and in the
case of new species, unknowable  number and kinds of entities are capable of
reproduction and heritable variation.
It turns out, however, that the limitations on reductive explanation are
actually far broader. Contemporary organisms are based on DNA, RNA,
proteins, small and large metabolites, and so on. But self-reproduction is not
limited to the template-based replication of DNA by protein enzymes. Recently
it has been shown that peptide reaction systems are capable of autocatalytic
reproduction (Lee et al. 1996; Ashkenasy et al. 2004). More generally, peptide
systems are capable of what we shall call ‘collective autocatalysis.’ Here peptide
A catalyzes the synthesis of B while B catalyzes the synthesis of A from
appropriate precursors. But if two peptides can be collectively autocatalytic,
why not three or a thousand? Indeed cells are, themselves, collectively autocatalytic wholes. Meanwhile, chemists are discovering other modes of molecular reproduction, including self-reproducing lipid vesicles.
Indeed, the irreducibility may even be more dramatic. Imagine that the
constants of nature are real numbers and that at least a small range of those
values are consistent with universes in which chemistry, self-reproduction, and
heritable variation  hence natural selection  are possible. Darwin’s mechanism of reproduction and heritable variation could then be realized in a nondenumerable infinity of universes. Hence, again, we cannot finitely pre-state all
possible modes of reproduction and evolution, even in principle. It follows that
downward explanatory arrows in this universe are not explanatory of evolution in
neighboring universes. And yet that fact does not preclude us from hypothesizing that evolution by natural selection could occur in those universes.
This leads us to ask what it would mean to ‘reduce’ an explanation of
organisms and natural selection and evolution to a physical or microphysical
level. Although explanatory reduction is variously defined, most accounts
agree that successful reduction  say, from some higher level L2 to a lower level
L1  requires that at least three conditions be met:
 the laws governing the behavior of the L2 entities can be expressed in the
language of L1, and these laws are sufficient to explain the L2 phenomena;
 the terms in the L2 language are replaceable by a finitely pre-specified list of
terms in the language of L1;
 by virtue of this equivalence between an L2 phenomenon and a set of L1
terms, the L1 theory and entities specify the causal mechanisms that are
sufficient for explaining the L2 phenomenon.
512
But meeting these conditions is just what is precluded by our incapacity, even
in principle, to finitely pre-state all modes of reproduction and heritable variation. So, it seems, biology is cut off at the explanatory level from any final
reduction to physics. Hence, we have no choice but to explain biological
evolution as Darwin taught us to do  supplemented, of course, by those
emendations to Darwin that subsequent advances in evolutionary theory have
established.
Darwinian pre-adaptations
The fact that we cannot predict (i.e., deduce ahead of time) the future evolution
of the biosphere might be taken to support epistemic but not ontological
emergence. But Darwinian pre-adaptations appear to support the stronger,
ontological construal of emergence.
It’s well known that Darwin distinguished between adaptations and what he
called pre-adaptations. An example of an adaptation would be a modification
in the beak of a bird such that it was better fitted to the size and composition of
the seeds available to it in its local habitat. The notion of a pre-adaptation is
logically dependent on the concept of the function of a part of an organism.
The standard account of this notion divides the causal consequences of parts of
organisms into two logical subsets: those that constitute a part’s (biological)
function and those that do not.2 For example, although the primary function
of the heart is to pump blood, the heart also has causal consequences that are
not functions, such as making heart sounds. Natural selection is said to have
acted to modify those causal consequences that are the part’s function. Thus
pumping blood is, roughly, the causal consequence in virtue of which the heart
was selected. In order to identify the function of a part of an organism, we
typically have to analyze the organism as a whole in its environment over a life
cycle and in light of its specific selection pressures. To do so is necessarily to
make an evolutionary claim about selection for that particular causal consequence.
Consider the implications of the Darwinian concept of pre-adaptations. A
causal consequence of a part of an organism which, in the normal environment, is not of selective significance might, in a different environment,
become significant for selection. In such cases it turns out that, by virtue of
the pre-existence of a specific causal consequence, the organism was preadapted to the novel environment. Many major adaptations in evolution,
and even more minor ones, are thought to have been Darwinian preadaptations: lungs were derived from the swim-bladders of certain fish, the
2 We speak of ‘primary function’ in the singular for the sake of convenience. In fact a set of
functions may serve as the primary function of an organ. Still, it is never the case that all the causal
consequences of an organ are its function.
513
inner ear bones that mediate hearing were derived from the jaw of an early
fish, and so forth.
Incidentally, examples of pre-adaptation also play a crucial role in human
technological evolution. The following story may be merely a story, but it will
do. It is said that a group of engineers was trying to invent the tractor. They
knew that they needed a massive motor, so they mounted a massive engine
block on a chasse. The chasse promptly crumbled. They tried a bigger chasse,
but again it broke. After multiple attempts one of the engineers said, ‘You
know, this engine block is so big, massive, and rigid that we could use it as the
chasse and hang everything else off of it.’ That, indeed, is how tractors are
made. Now the rigidity of the massive engine block was a causal property of
the engine block that, for normal purposes, would not be put to use. For this
novel purpose, however, the rigidity of the engine block could be put to good
practical use.
These examples raise a central question: is it possible to specify ahead of
time  or more precisely, to finitely pre-state  all possible Darwinian preadaptations for the species alive today? We believe that the answer to this
question is ‘No.’ In part, the difficulty lies in attempting to pre-state what all
possible environments for organisms are. An environment for an organism, its
niche, is, roughly, those aspects of its abiotic and biotic surroundings that bear
on how it makes a living in the world. Organism and niche fit and mutually codefine one another. Yet note that the concept of a niche, or an environment, is
systematically vague. There seems to be no way to enumerate all possible
environments, and thus to specify in advance what all possible pre-adaptations
might be. Hence it appears impossible in principle to finitely pre-state all
possible Darwinian pre-adaptations.
Clearly, this limitation is epistemological: when such a pre-adaptation occurs, it could not have been foretold or deduced ahead of time. It thus represents at least a case of epistemic emergence.3 But pre-adaptations support
ontological emergence as well. It’s not just that, after the fact, we cannot
explain a given pre-adaptation in purely physical terms (though this is certainly
true). Much more, before the particular causal feature began to confer selective
advantage the world was such that the particular causal feature was not yet
distinguished from among all the other causal properties of the organism and its
parts.4
If valid, this conclusion has important consequences. For example, it implies
that we cannot pre-state the configuration space of the biosphere. One cannot
know ahead of time the kinds of entities, processes, and functionalities that will
come to exist. This directionality suggests the existence of a distinctively
3 This is not to deny, of course, that after the fact we can explain how it was that the pre-adaptation
functioned as it did. Here we have a case of the well-known distinction between prediction and
retrodiction, which reflects, once again, the existence of a biological arrow of time. In biology, as
Broad (1925) argued, we have to see a case before we can explain it.
4 Incidentally, note that ‘all possible features of organisms’ also cannot be finitely pre-specified,
and for similar reasons.
514
biological arrow of time. Contrast this essential time-dependence with statistical mechanics. Physicists can pre-specify all possible combinations of positions and momenta in a liter of gas, and hence pre-specify the phase space of
the gas system as a whole; indeed, the constructs of statistical mechanics are
derived from that phase space. For a biosphere this is impossible.
Now a classical physicist might object that one could consider the solar
system, say, as a classical 6N dimensional phase space, in which all possible
biospheres can be represented as points or trajectories. Thus, she concludes, we
can pre-specify the configuration space of the biosphere. For the sake of
argument, let us grant her this possibility as far as it goes. Unfortunately,
however, there seems to be no way of pre-stating the collective variables  the
lungs, wings, mitochondria, and so forth  that will come to exist in the
ongoing evolution of the biosphere. Note that it is precisely those collective
variables that play a causal role under selection in the ongoing evolution of the
biosphere, and hence play the key explanatory role for what comes to exist in
the biosphere. This fact suggests, again, that Darwinian pre-adaptations are
ontologically emergent. The collective variables should be taken as entities at
their own level associated with their own causal powers, e.g., it was lungs that
allowed amphibians and later taxa to invade the land.
For the reasons given, pre-adaptations resist analysis in terms of the familiar
frequency theories of probability. When one asserts that a fair coin will tend to
land on its head about 50% of the time, one knows ahead of time the full
configuration space of, say, 10,000 coin flips. But we do not know the macroscopic collective entities that will arise in the evolution of the biosphere. We
do not even know, ahead of time, what entities will be involved in causal
interactions. So we cannot make the familiar frequency-based probability
statements about ‘the future of the biosphere in general.’
The role that Darwinian pre-adaptations play in evolution forces us, then, to
accept the thesis of ontological emergence. For the new adaptation surely is a
newly existing entity, a new functionality; it could not be pre-specified, even in
principle; and it has causal powers of its own (wings allow the creature to fly).
In science in general, what plays a causal role (say, under selection) also plays
an explanatory role: it determines what comes to exist in the biosphere. Those
things that are causes and are essential for scientific explanations are things
that scientists are justified in treating as existing. Hence the emergence in
question is ontological.
Downward causation
Is the claim that explanatory arrows always point upward correct? We argue
that it is not. Consider a concrete case. Most species that ever lived are now
extinct. Now when a species goes extinct, there is some last living member of
that species. When that member dies, it dies as a whole biological organism,
not as a mere aggregate of its (physical and chemical) parts. But when it dies,
515
the specific molecules, proteins, genes, RNA, and so forth that were particular
to that species vanish from the further unfolding of the biosphere. That is, the
causal account of the extinction of the species requires one to employ the laws
of population genetics and ecology, and perhaps also to give a historical account of the conditions of the abiotic environment. By virtue of the death of
that organism, its unique molecules are lost from the future unfolding of the
biosphere. Hence, the extinction of species alters the molecular makeup of the
biosphere.
This case provides a simple, concrete instance of downward causation. An
unknown diversity of genes, proteins, and small molecules are no longer
present in the biosphere because the organisms of species carrying them no
longer proliferate. Notice also that, in order to explain this case of extinction,
we need the concept of species and, for sexual species, also the notion of the
reproductive isolation of species. Yet, at the molecular level of detail, there are
indefinitely many ways that species can be reproductively isolated from one
another. Here again we confront the multiple platforms issue. We cannot
finitely pre-state the mechanisms of reproductive isolation, and thus we cannot
carry out the reduction of species-and organism-level language and explanation
to a lower level of laws and entities. The mechanisms of species isolation from
one another, and the population-ecological level explanations of extinction, are
just not reducible in this sense. In living and dying, organisms exercise a causal
influence on their parts  an influence that is well described as ‘downward
causation,’ in what we take to be a completely non-mysterious sense of the
term.
Species extinction pertains to whole organisms, yet it changes the molecular
future of the biosphere. The laws that govern species evolution, including
extinction, are not identical to the laws of physics or to explanations at the
physical level alone. This particular form of downward causation from biology
to physics is a necessary condition for the stronger forms of downward causation debated by philosophers of mind, but one can accept the former without
accepting the latter.
Areas for further research and reflection
Teleonomic vs. teleological ascriptions
It is obvious that our minimal molecular autonomous agent fulfills the teleonomic requirements for ‘acting as if ’ and ‘referring’ and ‘choosing’; and if it
does, real cells do as well. One need merely add selection and heritable variation to our account to achieve a model of what real cells do. But teleonomic
talk is not yet teleological. It is one thing to have the appearance of goaldirected behavior; it is something else to actually engage in goal-directed
behavior.
516
We suggest that minimal molecular autonomous agents are the minimal
systems to which one can justifiably attribute referring, choosing, and acting.
We are hesitant to make this claim as a metaphysical assertion, since it’s not
clear how one would ‘prove’ the appropriateness of playing the agency language game. But the minimal molecular autonomous agent seems to be about
as minimal as one can get and still plausibly use the language of agency.
Obviously, agency starts at some level of organization within the biosphere.
Humans are agents  we plan and carry out actions consciously  and rocks
are not; they are involved in cause-and-effect interactions but do not carry out
actions. We take agency to be a matter of degree: some minimum conditions
must be met for one to ascribe it at all, and then it increases (roughly, as a
function of the increase in organizational complexity through evolution) until
one encounters full, robust, conscious agency.
At least two paths exist for defending the minimal ascription of agency, and
both have already been introduced. On the one hand, an agent is an organism
that is faced with a choice situation (it may act this or that way, to achieve this
or that end) and then acts to achieve an end (e.g., it moves toward the
food source or away from the poison). On the other hand, an agent is an
interpretant in a system of semiotic signaling, that is, a system that includes
signs, interpretations of the signs, and actions in accordance with the interpretations.5
A general biology
Astrobiology is a major growth field within the biological sciences. Astrobiologists attempt to understand biospheres anywhere in the cosmos. In Investigations Kauffman (2000) rather boldly proclaimed four candidate laws for
any biosphere. The first candidate law arises out of work on parallel processing
non-linear dynamical systems, such as random Boolean networks and their
continuous variable cousins, which shows that such systems behave in two
broad regimes, ordered and chaotic, with a phase transition, sometimes called
the ‘edge of chaos,’ separating the two regimes.
It is not important for present purposes whether all four candidate laws are
actually true. Let us assume for the sake of argument that at least the first is
true and ask whether, if so, it is reducible to physics. Kauffman and others
(Kauffman 1993) have argued that the ordered vicinity at the edge of chaos is a
favorable location for the dynamics of autonomous agents. Here the most
complex coordinated behavior can occur. Thus it is an attractive hypothesis
5 We cannot here summarize the recent applications of semiotic theory to biological systems,
which, despite some critical reservations, we find intriguing and potentially empirically fruitful. For
an introduction to the field see Weber (2003), Emmeche et al. (2002), Taborsky (1999), and
Hoffmeyer (1996). Of course, for cells it is natural selection that produces the object-sign-interpretant relationship rather than the socially much more complex way in which humans achieve
reference to objects using signs.
517
that agents will evolve  that they will learn to play the increasingly complex
natural games that agents play to earn a living  by tending to move to the
ordered vicinity at the edge of chaos. Assume for the moment that this
hypothesis is true. Now note that, like Darwinian natural selection, the
hypothesis is systematically vague about the physical mechanisms of action of
the non-linear variables such that the systems they comprise will lie in the
ordered regime near the phase transition. Thus, again, it appears to be
impossible in principle to pre-specify the necessary and sufficient physical
conditions such that autonomous agents would evolve to the ordered vicinity
of chaos. Furthermore, and importantly, the laws that govern the whole are not
to be found in any specific physical realization of such a system, but rather in the
mathematics of this broad class of dynamical systems, whatever their material
realization. Put differently, the laws exist at the level of the collective variables,
of whatever physical character, that comprise the system as a whole. Here too,
it appears, we are confronted with a case of ontological emergence.
The need for a theory of the organization of processes
The holism and task closure that occur in the reproduction of a minimal
autonomous agent  and hence, to be more concrete, in most living cells 
involves an organization of processes. Clearly, if a cell is to be evolutionarily
successful, one of its main tasks must be reproduction. Yet as we showed, not
all the causal consequences of its parts are implicated in the causal and
functional account of the closure of the reproductive task that the cell carries
out.
No adequate theory of organization currently exists. The required but not
yet available theory of organization will involve more than merely a discussion
of entropy. Entropy counts the number of configurations a system occupies,
but it is not concerned with constraints. As we saw, central to cell functioning
is the use of energy to do work to construct constraints on the release of energy,
which is work that propagates to construct more constraints on the release of
energy, leading to other construction tasks that finally close upon themselves in
the reproduction of an autonomous agent, a living cell.
We may not yet be able to formulate an acceptable theory of organization.
Still, it may be helpful to speculate about some of the features theory-candidates should have. Several clues are available. First, in the case of full-fledged
human action we know (and need to know) both what overarching tasks and
linked tasks are. We know what it is to plan and prepare dinner, eat dinner, and
clean up after dinner. Implicit in this knowledge is a tacit knowledge of tasks
and other events that are linked to the overarching tasks, as well as causal
factors that are not essential to them. We peal the carrots. How we do so 
whether, for example, we do so over the counter or over the sink  is not
crucial in specifying what dinner preparation entails. Knowledge of tasks, then,
identifies a subset out of the sum of all causal consequences and events; that
518
subset is essential for specifying the tasks, whereas a myriad of other causal
factors are not essential. This fact suggests that any theory of organization
must have a way to pick out subsets of causal consequences and events that are
relevant for accomplishing biologically essential tasks.
One wants to know, second, which subset of causal consequences is relevant
to distinctively biological explanations. We have made the standard biologist’s
argument that, at the level of parts of organisms, one must pick out those tasks
or functions upon which selection has acted. This too is a clue: in biology no
theory of organization will be adequate that does not incorporate the core
principles of evolutionary theory.
A third clue lies in the concept of the cycle of work: the constrained release
of energy, and the use of work to construct those very constraints. Interestingly, work seems to include its own distinction between relevant and irrelevant
causal consequences. Consider again the cylinder and piston with hot gas in the
cylinder head. During the power stroke the gas does work on the piston,
causing transversal motion. But the gas also exerts pressure  momentum
transfer  to the (rigid) cylinder. We distinguish the former as relevant and the
latter as irrelevant to work done. A theory of the organization of process must
be able to make the same sort of distinction.
Undoubtedly there are other clues. Signals are low mass-energy flows that
unleash larger mass-energy flows in the organization of cells and human
activities. It seems likely that any theory of organization must include signals
and the semiotics of signals. If so, this hints that some concept of reference
must play a role in a theory of organization. But how? Perhaps it is that the
signinterpretant system itself picks out the relevant from the irrelevant aspects of a situation, object, or feature of the object to be interpreted by the
interpretant. How these clues fit together is at present unclear. Dare one say
that ‘goals’ or ‘purposes’ may enter into the task of distinguishing between
relevant and irrelevant causal consequences? At a meta-theoretical level it
seems clear that the theory of organization needs to be able to specify goaldirected processes, even if, at present, we don’t know what this would mean
for actual biological theories at the level of minimal autonomous agents. In
full-fledged human design (as in architecture, engineering, and other arenas)
the goal or purpose of the artifact distinguishes between the causal consequences that are relevant and irrelevant. In evolution the ‘purpose’ of the
heart is to pump blood; biologists then provide selection-based explanations
and interpret the ‘purpose’ teleonomically. It is therefore possible that future
theories of biological organization will need to be restricted to evolved
entities and purposive acts and constructions. But this suggestion remains
speculative for now. The fact that work itself distinguishes relevant from
irrelevant causal consequences may suggest that a natural theory of selforganization can be created using the work concept. Getting clear on this
particular issue would be an important step toward a broader theory of
organization.
519
Summary
We have argued that Darwinian natural selection is agnostic with respect to the
indefinitely many mechanisms by which reproduction and heritable variation
can arise within this or that member of a small family of similar universes. We
suggested that reduction requires formulating descriptions and laws in a lowerlevel language that are necessary and sufficient for the truth of the higher-level
language and explanatory of the phenomena at that higher level. Given what
we called systematic vagueness  the incapacity in principle to pre-state in
physical terms the mechanisms that would fulfill Darwin’s requirements  we
concluded that neo-Darwinian evolution cannot be formally reduced to physics
alone.
We also argued that extinction is a case of downward causation: when the
last member of a species dies, it dies as a whole organism; yet at the same time
its death alters the molecular makeup of the biosphere. The physicist must
appeal to laws and phenomena at the level of populations of organisms if she is
to explain extinction. Finally, we showed that one cannot finitely pre-state all
possible Darwinian pre-adaptations because (among other reasons) the concept of an environment in which an organism makes a living is systematically
vague and hence cannot be reduced, even in principle, to a lower-level language. Physics does not contain what turn out to be the essential theoretical
terms for conceptualizing the relationship between organisms and environments, such as fitness and selection pressures.
All of these conclusions appear to fulfill the conditions for ontological
emergence: biological entities have causal powers, such as the capacity to
evolve, that pre-biological physical entities do not have. We have argued that
agency, which is present in full-blown human action, can be meaningfully and
fruitfully ascribed to minimal autonomous agents as well. It would follow that
living organisms represent a new form of matter, a new instance of the organization of processes, that fulfills Kant’s dicta and is thus ontologically
emergent. We close by reemphasizing that what is needed to fully understand
biological agency has not yet been formulated: an adequate theory of organization. Such a theory may well be the missing link in the contemporary
emergence vs. reductionism debate.6
References



---------



dTal almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
It's sleight of hand because the sentience of the human in the system is irrelevant. The human is following a trivial set of rules, and you could just as easily digitize the books and replace the human with a microcontroller. Voila, now you have a Chinese-speaking computer program and we're back to where we started. "The books" don't feel anything, true - but neither do the atoms in your brain feel anything either. By asserting that the human in the room and the human who wrote the books are the only "relevant entities" - that consciousness can only emerge from collections of atoms in the shape of a human brain, and not from books of symbols - you are begging the question.

The Chinese room is in a class of flawed intuition pump I call "argument from implausible substrate", the structure of which is essentially tautological - posit a functioning brain running "on top" of something implausible, note how implausible it is, draw conclusion of your choice[0]. A room with a human and a bunch of books that can pass a Turing test is a very implausible construction - in reality you would need millions of books, thousands of miles of scratch paper to track the enormous quantity of state (a detail curiously elided in most descriptions), and lifetimes of tedious book-keeping. The purpose of the human in the room is simply to distract from the fabulous amounts of information processing that must occur to realize this feat.

Here's a thought experiment - preserve the Chinese Room setup in every detail, except the books are an atomic scan of a real Chinese-speaker's entire head - plus one small physics textbook. The human simply updates the position, spin, momentum, charge etc of every fundamental particle - sorry, paper representation of every fundamental particle - and feeds the vibrations of a particular set of particles into an audio transducer. Now the room not only speaks Chinese, but also complains that it can't see or feel anything and wants to know where its family is. Implausible? Sure. So is the original setup, so never mind that. Are the thoughts and feelings of the beleaguered paper pusher at all relevant here?

[0] Another example of this class is the "China brain", where everyone in China passes messages to each other and consciousness emerges from that. What is it with China anyway?

2.
crdrost about 1 year ago | on “LLMs can't self-correct in reasoning tasks, DeepMind study finds”
The Chinese room argument is just missing kinda one basic premise and people don't realize it and argue around the premise without actually addressing it.

“Can we agree that of the many different things folks mean when they say your consciousness is ‘causally generated by your brain,’ that at least we can say your brain in its present state is only capable of generating exactly one conscious process at a time, namely you?”

So it's a sort of causal sufficiency, Searle believes that there is something about this squishy machine that the physics is doing that makes it conscious, and that's being caused very directly by the squishy machine. It's not some magic.

On the other hand, if you believed in souls, you might think that a given brain contained both the soul of a human person, and the soul of some demon possessing them—two conscious processes, one of which was stuck “along for the ride.” [Searle is also interested in the cases of split-brain patients where you have a similar phenomenon because essentially two separate brains inhabit a single body. He has I think mentioned at a talk that one of the interesting things about consciousness to him is that it all gets Unified, so it's interesting that if the two parts of the brain can talk to each other they merge their consciousnesses into one more powerful consciousness, rather like (my analogy, not his) how if you have two water droplets on a plastic plate and you push them with a toothpick together, at some point they merge into one bigger drop.]

Now the Chinese room thought experiment is not about the computational model of consciousness—not directly! It was always phrased as a rebuttal of the Turing test in particular, and the computational model only indirectly after the Turing test falls. Note that the Turing test has no direct beef with the casual sufficiency axiom, which is why it went unstated originally. According to the Turing test, written text goes in, written text comes back out, a dialogue appears to happen to the outsider, this is sufficient to conclude conscious understanding of the language used, which confirms consciousness.

Searle’s objection is, “if it's really all about inputs and outputs and not how I get it done, then you've left out what for me feels like the most important part about understanding a language: understanding it is part of how I get it done!” Right? You understand English because you can phrase your ideas into it, you can mold it to suit you, and it can (when heard) so mold you—it’s not just because the words can come out of your mouth, triggered by other words that came in through your ears. The Turing test has always ever stated “don't worry what happens in the middle” and Searle is saying “but for me understanding is part of the details that are happening in the middle!”

So where does the computational theory come in? It comes in because Searle wants to make this argument rigorous! He says, “if your computational theory is true, then there is in fact another way that I can speak a language—words in, words out—where I don't understand a word of Chinese. I am Turing complete, I could memorize a program for speaking Chinese and happen to execute it flawlessly and at no point would my ideas get into Chinese, at no point would the things that I heard mold me. So the Turing test is crap!”

The prominence of the words “I, me” is what invokes the idea of causal sufficiency here, “I have the right inputs and outputs, but I don't understand.” The Turing test is not able to distinguish between multiple consciousnesses instantiated by the same hardware, if such is even possible. The inputs and outputs go into the same box, as far as Turing is concerned as long as only one conversation happens, there's only one person in there.

This does do a great job of defanging the Turing test, because all of the ways that you might weaken the concept to address this major limitation do make it sound completely tautological. “Yeah well something consciousnessy is happening in that box but I don't know what.” / “Okay then why are we even talking about it.” / “Because computers can speak!” / “Right, so we care that computers can speak because they can speak?” / “No, like, we gotta give them rights now, or some shit.” / “John Searle already has human rights, if he's memorized a program that lets him speak Chinese without understanding it, you're saying we need to give that program human rights?” / “Yeah!” / “So uh is it murder if Searle decides that running the program is no longer fun? Is he a slave to this program forever?” / “Uhhhh...”

It doesn't defang the computational model, not directly. But the computational model does imply that VMs exist. We use them every day! And that's all that the Chinese room is, it's running a VM inside of another computer, one consciousness carrying a separate consciousness inside of it, a willful sort of demonic possession. The only thing the Chinese room has to say about this, is that we don't use our language very well if it is true. Philosophers who believe in that will need to generate an alternative language that is able to distinguish between “I am doing it” and “I am sustaining a daemon who is doing it,” because for them that's a real difference, you might have a hundred consciousnesses in your head that you don't have direct access to. That is a necessary part of believing that consciousness is software, you don't know if you're in a VM inside your brain, you don't know if something you're doing is actually secretly a Brainfuck program instantiating another VM inside your consciousness, software embeds within software, that's a core feature of software.

But of course Searle thinks that that's kind of ridiculous because he thinks that it's obvious that consciousness is something that the squishy wetware of the brain does, and this forces him to believe in that causal sufficiency—“my brain is only sustaining one consciousness, namely me,”—which the computationalists cannot ever agree with because that's not how software works. Anyone who believes in that causal sufficiency, even if they don't have the same basis that Searle does for believing in it, also thinks that the computationalists are ridiculous.

But the point is, that's happening at a level way above the Chinese room argument, Chinese room baseball came and went, now this is a whole separate game being played at the same ballpark afterwards.

3.
shagie over 1 year ago | on “Richard Stallman's thoughts on ChatGPT, AI and their impact on humanity”
The argument of the Chinese room is the strong claim.

From https://en.wikipedia.org/wiki/Chinese_room#Complete_argument the conclusion of the complete argument is:

> (C1) Programs are neither constitutive of nor sufficient for minds.

> This should follow without controversy from the first three: Programs don't have semantics. Programs have only syntax, and syntax is insufficient for semantics. Every mind has semantics. Therefore no programs are minds.

---

I personally don't agree with it and believe that there is a flaw in:

> (A2) "Minds have mental contents (semantics)."

> Unlike the symbols used by a program, our thoughts have meaning: they represent things and we know what it is they represent.

While a person may know what they are thinking, examining the mind from the outside it isn't possible to know what the mind is thinking. I would contend that from the outside of a mind looking at the firings of neurons in a brain it is equally indecipherable to the connections of a neural net.

The only claim that "we know what it is they represent" is done from the privileged position of inside the mind.

I would argue that intelligence is more related to the Kolmogorov complexity exhibited by something.

( David Dowe: Minimum Message Length, Solomonoff-Kolmogorov complexity, intelligence, deep learning... https://youtu.be/jY_FuQbEtVM?t=886 )

That the model of GPT is much smaller than its input.

The Chinese room lookup table is enormously large.

If we attempt to relegate GPT as no better than a Chinese room, we can show that the Chinese room look up table is impossible with the amount of data that GPT has access to as part of its model.

If we say that its not a lookup table but instead an enormously complex interplay of inputs and variables, then the distinction between the room that GPT exists in and our own mind breaks down trying to distinguish which is which.

If we want to switch to consciousness, then possibly the argument can progress from there because GPT doesn't have any state once it is run (ChatGPT maintains state by feeding its output back into itself and then summarizing it when it runs out of space). However, in doing this we've separated consciousness and intelligence which means that the Chinese room shouldn't be applied as an intelligence test but rather a consciousness test.

Are GPT 3 and 4 conscious? I'll certainly agree that's a "no". Will some future GPT be conscious and if so, how do we test for it? For that matter, how do we test for consciousness for another entity that we're conversing with (and its not just Homer with a drinking bird tapping 'suggested replies' in Teams ( https://support.microsoft.com/en-gb/office/use-suggested-rep... ))?

4.
cercatrova over 2 years ago | on “A demo of GPT-3's ability to understand long instructions”
This is a philosophical question, really. Is there ever true understanding, or just pattern matching? The Chinese Room thought experiment talks about this:

> Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that they are talking to another Chinese-speaking human being.

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI."

> Searle then supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient papers, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing. If the computer had passed the Turing test this way, it follows, says Searle, that he would do so as well, simply by running the program manually.

> Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that is then interpreted by the user as demonstrating intelligent conversation. However, Searle himself would not be able to understand the conversation. ("I don't speak a word of Chinese," he points out.) Therefore, he argues, it follows that the computer would not be able to understand the conversation either.

> Searle argues that, without "understanding" (or "intentionality"), we cannot describe what the machine is doing as "thinking" and, since it does not think, it does not have a "mind" in anything like the normal sense of the word. Therefore, he concludes that the "strong AI" hypothesis is false.

https://en.wikipedia.org/wiki/Chinese_room

5.
h0l0cube 7 months ago | on “Scientists Find an 'Alphabet' in Whale Songs”
> Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that they are talking to another Chinese-speaking human being.

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI".

(Emphasis added)

If we were to make an analogy to contemporary machine learning, we're talking about the difference between an LLM (with context) and a Markov Chain. 'Understanding' requires novel reuse of recollections. Recollections require memory (i.e. context), and the novel reuse of those recollections require a world model with which to inference.

6.
zwaps over 2 years ago | on “Consciousness is not computation”
The point of the experiment is to think about the individual in the room. You can not say it's irrelevant, because it's the entire point.

The system's response is trivial: Sure, if the room+person combination leads always to a coherent response in Chinese, then the entire system understands Chinese.

I'd go even further: If the person in the room does not understand Chinese, but the system does, then there is some entity that understands Chinese - either a person or an advanced AI, feeding the inputs. Then, from the systems perspective, the person in the room is largely irrelevant.

But this is not the argument: Despite no discernible difference from the outside, the person in the room may either understand Chinese, or they may not.

And so there is a distinction - from the perspective of the individual in the room, that does not depend on the outside observation.

That's all there is to it. It shows that meaning and understanding are not the same as syntactic computation (an important point, to be sure), but it does not show that one can exist with or without the other. By extension, it does not otherwise disprove consciousness as being this or that.

7.
hackinthebochs almost 2 years ago | on “Kissinger: “ChatGPT Heralds an Intellectual Revolution””
>which definitely doesn't think

Why do you think it "definitely" doesn't think? If this is about lacking consciousness, then you should make that explicit. But then the question just gets pushed back further, why are you sure it isn't conscious?

The problem with the Chinese room at its core is the problem of attribution. We want to attribute properties like consciousness or understanding to "things" we are familiar with, and the only sufficient thing in the room is the man. But this intuition misdirects us to bad conclusions. The question to ask is what is responding when questions are being asked of the room. The responses are being generated by the algorithm reified into a causally efficacious process. Essentially, the reified algorithm is a set of properties without objecthood. It's so hard for people to see this as a live option because our cognitive makeup is such that we reason based on discrete entities. Considering extant properties without entities to carry them is just an alien notion.

We can go further and conceptualize a virtual objecthood for the process to help us reason about the entity we interface with in the Chinese room. We can then ask what properties does it have, how much does it understand, and so on. But we can't simply say it doesn't have any of these properties because it's merely a mechanical/computational process.

8.
mcguire about 1 year ago | on “Large language models lack deep insights or a theory of mind”
The Chinese Room gag asserts that, although the room behaves in every way like it is an intelligent Chinese speaker, we can see inside the room and determine that there is nothing there that intelligently understands Chinese.

Searle seems to see a distinction between the "substrates" (which means an LLM cannot be intelligent; it's running on regular computer hardware and there's nothing to be found which "understands"), but unless someone can point out exactly what part of the substrate is intelligent, I am going to continue to suggest that his substrate difference is exactly identical to having a soul or not having a soul.

I, as a materialist, assert that, if you dug through my noggin down to the level of subatomic particles you will never find anything that is intelligent or which understands. (Quantum mechanics does not help here, by the way. It just replaces a (rather naive) determinism with randomness---see Bell's Inequality.) There is no magic going on there. You and your tree are both doing nothing but chemistry and physics (you share something like 50% of your genes, by the way). And that means that a computer could, in principle, behave as intelligently as I do. That's "substrate independence".

Now, whether a given system does do so or not is another question entirely. :-)

9.
roddylindsay over 1 year ago | on “We need a more sophisticated debate about AI”
This is hardly an indefensible position. John Searle, for one, has argued this point for more than 40 years:

"Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese."

Later he wrote:

"I demonstrated years ago with the so-called Chinese Room Argument that the implementation of the computer program is not by itself sufficient for consciousness or intentionality (Searle 1980). Computation is defined purely formally or syntactically, whereas minds have actual mental or semantic contents, and we cannot get from syntactical to the semantic just by having the syntactical operations and nothing else. To put this point slightly more technically, the notion “same implemented program” defines an equivalence class that is specified independently of any specific physical realization. But such a specification necessarily leaves out the biologically specific powers of the brain to cause cognitive processes. A system, me, for example, would not acquire an understanding of Chinese just by going through the steps of a computer program that simulated the behavior of a Chinese speaker."

10.
jodrellblank 3 months ago | on “Stephen Fry – AI: A Means to an End or a Means to Our End?”
> "For one thing, I expect we’d not see so many cases of them chasing (if you will) the prompt and request into silliness."

In the one example I gave you saw one output and declared it "not understanding the concept of bridging". I'm asking specifically that output, how would it look different if ChatGPT had some understanding of the concept of bridging? You're back to arguing "it's not human level!" which was not my claim. My claim is that it's above zero level. In another comment I asked it to use the concept of bridging in new ways, and it provided sentences which have no hits on Google but are plausibly the kind of thing I might see in a book from a human author.

> "There’s no understanding"

Say to your pet "I like it when you do human-like things such as standing on two feet. Come up with more human-style things for more treats" and it won't. You can ask ChatGPT to come up with more uses of the bridging concept, and it does. That is demonstrating understanding at higher than rock level and higher than rat level, and you can't reject that evidence just by repeatedly saying "there's no understanding there's no understanding there's no understanding".

> "they can also be induced to correct the already-correct, so that means nothing."

So can I; if my boss tells me there is an error and I need to correct it, I might correct a non-error to please them. Knowingly ("I'll change this part from correct to wrong if that pleases them") or unknowingly ("if they tell me there is an error there must be one, I'll take a guess that this bit is wrong and put something else here"). Does that show I have no understanding?

> "Every single part of what an LLM does can be replicated with big books of lookup tables, dice, and a list of rules. There’s nowhere for anything to do the understanding to exist."

You're doing the God of the Gaps argument with the human brain - an LED screen is RGB pixels, there's nowhere for a picture of a cat to exist separately from bright and dark pixels. A book is printed characters, there's nowhere for a story to exist separate from blobs of ink on paper. A brain is meat grown from a foetus, uses ~20 Watts of energy, if the blood supply is cutoff then it dies, if it gets too hot or cold then it dies, there are many areas which can be damaged and harm something like leg movement but there is no single area which can be damaged which stops 'understanding' but leaves everything else unchanged, there are no examples of people being decapitated, having no brain, having brain death, and still having 'understanding' provided by whatever other thing you are implying exists and does understanding.

There's nowhere for anything to do the understanding to exist, unless there is a) new physics which aligns perfectly with every observation we have about the brain but also augments it an adds some magical 'understanding' thing which can't be done or simulated in software. b) something non-physical such as a soul which is tied closely to the meat and powered by the food and blood and can't be tied to silicon because reasons. c) ??? As far as I can see this isn't reasoning from anything more convincing than you not wanting to accept the Occam's Razor simpler explanation that a purely physical information processing system can understand.

(Or that humans don't understand and it's all some weird illusion; the picture of the cat is not in the LED screen, it is in the eye of the beholder. The understanding isn't in your behaviour, it's in the beholder's interpretation, I believe you understand because you demonstrate the behaviours of understanding. We are seeing intelligence in others where there isn't any. And that view turned on ourselves is our own perception of our own understanding - we see ourselves identifying patterns, extrapolating patterns, continuing coherent sentences, and conclude that we must have 'understanding' as a thing separate from those behaviours).

> "The thing about the Chinese Room is that we comprehend the entire process, and there’s no room for some unknown factor affecting the output"

We don't comprehend the entire Chinese Room; the instructions that Searle is following are a massive handwave. Does following the instructions require Searle to make human judgements on where to branch? Then it's offloading understanding onto his human brain. Does it not require that but it still outputs coherent responses? Then the instructions must encode intelligence in them in some way - if intelligent behaviour doesn't demonstrate intelligence we're in non-scientific nonsense land.

Peter Cochrane wrote about 'dying by installments' of a human turned into a cyborg replaced bit by bit, Ship of Theseus style. We can do similar and make up a Cochrane's Chinese Brain - instead of a neuron firing and affecting the connected neurons, it raises an alert and Searle walks over and writes down the firing pattern on a scratch pad, walks to all the other relevant neurons, and taps in the firing pattern on an input device, without understanding the information content of the firing pattern. Does the brain keep responding coherent Chinese but no longer understand Chinese?

11.
dTal 4 months ago | on “Anthropic publishes the 'system prompts' that make Claude tick”
I think you have misunderstood Searle's Chinese Room argument. In Searle's formulation, the Room speaks Chinese perfectly, passes the Turing test, and can in no way be distinguished from a human who speaks Chinese - you cannot "pop the illusion". The only thing separating it from a literal "robot that speaks Chinese" is the insertion of an (irrelevant) human in the room, who does not speak Chinese and whose brain is not part of the symbol manipulation mechanisms. "Internal cause and effect" has nothing to do with it - rather, the argument speciously connects understanding on the part of the human with understanding on the part of the room (robot).

The Chinese Room thought experiment is not a distinct "scenario", simply an intuition pump of a common form among philosophical arguments which is "what if we made a functional analogue of a human brain that functions in a bizarre way, therefore <insert random assertion about consciousness>".

12.
emtel almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
I've read one of Searle's popular books (Rediscovery of the Mind) and many other shorter writings. I'm aware he rejects dualism - my point is that the Chinese Room argument wouldn't even make sense to someone who had never been contaminated by dualist notions. Searle rejects it, but it has crept back in, imo.

His arguments against the "systems response" have to stand or fall on their own. The Chinese room doesn't have anything to add. If the "systems response" fails, then we already know a computer can't be conscious without any additional thought experiments.

Lastly, I think he goes much too far in dispelling the mystery of the word "computer". As Dennett and others have pointed out, a working chinese room that displayed clear signs of consciousness would be an artifact far beyond current human comprehension, either in scale or sophistification or both. Searle asks you to imagine a human taking slips of paper and calmly consulting a rule book and producing new slips of paper, and claims that it is "obvious" that this system can't be conscious. Well, I agree, but it's equally obvious that the system couldn't possibly display conscious-seeming behavior.

The best defense of the CR that I've heard is that it is only intended to prove that a _purely formal_ system (i.e. GOFAI) can't be conscious. I find this more plausible, but I still don't think the CR adds anything to that argument. And, to reiterate the previous paragraph, I think if you had a formal system that everyone agreed _seemed_ conscious, it would be such a complex artifact that it wouldn't be obviously unconscious in the way that, say, an eliza bot is.

13.
boole1854 about 2 years ago | on “An End to Pornography, Sophistry, and Panty Raids: on Epstein's Edge.org (2...”
> Why shouldn't sufficiently-sophisticated symbol manipulation, producing results the same as "conscious understanding" (which seems to be the same as "semantics" as Searle was using it), be identical to "conscious understanding"?

This is what the Chinese Room argument is trying to answer. In the room, there is a human, and the human is performing symbol manipulation according to the rules listed in various books. However, (1) the human has conscious understanding of various topics, but Chinese is not one of them, and yet (2) the outsider is able to send and receive messages in / out of the room in Chinese, indistinguishable from communicating with someone who does understand Chinese. This shows that an entity performing symbol manipulation which results in a Chinese conversation is not the same as that entity understanding a Chinese conversation. To say otherwise would require that the conjunction of 1 and 2 is not possible, but it is clearly possible (or at least this is clear to most people).

There are various replies which address the argument coherently.

The illusionism theory of consciousness denies that the conjunction of 1 and 2 is possible because 1 on its own is not possible. That is, the human in room does not have conscious understanding because no one has conscious understanding of anything (since consciousness does not exist). This theory has not widely caught on because most people think it is just too obvious that conscious experience does exist.

Another reply is that the conjunction of 1 and 2 is not possible because as soon as the human executes enough of the symbolic manipulation rules, they do in fact come to understand Chinese. The trouble with this reply is that it runs up against the strong concrete experience that we have which shows this is not remotely how gaining an understanding of something works.

A few other remarks:

(1) The argument does not show, and is not intended to show, that building a conscious machine is impossible. Searle explicitly thought, and wrote in his original paper, that it was possible to build such machines.

(2) The argument does not show, and is not intended to show, that conscious machines require biological substrates. Searle wrote in his original paper that it was an open question what substrates could be used to build conscious machines, and that it might well be possible to build conscious machines without any biological components.

(3) A reply which says, "alright, the human in the room does not understand Chinese, but maybe the 'room itself' does have a conscious understanding of Chinese" is perhaps interesting. Some people, including Searle, have debated how likely this is. However, it is beside the point of the original argument, which is just about showing a difference between the concept of syntax and the concept of semantics. (And, again, the argument shows that there is a difference by exhibiting a counterexample where an entity is able to follow arbitrarily complex syntactic rules and yet that entity does not have a corresponding understanding of the natural language which the rules implement.)

(4) For some, e.g. people in software development who work with formal languages on a regular basis, the fact that there is a difference between syntax and semantics is incredibly obvious. If you think it is simply obvious that there is a difference, then congratulations, you understand the concepts involved. In that case, Searle's argument is superfluous, and it's not intended for you. His argument is directed at those working in philosophy of mind or related fields who were not familiar enough with the difference between syntax and semantics to understand that there is a clear difference.

edited to add a reply about Point 1 begging the question: just to clarify, the ARGUMENT I posted above is not Searle's argument. It was an arguing I was giving to the previous poster to illustrate the fact that Searle's argument was not equivalent to claiming that consciousness does not exist in humans because individual neurons have no understanding of what the entire brain is doing. My starting point was taking Searle's conclusion as a given, in order to work out consequences from it.

14.
emtel almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
The Chinese Room is a really, really bad thought experiment. It's just an attempt at misdirection. It asks you to imagine a computer (one that operates on slips of paper according to rules that are written down in a big book), puts a human inside that computer, and then says that because the human doesn't understand what the computer program is doing, the computer program isn't conscious.

But this is just question-begging. The experiment purports to resolve the question "can a computer be conscious", and answers it by showing you a computer and claiming that it is not conscious.

Worse, the Chinese Room is straight-up Cartesian dualism. By placing the human inside the computer, Searle is preying on your Cartesian intuition that there must be a theatre of the mind where reality is experienced. And since Searle's homunculus doesn't understand Chinese, nothing can be experienced, therefore no conscious experience is happening in this system.

If I'm wrong, and the Chinese Room thought experiment is conclusive, it must be possible to reformulate it without the human. After all, the human does nothing but follow rules in a purely mechanistic fashion. Can anyone offer a reformulation of the experiment, that has no human, that they still feel is convincing?

15.
epups almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
>Quoting Searle himself, "The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have."

> I think the most succinct description of his error is substituting the (lack of) understanding of a part of the system (the man) for the understanding of the entire system (the rules and file cabinets, etc.). But I'm interested in learning that I'm mistaken.

As you know, there have been many replies to this thought experiment, and some of the most interesting ones (to me) go in the direction you went here, ie, where is "understanding" occuring? The most basic version of the Chinese Room does intend to make you see yourself literally as a man who does not understand any Chinese and is just asked to look up symbols in a list. Perhaps that man doesn't understand Chinese, but the room as a whole at least gives the impression that it does.

However, I think the most important aspect is not this "intuition pump" as Daniel Dennett calls it. To me, what is key here is that we can all agree that such a Chinese Room, or ChatGPT for that matter, does not necessarily replicate the fundamental mechanisms of human cognition. Then, it follows that other human properties such as awareness or qualia do not necessarily emerge from such cognitive architectures in the same way that it emerges from our brains.

To me, Searle's point is ultimately that we don't know enough about the human mind to be able to judge whether it can be replicated artificially. And now that we have almost literally developed a Chinese Room, we can see that clearly. The arguments you bring up in your last paragraph are a great example of that, it's just very hard to conceive that this thing is conscious at all, even though it is capable of producing output that could convince people of that.

Regarding Searle's quote that you brought up, I think "solely on that basis" is doing a lot of heavy lifting there, but it does align with what I said previously. He is saying that simply producing intelligent output, like in 1974 translation would represent, does not mean you are reasoning in a human way.

16.
jedharris 10 months ago | on “Sora: Creating video from text”
That matches how philosophers typically talk about the Chinese room. However the Chinese room is supposed to "behaves as if it understands Chinese" and can engage in a conversation (let us assume via text). To do this the room must "remember" previously mentioned facts, people, etc. Furthermore it must line up ambiguous references correctly (both in reading and writing).

As we now know from more than 60 years of good old fashioned AI efforts, plus recent learning based AI, this CAN be done using computers but CANNOT be done using just ordinary if - then - else type rules no matter how complicated. Searle wrote before we had any systems that could actually (behave as if they) understood language and could converse like humans, so he can be forgiven for failing to understand this.

Now that we do know how to build these systems, we can still imagine a Chinese room. The little guy in the room will still be "following pre-written scripted algorithmic procedures." He'll have archives of billions of weights for his "dictionary". He will have to translate each character he "reads" into one or more vectors of hundreds or thousands of numbers, perform billions of matrix multiplies on the results, and translate the output of the calculations -- more vectors -- into characters to reply. (We may come up with something better, but the brain can clearly do something very much like this.)

Of course this will take the guy hundreds or thousands of years from "reading" some Chinese to "writing" a reply. Realistically if we use error correcting codes to handle his inevitable mistakes that will increase the time greatly.

Implication: Once we expand our image of the Chinese room enough to actually fulfill Searle's requirements, I can no longer imagine the actual system concretely, and I'm not convinced that the ROOM ITSELF "doesn't have a mind" that somehow emerges from the interaction of all these vectors and weights.

Too bad Searle is dead, I'd love to have his reply to this.

17.
nullc about 2 years ago | on “Galactic settlement of low-mass stars as a resolution to the Fermi paradox”
> Dr. John Searle's objection is that software is not a mind, and goes on to explain why with his Chinese Room thought experiment.

I always found that experiment to be extremely unconvincing, to put it mildly. Using that framework I can prove that you are not conscious.

I construct a "english room". You are in the back without any contact to the outside world. A Chinese speaker in the front can pass messages to you from the outside world, in English by laborious copying. Clearly you aren't conscious because the Chinese speaker can't understand the English.

Or, alternatively, no one would dispute a single neuron is not conscious and can't understand english. I pass signals into your neurons and through some electrochemical magic you reply in english, yet the neurons involved don't understand english therefor you aren't conscious.

All Searle has done is picked out one component of a composite system, concluded that in isolation it has no clue whats going on, and from that concluded the system as a whole doesn't.

We have a hard time clearly defining consciousness, and it's not clear if some AGI would be conscious (maybe it's just an emergent phenomena)-- it's also not clear if there would be any particular advantage in it being so. But in any case, I don't think the chinese room is a useful argument for anything.

18.
kghe3X over 2 years ago | on “Google Sidelines Engineer Who Claims Its A.I. Is Sentient”
A few things wrong with this in my opinion.

1. If simulation of a mental phenomenon is not equivalent, in essence, to the phenomenon itself, then what's a meaningful difference between the two? What is this elusive essence of the phenomenon and why is it a requirement in any meaningful definition of intelligence, consciousness, sentience, etc? If we build a silicon brain some day that passes as human, does it matter that its essence is different from a carbon-based brain?

2. Searle's Chinese Room neglects to acknowledge that the room itself is what would be exhibiting intelligence, not the homonculus within. His argument fails by presuming without evidence that consciousness is not an emergent phenomenon.

3. Of course we don't expect the computer to urinate on the desk, just as we don't expect a silicon mind to produce serotonin. That's irrelevant to the question of whether or not it exhibits the qualities or behaviors that we associate with consciousness.

I guess I just don't understand why there needs to be this added, untestable, metaphysical requirement attached to all of these words. And my question to those who suggest this is, what would it convince you otherwise?

19.
gianpaj over 1 year ago | on “Searle vs. Boden (1984) – Consciousness and Understanding in Machines [vide...”
The Searle vs. Boden debate revolves around the philosophical question of whether machines can truly possess consciousness and understanding, or if they are limited to simulating intelligent behavior without genuine understanding. This debate primarily involves John Searle, an American philosopher, and Margaret Boden, a cognitive scientist and philosopher.

John Searle is best known for his thought experiment called the Chinese Room argument, which he presented in 1980. In this argument, Searle imagines himself locked inside a room with a set of instructions that enable him to respond to written Chinese symbols without understanding the Chinese language. He argues that even though he can generate appropriate responses and appear to understand Chinese, he does not genuinely understand the language because he is simply following a set of syntactic rules without grasping the meaning behind them. Searle extends this argument to claim that computers, no matter how complex or sophisticated, can only manipulate symbols according to algorithms but lack true understanding.

Margaret Boden, on the other hand, represents a more optimistic view about machine understanding and consciousness. She acknowledges Searle's Chinese Room argument but argues that it overlooks the potential of computational systems to exhibit genuine understanding. Boden emphasizes the distinction between "strong" AI, which aims to create machines with consciousness and understanding comparable to human intelligence, and "weak" AI, which focuses on simulating intelligent behavior without claiming genuine understanding. She argues that while Searle's argument may hold for weak AI, it does not necessarily refute the possibility of strong AI.

Boden suggests that machine understanding can emerge through the combination of syntactic manipulation of symbols (as Searle describes) and semantic processing, which involves the interpretation and mapping of symbols to meaning. She highlights the importance of cognitive architecture, which encompasses not only the symbolic manipulation but also the grounding of symbols in sensory-motor experiences. According to Boden, by providing machines with rich sensory inputs and allowing them to interact with the physical world, it becomes possible for them to develop genuine understanding and consciousness.

20.
simonh over 2 years ago | on “The Chinese Room Thinks”
This is a decent refutation of Searle, mainly because Searle's characterisation of the Chinese Room is that it just encodes and processes syntax. Engaging in a conversation requires much more than syntax, it requires analysing and decomposing the meaning of what is being said, which for example could require constructing a model of a situation and reasoning about activities and changes in a situation. It might require critiquing suggested strategy for playing Chess for example, or commenting on economics. Actual conversation takes a lot more than just syntax, so by characterising it in this simplistic way he's stacking the deck in the reader's mind against the Chinese room.

This result is incredibly impressive. There's a lot of analysis going on here, far beyond mere processing of syntax. In this sense it exposes the inadequacies of Searle's critique of AI. It won't kill the Chinese Room argument though, it's supporters will simply extend it beyond just syntax. They will still claim there is no thinking mind producing these results and they're probably right, after all as I understand it this system is essentially a hierarchy of nets that filer input to generate output, there's no persistent running cognitive process that does anything like thinking on an ongoing basis.

The Chinese Room argument is still bunk though for many other reasons.

21.
ftxbro over 1 year ago | on “Chinese Room”
> Searle supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient papers, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing.

This is an interesting and valuable thought experiment!

> The Chinese room argument holds that a digital computer executing a program cannot have a "mind", "understanding", or "consciousness", regardless of how intelligently or human-like the program may make the computer behave.

This doesn't seem to follow from it. For example what if they made artificial pieces of brain made from computer chips and replaced my brain with it bit by bit like a brain of Theseus. This is how your brain already works to some degree, the atoms in your brain aren't the original ones. I don't see how they say this replaced brain cannot have a 'mind' or 'understanding'.

22.
virchau13 over 2 years ago | on “Consciousness is not computation”
I like Scott Aaronson's response[1] to the argument:

> So, class, how might a strong AI proponent respond to this argument? Duh: you might not understand Chinese, but the rule book does! Or if you like, understanding Chinese is an emergent property of the system consisting of you and the rule book, in the same sense that understanding English is an emergent property of the neurons in your brain. Like many other thought experiments, the Chinese Room gets its mileage from a deceptive choice of imagery -- and more to the point, from ignoring computational complexity. We're invited to imagine someone pushing around slips of paper with zero understanding or insight -- much like the doofus freshmen who write (a+b)^2=a^2+b^2 on their math tests. But how many slips of paper are we talking about? How big would the rule book have to be, and how quickly would you have to consult it, to carry out an intelligent Chinese conversation in anything resembling real time? If each page of the rule book corresponded to one neuron of (say) Debbie's brain, then probably we'd be talking about a "rule book" at least the size of the Earth, its pages searchable by a swarm of robots traveling at close to the speed of light. When you put it that way, maybe it's not so hard to imagine that this enormous Chinese-speaking entity -- this dian nao -- that we've brought into being might have something we'd be prepared to call understanding or insight.

[1]: https://www.scottaaronson.com/democritus/lec4.html

23.
wruza over 1 year ago | on “Ask HN: AI read books, Human also. What's the difference?”
The standard argument against the chinese room is that if the 'symbols' are at lower level, audio, video stimuli, and you have enough scale, then you can imagine intelligence, emerging somehow.

Qualian chauvinism. The ‘observer’ field doesn’t have to be locally or real-ly connected. E.g. a current is real, but it doesn’t flow through a real-connected surface.

Chinese room is conscious and intelligent by definition if the above is true, just not human-like conscious and intelligent.

The juice of this question is that we may want to determine it in a practical binary sense, when there may be a spectrum or an entire space of it.

Sadly it can all boil down to a boring “shut up and interact” principle again.

24.
mcguire over 1 year ago | on “We need a more sophisticated debate about AI”
As an alternative to Loquebantur's comment, Searle's is trivially reversed: he has no way to demonstrate that he has "semantic" capabilities.

On the first hand, all communication is purely syntactic; letters, words, gestures, and every other formulation of communication is the transfer of symbols, which cannot carry semantic information. They can point to it, they can imply it, but you'll have to dance pretty hard to actually prove the transfer of semantic information. If you and I both have some semantic concept associated with the symbol "China" in our mental landscapes, neither I nor you, nor anyone, has any way determining whether that is the same semantic concept. In fact, it almost assuredly isn't, since those semantic concepts would have to be built from our unique personal histories, which are unique.

On the second, the fundamental assumption of semantic communication is that I feel like I have semantic shenanigans going on in my mental life and Searle sort of looks like me, so I should assume that he has semantic stuff going on. Not only is that not a particularly well-founded assumption, I can't even be sure I have semantic contents in my mental life. Introspection isn't exactly reliable, right?

On the third hand, you have a choice: if you do not agree with Penrose, et al, on the quantum nature of the magic of mind, then Searle's "biologically specific powers" are chemistry. Now, I never actually took a chemistry class, but I don't believe there is any chemical reaction that comes with a label of semantics. It's all physics anyway, which I know cannot introduce semantics.

If you do agree with Penrose (which I understand Searle does), the situation is not actually better. Quantum mechanical antics are random, right? Bell's Inequality and all? Being fundamentally, quantum mechanically, random is not in any sense better from a semantic standpoint than being syntactically deterministic.

So my bottom line is that I don't know if I have semantic contents, I absolutely don't know if Searle has semantic contents (and I'm not feeling generous), and I do not know of any way that he could prove to me that he does have "semantic contents". So, yeah.

You notice I only addressed the second paragraph from Searle. That is because the fundamental flaw in his Chinese Room "demonstration" is on obvious display in his final sentence, "The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese"; it is purely, straight up, dualistic. He is asserting that the person in the room has some magic property of "consciousness or intentionality" and that if the person in the room does not have the "semantic capability" of understanding Chinese, then it is impossible despite all evidence that anything in the scenario has the semantic capability of understanding Chinese.

You could put a native Chinese speaker in the room, reading and responding to the inputs, and he claims it would have "consciousness or intentionality"; you then replace her with me (who didn't exactly dazzle at Spanish and Latin) and even if I produce the same outputs in response to the inputs as she does, there is no "consciousness or intentionality". The difference is simply a magic, intangible, imperceptible soul.

Whoo. Went off on that one.

25.
bonoboTP almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
In some more "rational"-oriented circles, the Chinese Room argument is seen as obviously wrong, outdated and mysticistic, for example see Dan Dennett.

The Chinese Room would need to be so enormous, and it would work so slowly, that it's not a good intuitive analogy. It also introduces a homunculus actor that reads, interprets and understands the instructions, which muddles the analogy and focuses our attention to the conscious quality of the person following the instructions.

26.
ryandv 7 months ago | on “Here comes the Muybridge camera moment but for text”
To this list I would absolutely add Julian Jaynes' "The Origin of Consciousness in the Breakdown of the Bicameral Mind."

> simulations of conscious thought entirely via prediction language tokens

Jaynes goes so far as to assert that language generates consciousness, which is characterized by (amongst other features) its narrative structure, as well as its production of a metaphor of our selves that can inhabit a spatiotemporal mental space that serves as an analog for the physical world; the mental space where we imagine potential actions, play with ideas, predict future outcomes, and analyze concepts prior to taking action in the "real, actual" world.

The generation of metaphors is inextricably linked to the psychotechnology (to pull a word from vocabulary discussed by John Vervaeke in his "Awakening from the Meaning Crisis" series) of language, which is the means by which one object can be described and elaborated by its similarity to another. As an etymological example: the Sanskrit word "bhu" which means "to grow" forms the basis of the modern English verb "to be," but predates lofty abstract notions such as that of "being," "ontology," or "existence." It's from the known and the familiar (plant or animal growth) that we can reach out into the unknown and the unfamiliar (the concept of being), using (psycho-)technologies such as language to extend our cognition in the same way a hammer or a bicycle extends our body.

There is something here about language being the substrate of thought, and perhaps even consciousness in general as Jaynes would seem to assert in Book I of his 1976 work, where he spends a considerable amount of time discussing metaphor and language in connection to his definition of "consciousness."

There are also questions of "intentionality" and whether or not computers and their internal representations can actually be "about" something in the way that our language and our ideas can be "about" something in the physical (or even ideal) world that we want to discuss. Searle and the "Chinese room" argument come to mind.

Turing famously dodged this question in his paper "Computing Machinery and Intelligence" by substituting what is now called the "Turing test" in lieu of answering the question of whether or not "machines" can "think" (whatever those two words actually mean).

27.
pringk02 over 1 year ago | on “A non-technical explanation of deep learning”
Searle's argument in the Chinese Room is only that passing the Turing Test isn't enough to prove evidence of Mind (capital 'm' to distinguish it at the philosophical jargon term, and all it entails). He does hold the stance that he doesn't think Computationalism (in the style of Dennet) is correct. I'm not sure if he personally feels the Chinese Room argument refutes that stance as a whole, but I believe the general consensus is that, as originally formulated in his essay, it does not stand as a total refutation of Computationalism, without reading between the lines or squinting your eyes a bit. Searle does have a wider stance that he does not think computations can have things equivalent to mental states, especially intentionality. Obviously there is a whole separate debate to his correctness there, but I'm skipping over it to just discuss the Chinese Room

That passing the Turing Test is not enough to exhibit evidence of Mind is not that controversial today. GPT-4 could easily pass the Turing Test as it was originally formulated. There are not many out there that think it possesses conciousness or intentionality or any mental states at all really. We'd generally agree now that passing the Turing Test is only a step towards creating an actual artificial mind (how large or small a step is still up for debate).

Anyway, all this is a tangent as I still don't understand why the original commenter feels this article provides a refutation of the Chinese Room argument when it seems (to me) to reinforce it. I'm just curious on that perspective and was interested in hearing more.

28.
twicetwice almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
I think Searle's Chinese Room argument is sophistry, for similar reasons to the ones you suggest—the proposition is that the SYSTEM understands Chinese, not any component of the system, and in the latter half of the argument the human is just a component of the system—but Searle does believe that quantum indeterminism is a requirement for consciousness, which I think is a valid response to the argument you've presented here.

29.
3PS almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
There is an excellent refutation of the Chinese room argument that goes like this:

The only reason the setup described in the Chinese room argument doesn't feel like consciousness is because it is inherently something with exponential time and/or space complexity. If you could find a way to consistently understand and respond to sentences in Chinese using only polynomial time and space, then that implies real intelligence. In other words, the P/NP distinction is precisely the distinction underlying consciousness.

For more, see:

https://www.scottaaronson.com/papers/philos.pdf

30.
Barrin92 almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
The notion that the Chinese room is 'mysticistic' is one of the most bewildering responses to it to the point where I don't think people who make that argument actually understood Searle.

Searle, simplified holds that consciousness is a property of very particular biochemical systems, not a disembodied thing that exists independent of the matter it inhibits.

It's the rational-oriented circles ironically enough who almost dabble in a Cartesian Dualism by positing "computation" or "minds" as distinct from matter. In fact with on occasion almost religious-sounding implications.

31.
jll29 over 1 year ago | on “Artificial Consciousness Remains Impossible (Part 2)”
Searle's Chinese Room thought experiment is about mastering the Chinese language.

There are two issues I have with it as it was originally presented:

1. Mastering a language is not the same as having consciousness.

2. Who "knows Chinese" in the Chinese Room thought experiment? I would say neither Searle nor Searle in/as part of the Chinese Room "speak" Chinese. But it is

fair to say that the book that the fictional Searle follows can be

seen as a model of the Chinese language; or at least the combination of

the book and Searle as its "processor" collectively are an implemented

operational model of the Chinese language. And a model of Chinese is

NOT the same as being skilled at conversing in Chinese (executing the

model in a particular way). Other posters here have drawn analogies from

music evoking certain subjective emotions, and again a semantic network

that has concept nodes labelled with the names of these emotions is not

the same as experiencing these emotions, although the semantic network

can be said to constitute a model of sorts of the music's effects. But

again, model(x) != qualia(x).

32.
vundercind 3 months ago | on “Stephen Fry – AI: A Means to an End or a Means to Our End?”
> If you cannot suggest any way its output would look different to how it looks now and instead have to resort to changing the subject, shouldn't you retract that claim?

For one thing, I expect we’d not see so many cases of them chasing (if you will) the prompt and request into silliness. The code attempts to satisfy prompts in a transparently mechanical fashion, which is part of why they so gleefully (if you will) mislead. There’s no understanding. You can ask them to correct and they might, but they can also be induced to correct the already-correct, so that means nothing. To the extent we fix that, it’s not by adding any factor that might represent understanding, it’s further prompting that amounts to “follow these patterns slightly differently”. The fix isn’t, so far, “teaching” them to understand. Maybe we’ll get there! But we don’t appear to be anywhere near that yet.

> Then you must say a human doesn't understand. For what else is there in a human brain except a finite amount of learned behavioural rules for signal inputs and outputs? Learned over a billion years of evolution in the structure, and filled in by a lifetime of nurture.

The thing about the Chinese Room is that we comprehend the entire process, and there’s no room for some unknown factor affecting the output—or for a known factor that might be processing something like what we mean by understanding (let alone consciousness, say).

Every single part of what an LLM does can be replicated with big books of lookup tables, dice, and a list of rules. There’s nowhere for anything to do the understanding to exist. It’s not that we have to be confused by part of it for that to be there—I’m not saying mystery is a necessary component—just that this process doesn’t have a place for that to be.

33.
antognini almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
You raise good questions, but I would really suggest listening through the lectures because Searle addresses all your points in a lot of detail. (It's a lot of material, but it's well worth it and he's an entertaining lecturer.) It's actually a bit funny that you bring up Cartesian dualism because Searle is pretty clear from the outset that he's of the opinion that Descartes's dualism was the worst thing ever to happen to philosophy. He certainly has no truck with it.

The question Searle is raising with the thought experiment is where the understanding of Chinese is happening. It's clear that the understanding is not happening with the homunculus. But if not there, then where is it happening? Some functionalists responded with the "systems response" which is that the system of the room plus the homunculus is what has the understanding even though no element of the system taken individually has understanding. He goes through a pretty detailed reply to the systems response, but the basic argument against it is that it's incoherent. There's no well defined thing that is the system. You can remove the man from the room and put him under a tent in a field. Where does the system end and the rest of the world begin?

You are of course correct that the experiment can be reformulated without the human, but that's exactly the point. A computer is mechanistically doing exactly the same thing as the man in the room. But if the man has no understanding and the room has no understanding and the system has no understanding, removing the human from the equation changes nothing. The reason Searle included the human is because saying "computer" invokes a lot of mystery to a lot of people as to what's going on. But a computer is doing nothing different than the man in the room.

34.
hnfong almost 2 years ago | on “'I Worked on Google's AI. My Fears Are Coming True'”
The Chinese Room thought experiment is kind of a different mistake. In the Chinese room argument, Searle attempted to imagine an AI system that was "obviously not sentient" because it was made up of hard rules and facts (presumably legible to us humans), and it was argued that it should be obvious that the system wasn't sentient.

For the ChatGPT case it's not clear the Chinese room argument even applies. First, it's not even a Chinese room, it's trained by a neural network, an a neural network is inspired by the structure of our brains. There's no "set of instructions in English" for anyone to follow, but it's a rather overwhelming set of neurons somehow producing the results that it was trained on, and as an outsider we have no idea what's going on inside individual neurons -- just like our brains.

I'm seeing a lot of people claim that "we know how ChatGPT is created, and so it's not sentient", maybe this looks like the Chinese room fallacy (I'm calling it as such), but it's like claiming we know the laws of physics (Schrödinger's equation etc.) and then claiming we know how the brain works because it's all quantum physics. The fact is we know how to build GPT but we don't actually know how it works.

Besides what happens to humanity when we learn more and more about how our own brains work? I'm pretty comfortable with the idea that we're not sentient (yay), but I'm pretty sure those same people would then come up with other nonsensical arguments to prove that they are different from the robots we created.

35.
danbmil99 over 1 year ago | on “AI vs. AGI vs. Consciousness vs. Super-Intelligence vs. Agency”
> John Searle, broadly, framed the issue correctly in the 80s and the standard critiques are wrong.

The standard critiques are not wrong, IMNSHO. Searle's Chinese Room is facile mind-poison. It is an unfalsifiable hypothesis.

What if I could simulate physics down to the molecular level, including simulating a human brain? Would that be conscious? If not why not?

And if I ran that simulation (a bit slowly, granted) by having that guy from the Chinese Room manually run the simulation, painstakingly following the instruction code of that simulation, would the fact that the simulation is being implemented by someone who unrelatedly is conscious himself, have any bearing on the scenario?

Searle's argument here is "Not Even Wrong".

36.
krick about 2 years ago | on “This Week's Hype”
Searle is proposing what he supposes to be a proof of a computer being unable to have mind in the same sense humans have mind, and thus human intelligence being something more than a "mere computation". While in fact his argument proves absolutely nothing, and I'm honestly surprised that anyone buys into this bullshit. Actually, I think that the question itself is meaningless, but it's besides the point what I think, I'll just try to explain where's the fallacy. And since his argument is "reductio ad absurdum" it's easiest to counter by assuming the opposite, and showing that there's nothing absurd about it, i.e. there's no self-contradiction here. Just keep in mind that I'm not trying to prove the opposite, I'm only assuming it for the sake of argument.

So, the opposite assumption is that there's nothing more to intelligence than a "mere computation", and all that stuff like inner dialog and "a sense of self" are just side-effects of it. That way, there's no difference between "strong AI" and "weak AI" in his terminology, no difference between "understanding" and "simulation of understanding". Then, of course, a digital computer has a mind and understands Chinese, and also the Chinese room has a mind and understands Chinese. But Searle doesn't understand Chinese and this is supposed to be a contradiction.

But there's no contradiction here, because Searle is not the Chinese room. He isn't the mind, in this model he is to Chinese room what electricity is to a digital computer, or what neurotransmitters are to human brain. So, yeah, electricity doesn't understand Chinese, serotonin doesn't understand Chinese, single human neuron doesn't understand Chinese, but Turing-test-passing computer, Chinese brain and Chinese room all do. Searle is just a detail in the Chinese room — tiny and insignificant compared to a book, containing a complete description of a program that speaks Chinese and is able to form coherent (and persistent!) opinions on every possible topic in the world.

So, the fallacy is equating Searle's role in the hypothetical Chinese room to the whole digital computer that speaks Chinese and passes Turing test with breeze.

37.
kbelder almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
Maybe there's a small human inside the human's brain, not understanding the rules the human is following, and it's only the innermost human that is really conscious?

I think Searle's Chinese room argument is absolutely nonsensical, a bit like arguing about philosophical zombies. It's an argument that only makes any sense if you already are committed to mysticism or dualism.

38.
notahacker almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
"if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis"

The italics summarise it pretty neatly. It's an argument explicitly framed against Turing's more dubious thought experiment. If even a conscious being in the room can follow instructions, retrieve data and perform operations on it related to symbol manipulation flawlessly without having any sort of "understanding" of anything the symbols actually correspond to, there's no reason to deduce that the running part of a silicon-based machine must from the quality of the symbol outputs it can emit when plugged into a big enough library. Critics' insistence that this makes the "error" of neglecting the possibility that ongoing "understanding" (as opposed to inert symbolic representation of an absent writer's understanding) takes place in the books are actually irrelevant to this point, as well as more than a bit weird. Living outside a Chinese room, I also improve my communication skill and interpret others' understanding by interacting with books, but I wouldn't consider the books themselves a constituent part of my thought processes.

As you point out yourself, GPT has structural properties which make it very easy to classify as an entirely different thing from a human mind despite the similarity of outputs it is capable of producing, and the hypothetical room is even more dissimilar. The possibility it can produce output tokens which correspond to abstractions which humans interpret as consistent with human thought is not evidence that "thought" resides in patterns of abstract representation, not the physics of the organism. We know language is lossy.

39.
masswerk over 2 years ago | on “A demo of GPT-3's ability to understand long instructions”
Well, this, the Chinese Room, is still pretty much a behavioristic work-around (as it is still attempting to argue without any reference to internal states.)

Even, if we don't (clearly) understand what "understanding" means, or, at least, aren't able to provide a sane definition, we do know about the semantics of the term and the kind of connotations that come with it. Like a reflexive component. (Which wasn't much of a problem in the age of behaviorism, as this had to be ignored by requirement anyway. If there is no acknowledged difference between a human and a pigeon, what is the problem with computers, as far as the model is concerned?) So we do have a notion of the semantic field and its implications. And these are, well, quite disastrous for this purpose.

40.
akasakahakada over 1 year ago | on “Consciousness in AI: Insights from the Science of Consciousness”
I would say that Chinese Room is just simply a subclass of recursive nonsense that solely rely on ignorance of emergent properties of substance.

Likewise Chinese Roomer must never reject the idea that human means nothing because human's composite parts a.k.a atoms do not think or talk or being 'conscious'.

But they can't reject. Because they must stand for the point that human is conscious. Simultaneously this bring to a contradictory.

It is mind boggling why folks think it is valid argument.

41.
epups almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
> In any case why is the Chinese Room always stated as if it has a clear conclusion rather than "this doesn't really prove anything" if we don't know enough to say either way?

To me, it does have a clear and definitive conclusion, which is that mere intelligent output does not mean you are replicating human intelligence, or any higher order mechanism such as consciousness. We don't know enough to tell that it doesn't have any consciousness, but that's beside the point.

You mentioned that a computer that could simulate every molecule of a human brain would also likely replicate sentience. Of course the tricky part is how do you prove that assertion, if all you have is output? If I transfer your brain to an advanced computer as you describe, can I conclude that you're conscious based on what you tell me? I don't think so, because with present technology I could likely make a passable version of your writing output with a LLM. To me that's the real value of the Chinese Room, which is to expose precisely this dillemma. People wrote all sorts of replies to it in order to tackle that - you may be interested in reading about Dennett's p-zombies if you haven't already.

42.
sourcepluck about 1 month ago | on “OpenAI, Google and Anthropic Are Struggling to Build More Advanced AI”
Searle's Chinese Room Argument springs to mind:

https://plato.stanford.edu/entries/chinese-room/

The idea that "human-like" behaviour will lead to self-awareness is both unproven (it can't be proven until it happens) and impossible to disprove (like Russell's teapot).

Yet, one common assumption of many people running these companies or investing in them, or of some developers investing their time in these technologies, is precisely that some sort of explosion of superintelligence is likely, or even inevitable.

It surely is possible, but stretching that to likely seems a bit much if you really think how imperfectly we understand things like consciousness and the mind.

Of course there are people who have essentially religious reactions to the notion that there may be limits to certain domains of knowledge. Nonetheless, I think that's the reality we're faced with here.

43.
notahacker over 2 years ago | on “Is DALL-E 2 ‘gluing things together’ without understanding their relationsh...”
I see it the other way round.

The Chinese Room and the brain of a Chinese-speaking person are completely different physical processes. Looked at on an atomic level, they have almost nothing in common. Mind-body dualists may or may not agree that the room is not "conscious" in the way a human is, but if consciousness is purely a material process, I can't see how the materialist can possibly conclude all the relevant properties of the completely dissimilar room and person are the same.

Those that would argue the Chinese Room is "conscious" in the same way as the Chinese person are essentially arguing that the dissimilarity of the physical processes is irrelevant: the "consciousness" of the Chinese person doesn't arise from molecules bouncing around their brain in very specific ways, but exists at some higher level of abstraction shared with the constituent molecules of pieces of paper with instructions written in English and outputs written in Chinese.

The idea our consciousness exists in some abstract sense which transcends the physics of the brain is not a new one of course. Historically we called such abstractions souls...

44.
PaulDavisThe1st over 2 years ago | on “We don't know what makes things sentient–so let's stop acting like we do”
Hofstadter and Dennett addressed the Chinese Room argument in a way that I find completely compelling. Either you are not familiar with it, or do not find it compelling. They call it the "system" position IIRC, and essentially, the entire room ("system") is conscious, even though none of the pieces are.

45.
visarga about 2 years ago | on “CICERO: An AI agent that negotiates, persuades, and cooperates with people”
What does this say about the Chinese room argument?

Is the model a Chinese room or does it understand the game. If it's just a Chinese room, how come it is so effective, if it understands the game how can it be possible with just a rule machine?

46.
gameman144 almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
The Chinese Room thought experiment seems like a weird example, since the same could be said of humans.

When responding to English, your auditory system passes input that it doesn't understand to a bunch of neurons, each of which is processing signals they don't individually understand. You as a whole system, though, can be said to understand English.

Likewise, you as an individual might not be said to understand Chinese, though the you-plus-machine system could be said to understand Chinese in the same way as the different components of your brain are said to understand English.)

Moreover, even if LLMs don't understand language for some definition of "understand", it doesn't really matter if they are able to act with agency during the course of their simulated understanding; the consequences here, for any sufficiently convincing simulation, are the same.

47.
int_19h about 1 year ago | on “Large language models lack deep insights or a theory of mind”
Searle's Chinese room is a good example of begging the question.

As for the rest of it, the LLM is basically "raw compute". You need a self-referential loop and long-term memories for it to even have the notion of self. But looking at it at that level and discounting it as "incapable of thinking" is missing the point - it's the larger system of which LLM is one part, albeit a key one (and which we're still trying to figure out how to build) that might actually be conscious etc.

48.
int_19h 7 months ago | on “Is artificial consciousness achievable? Lessons from the human brain”
That is still nonsense. If you take the whole room as a system and integrate it into your brain as a subsystem, then yes, you will understand Chinese.

The reason why we have to speak of the room and the person inside as a system is because the real magic is in the instructions. The fact that they are performed by a self-aware human is completely irrelevant to the setup and is only there to confuse the matter.

In your other example with two people, viewing them as a system doesn't make much sense because one of those people is redundant - you can leave just the person who speaks Mandarin, and that is sufficient for the whole to function. So they alone are "the system". And it also operates based on instructions, except that those instructions are stored in the person's head and executed by low-level processes in the brain.

Searle believes that consciousness cannot be simulated as a digital computation, period. Given that any other physical process can be, this requires a belief that consciousness is somehow magically different from any other physical process in some unspecified way (that appears to be conjured out of thin air solely to make this one argument, at that). That is not materialism.

49.
wcoenen almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
The Chinese Room thought experiment can also be used as an argument against you being conscious. To me, this makes it obvious that the reasoning of the thought experiment is incorrect:

Your brain runs on the laws of physics, and the laws of physics are just mechanically applying local rules without understanding anything.

So the laws of physics are just like the person at the center of the Chinese Room, following instructions without understanding.

50.
notahacker almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
> It's sleight of hand because the sentience of the human in the system is irrelevant. The human is following a trivial set of rules, and you could just as easily digitize the books and replace the human with a microcontroller. Voila, now you have a Chinese-speaking computer program and we're back to where we started.

Substituting the microcontroller back is... literally the point of the thought experiment. If it's logically possible for an entity which we all agree can think to perform flawless pattern matching in Chinese without understanding Chinese, why should we suppose that flawless pattern matching in Chinese is particularly strong evidence of thought on the part of a microcontroller that probably can't?

Discussions about the plausibility of building the actual model are largely irrelevant too, especially in a class of thought experiments which has people on the other side insisting hypotheticals like "imagine if someone built a silicon chip which perfectly simulates and updates the state of every relevant molecule in someone's brain..." as evidence in favour of their belief that consciousness is a soul-like abstraction that can be losslessly translated to x86 hardware. The difficulty of devising a means of adequate state tracking is a theoretical argument against computers ever achieving full mastery of Chinese as well as against rooms, and the number of books irrelevant. (If we reduce the conversational scope to a manageable size the paper-pusher and the books still aren't conveying actual thoughts, and the Chinese observer still believes he's having a conversation with a Chinese-speaker)

As for your alternative example, assuming for the sake of argument that the head scan is a functioning sentient brain (though I think Searle would disagree) the beleaguered paper pusher still gives the impression of perfect understanding of Chinese without being able to speak a word of it, so he's still a P-zombie. If we replace that with a living Stephen Hawking whose microphone is rigged to silently dictate answers via my email address when I press a switch, I would still know nothing about physics and it still wouldn't make sense to try to rescue my ignorance of advanced physics by referring to Hawking and I as being a union with collective understanding. Same goes for the union of understanding of me, a Xerox machine and a printed copy of A Brief History of Time.

51.
myrmidon over 2 years ago | on “DALL-E 2 generates images of Kermit The Frog in various films”
Do you find the Chinese Room argument convincing?

Do you feel that the human mind is more than an "appropriately" trained "biological" neural network?

What do you consider the limits of a DALL-E like system compared to a "true" mind?

My personal opinion is that the Chinese Room argument is fancy handwaving that crucially relies on never being explicit about what it means by "understanding", combined with an appeal to intuition.

I strongly believe that there is nothing "magical" about the human mind or brain (that could not be replicated artificially), and thus that a comparably trained, appropriately designed system ("DALL-E successor") OR a copy OR a simulation of a human brain would be all just as capable and "understanding"/"conscious" as another human...

52.
throwaway09223 almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
Theory of mind is substantially different from the Chinese Room argument. Theory of mind relates to an ability to predict the responses of another entity/system. An LLM is specifically designed to predict responses.

In contrast, the Chinese Room argument is essentially a slight of hand fallacy, shifting "understanding" into a layer of abstraction. It describes a scenario where the human's "understanding of Chinese" is dependent on an external system. It then incorrectly asserts that the human "doesn't understand Chinese" when in fact the union of the human and the human's tools clearly does understand Chinese.

In other words, it's fundamentally based around an improper definition of the term "understanding," as well as improper scoping of what constitutes an entity capable of reasoning (the human, vs the human and their tools viewed as a single system). It smacks of a bias of human exceptionalism.

It's also guilty of begging the question. The argument attempts to determine the difference between literally understanding Chinese and simulating an understanding -- without addressing whether the two are in fact synonymous.

There is no evidence that the human brain isn't also a predictive system.

53.
earth_walker almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
My completely naive reply to the Chinese room is that as stated doesn't learn from the inputs and outputs.

Let's modify it slightly to include a second stream of responses to the room's answers. Then make a guidebook that adds new rules based on the growing history of interaction.

Now the rulebook doesn't just compute a 1-1 map of input to output: each interaction updates not only its experience, but all present and future answers. An input to the system invokes memory, taps into experience, evaluates nuance.

Given lots of time and interaction and a good broad set of questions and responses, does the system understand Chinese? If it does not, then what is missing between it's understanding and ours?

54.
HeavenBanned over 2 years ago | on “Thinking as Computation (2011) [pdf]”
The Chinese room never made sense to me because, even though the person passing the arcane symbols through the door doesn't _understand_ what is being communicated doesn't mean that's not how it works in _our_ brain. Our individual neurons (or aggregate neurons depending on how they're working and firing based on symbol triggering) don't know what symbols they're processing either. So unless Searle is saying humans don't know any languages on some fundamental level, it's kind of useless because then we'd just end up admitting (something unsavory) that nobody knows anything barring some homunculus in our brains or even worse, we can never truly have free will. Those are scary implications.

55.
93po almost 2 years ago | on “Facebook LLAMA is being openly distributed via torrents”
I don't really understand the brain or AI enough to meaningfully discuss this, but I would wonder if there's some aspect of "intentionality" in the context of the Chinese Room where semantic search with embeddings still "doesn't count".

I struggle with the Chinese Room argument in general because he's effectively comparing a person in a room following instructions (not the room as a whole or the instructions filed in the room, but the person executing the instructions) to the human brain. But this seems like a crappy analogy because the better comparison would be that the person in the room is the electricity that connects neurons (instructions filed in cabinets). Clearly electricity also has no understanding of the things it facilitates. The processor AI runs on also has no understanding of its calculations. The intelligence is the structure by which these calculations are made, which could theoretically could be modeled on paper across trillions of file cabinets.

As a fun paper napkin exercise, if it took a human 1 second to execute the instructions of the equivalent of a neuron firing, a 5 second process of hearing, processing, and responding to a short sentence would take 135,000 years.

56.
retrac over 1 year ago | on “Does GPT-4 Understand What We’re Saying?”
> There’s a kind of understanding which is just coordination. For example, I could say, “Can you pass me the spoon?” And you’d say, “Here it is.” And I say, “Well, you understood me!” Because we coordinated. That’s not the same as generative or constructive understanding, where I say to you, “I’m going to teach you some calculus, and you get to use that knowledge on a problem that I haven’t yet told you about.”

I think hang-up -- the automatic insistence on understanding and projection of anthropomorphic traits -- is that our instincts strongly tell us that mere statistical coordination shouldn't be capable of such feats. Maybe the closest analogy is how simple game space search could produce software much better than us playing chess. Something that seemed like it required human thought could be fully mechanized, and mechanized in a way that humans do not think, and yet achieve superior results. We're used to it now, but it must have unsettled people ~50 years ago in a similar way to how ChatGPT does.

I consider it a very pressing problem in philosophy, actually. There's the concept of the "philosophical zombie", which is a being which would look like, talk like, and by all external indicators, sound, think, and feel like a human being. But which has no sentience. The lights aren't on. It's a pure automaton.

Some philosophers are sort of torn on whether the notion even makes sense, and is properly framed. But among those who do think it is a properly defined idea, it's roughly split, according to a survey I saw a few years ago [1], on whether they are, in theory, metaphysically possible. Is there something about consciousness that is fundamental to explaining everything humans can do? Would the laws of nature, philosophy, the mind, whatever, allow such a thing to exist?

I think LLMs tend to hint somewhat at their metaphysical possibility, which is rather disturbing to many people. It's much more comforting to project understanding on to it, than to envision the possibility of mindless human-like intelligence.

[1] https://philpapers.org/archive/BOUWDP

57.
Barrin92 almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
I agree that Searle is often too strong in his claims, but here is what I think people need to take away from the Chinese Room and why it is useful.

That solving a problem, or an array of problems that a conscious mind can solve may be perfectly able to be solved by a non-conscious machine. That the consciousness part may not be relevant in the problem-solving process, and that consciousness may be unique to living beings who have distinct physiology, anatomy, ways to process information, that computers as they exist now do not have. That a simulation is not necessarily the same thing as the thing it simulates, and does not have all of its properties.

These are good questions none of which are mystic and which following from Searle's argument. And I think you're really underestimating to what degree people are lured in by the 'magic' of neural nets because else we wouldn't be in this thread. A neural net that does some function approximation is not 'slightly conscious' unless we're going full in on panpsychism. Even from a functionalist perspective, the neural net performs nothing that is reasonably involved in what we consider to be responsible for consciousness. Nobody ever claimed a TI-83 is 'slightly conscious', or that Stockfish is merely because it plays exceptional chess.

58.
tsimionescu over 2 years ago | on “A hormone may boost cognition in Down syndrome”
I will start by saying that I personally do believe that the brain is a computer, with the minnd simply being software.

However, given our current level of understanding of how the mind works, we can't say for sure that certain thoughts are algorithmic. For example, we don't know an algorithm by which new systems of axioms can be created, nor even a general algorithm for proving a theorem in a given system of axioms, beyond brute force searching. Given that human minds do these things relatively often, it's unlikely they are arriving at it by brute force search in the enormous domain of mathematical statements.

So, we can't say we've ruled out the possibility that the mind is using some unknown non-computational process for finding novel theorem proofs. Note that proof checking is algorithmic, only proof discovery is potentially not.

Now, to me extrapolating this to a non-computational theory of mind smells a lot like a God-of-the-gaps argument, so I don't buy it. But it is worth remembering the limits of what is actually proven to be computable.

I've taken this example somewhat directly from Turing's paper on computation - there, he bases the model of the Turing machine specifically on a mathematician doing rote mechanical calculation with a pen and paper (the machine state is their brain state, the tape is the notebook, the writing head is the pen, and the reading head is their eyes). However, non-rote calculations are arguably left out of the paper's claims (or at least most of them).

Much more famous arguments about non-computational theories of mind are, to me, much less convincing. The most famous is probably The Chinese Room, where the idea is that a man locked in a room receives a question on paper written in Chinese characters, and follows lookup tables and English instructions to return another paper written with Chinese characters, but can't really be said to understand Mandarin even if he is always returning the right answer for the question asked (or so the author claims). Another such famous argument is Mary's Room, where Mary is also locked in a room where nothing has the color red, but where she gets to rid everything that can be known about the physics and biology of the color red and its perception in the human brain (much more knowledge than we have today), but where she is assumed to still have a new experience once she leaves the room and actually sees red for the first time directly.

59.
danparsonson 3 months ago | on “Chain of Thought Empowers Transformers to Solve Inherently Serial Problems”
> ... He rests his whole argument on the premise that the operator is illiterate in Chinese, or at least has no access to the semantics of the material stored in the Room.

...and yet outputs semantically correct responses.

> That's plainly not the case with ChatGPT, or it couldn't review its previous answers to find and fix its mistakes.

Which is another way of saying, ChatGPT couldn't produce semantically correct output without understanding the input. Disagreeing with which is the whole point of the Chinese Room argument.

Why cannot the semantic understanding be implicitly encoded in the model? That is, why cannot the program I (as the Chinese Room automaton) am following be of sufficient complexity that my output appears to be that of an intelligent being with semantic understanding and the ability to review my answers? That, in my understanding, is where the genius of ChatGPT lies - it's a masterpiece of preprocessing and information encoding. I don't think it needs to be anything else to achieve the results it achieves.

A different example of this is the work of Yusuke Endoh, whom you may know for his famous quines. https://esoteric.codes/blog/the-128-language-quine-relay is to me one of the most astonishing feats of software engineering I've ever seen, and little short of magic - but at its heart it's 'just' very clever encoding. Each subsequent program understands nothing and yet encodes every subsequent program including itself. Another example is DNA; how on Earth does a dumb molecule create a body plan? I'm sure there are lots of examples of systems that exhibit such apparently intelligent and subtly discriminative behaviour entirely automatically. Ant colonies!

> And you certainly would not get a different response, much less a better one, from the operator of a Chinese Room simply by adding "Think carefully step by step" to the request you hand him.

Again, why not? It has access to everything that has gone before; the next token is f(all the previous ones). As for asking it to "think carefully", would you feel differently if the magic phrase was "octopus lemon wheat door handle"? Because it doesn't matter what the words mean to a human - it's just responding to the symbols it's been fed; the fact that you type something meaningful to you just obscures that fact and lends subconscious credence to the idea that it understands you.

> It's just a vacuous argument from square one,

and it annoys me to an entirely-unreasonable extent every time someone brings it up. Add it to my "Stochastic Parrot" and "Infinite Monkeys" trigger phrases, I guess.

With no intent to annoy, I hope you at least understand where I'm coming from, and why I think those labels are not just apt, but useful ways to dispel the magical thinking that some (not you specifically) exhibit when discussing these things. We're engineers and scientists and although it's fine to dream, I think it's also fine to continue trying to shoot down the balloons that we send up, so we're not blinded by the miracle of flight.

60.
msla over 1 year ago | on “Yann LeCun and Andrew Ng: Why the 6-Month AI Pause Is a Bad Idea [video]”
These thought experiments rest on the assumption that human brains are magically complex such that being "just" anything we can understand is inherently lesser. Searle's Chinese Room experiment falls for the same fallacy, more-or-less, plus the fallacy that "human plus book" isn't a single system but "huge number of individual neurons" somehow is.

61.
PittleyDunkin 15 days ago | on “An 83-year-old short story by Borges portends a bleak future for the intern...”
I think the chinese room thought experiment is another stronger exploration of whether or not computers are conscious (and, I believe, is no longer sufficient). I think people into AI should at least be familiar with its implications (or lack thereof) about human cognition.

https://en.wikipedia.org/wiki/Chinese_room

62.
tgv over 2 years ago | on “DALL-E 2 generates images of Kermit The Frog in various films”
> My personal opinion is that the Chinese Room argument is fancy handwaving

It isn't hand-waving. It's against it, really. It's a thought experiment that encourages a sceptic attitude towards jumps in understanding mental processes. The operator in the Chinese Room doesn't understand Chinese. While the translations are excellent, he or she wouldn't be able to go out in the street and ask for a glass of water if their life depended on it. Hence, a computer that mechanically translates Chinese cannot be automatically assumed to understand Chinese.

The argument doesn't need to explain exactly what understanding means. We all (sort of) know what it means. The same goes for e.g. attention. That's what makes it so hard to define what strong AI is and how to verify it. The Turing Test famously tries to decide this (without defining anything, I might add), and the Chinese Room is a good argument against it being the proper test.

63.
mannykannot about 2 months ago | on “Language is not essential for the cognitive processes that underlie thought”
The brain is faster than the Chinese room, but other than that, yes, that's the so-called systems reply; Searle's response to it (have the person in the room memorize the instruction book) is beside the point, as you can teach people to perform all sorts of algorithms without them needing to understand the result.

As many people have pointed out, Searle's argument begs the question by tacitly assuming that if anything about the room understands Chinese, it can only be the person within it.

64.
lazyant over 1 year ago | on “Richard Stallman's thoughts on ChatGPT, AI and their impact on humanity”
The Chinese Room Argument holds that a digital computer executing a program cannot have a "mind", "understanding", or "consciousness",[a] regardless of how intelligently or human-like the program may make the computer behave.

https://en.wikipedia.org/wiki/Chinese_room

65.
kazinator over 2 years ago | on “Consciousness is not computation”
My rebuttal to the poorly considered Chinese Room thought experiment:

I'm going to use this Wikipedia text as the backdrop:

> Searle then supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient papers, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing. If the computer had passed the Turing test this way, it follows, says Searle, that he would do so as well, simply by running the program manually.

> Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that is then interpreted by the user as demonstrating intelligent conversation. However, Searle himself would not be able to understand the conversation. ("I don't speak a word of Chinese," he points out.)

The reason he would not be able to follow the conversation is that he's being the CPU, and not the computer program. The computer program (according to the Strong AI hypothesis) understands the conversation, evidence for it being that it can converse with Chinese speakers and convince them that they are conversing with a human being.

The content of the papers stuffed into the filing cabinets to keep track of the program state contains (somehow) the representation and behavior of understanding, not John Searle, the pencil-and-paper-pushing executor of the program (who doesn't even hold the content of those papers in his own mind, let alone failing to understand any of it.)

Searle misunderstands computers and software; his argument amounts to the assertion that it is the CPU which must necessarily be the seat of the computer's intellect, and since we know that CPU to be a very simple machine with no understanding of the data flowing through it, there is no intellect, QED.

Fact is that, in the thought experiment, the Chinese Room as a whole appears to be intelligent; it can converse with a Chinese speaker and pass the Turing test. That's what is intelligent: the room, and not the clerk sitting inside it who pushes the pencil.

It's like insisting that individual neurons must be intelligent (conscious, sentient, ...).

By Searles' argument, a computer doesn't actually know how to browse the web because an Intel chip doesn't understand what is TCP/IP or HTML.

I can't believe this twaddle was ever taken seriously, but that's philosophy for you.

If you want a good reason why kids should learn to code, there it is: so they have a fighting chance at not being duped by stuff like this.

66.
glenstein 8 months ago | on “Daniel Dennett has died”
The deep dive version of this convo might be a topic for another time, but the most concise answer I can give is to grab a copy of I Am A Strange Loop by Douglas Hofstadter, and flip to chapter 2 where he discusses an idea proposed by John Searle about the so-called "terribly thirsty beer can." This is an argument from Searle that he believes is a knockdown argument against the idea of consciousness embodied in something that isn't a biological mind as we know it. It is, and I do not say this lightly, it is just stunningly naive.

Hofstadter's dispensation of it in chapter 2 is to my mind, a completely decisive dressing down of the fundamental naivety of Searle's ideas about minds. I can't find any convenient quotation of the passage on the internet, but in my copy of the book it's page 29 chapter 2.

I think it puts on perfect display how truly ridiculous Searle's ideas are, and I think the Chinese room idea is similarly discreditable, and ultimately I think that Searle was more a fraud who more appropriately belongs in the category anti-science apologists along the lines of intelligent design proponents, rather than a positive contribution to the canon of Western analytic philosophy. And the extent to which he has gained influence in academic philosophy is something I take as discrediting of it as a field, to the extent that Searle is it's standard bearer. So if the commenter above cited him instead of Wittgenstein I would be cheering it on as a legitimate observation.

67.
IshKebab almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
> To me, Searle's point is ultimately that we don't know enough about the human mind to be able to judge whether it can be replicated artificially.

There's string circumstantial evidence that we do. And really "computers can simulate the physics in the brain" is the null hypothesis.

In any case why is the Chinese Room always stated as if it has a clear conclusion rather than "this doesn't really prove anything" if we don't know enough to say either way?

> And now that we have almost literally developed a Chinese Room

I don't think so. The GPTs are currently still very far from the complexity of the human brain, and they are missing many features that may make a big difference to consciousness - for instance the ability to learn while running.

So while it may be fairly easy to say ChatGPT isn't conscious/sentient, that isn't the question. It's whether computers theoretically can't be conscious because consciousness comes from some physical property that they can't reproduce (like quantum microtubule crankery).

68.
simonh over 2 years ago | on “The singularity is close?”
The Chinese room argument is a parlour trick that uses scale as a distraction. It posits a person in a room manipulating symbols to produce intelligent seeming outputs. It says, see, it’s absurd to think a person in a room with a stack of symbols could emulate intelligence.

But let’s say the room contains many billions of people, it is the size of a planet, and it contains racks of many trillions of symbols, and it spends millions or billions of years to produce an output. That’s more like the scale of a sophisticated computer system, or a brain.

Does that sound much like a man in a room with some symbols? No. Does it sound like that could do complex calculations and produce sophisticated and perhaps even intelligent outputs? Well, given enough time and scale, yes why not?

The Chinese room is pure misdirection and it amazes me anyone falls for it. There’s really no actual argument there.

69.
notahacker over 2 years ago | on “We don't know what makes things sentient–so let's stop acting like we do”
> Unfortunately, that argument applies to you, yourself. I mean, presumably you know that you yourself are intelligent, but you must take it on faith that everyone else is. We all could just be a kind of Chinese Room, as far as you know. Communicating with us is not a sure way to know whether we are "really" sentient because we could just be automatons, insensate but sophisticated processes, claiming falsely to be just like you.

I'm not sure the conclusion that Chinese people might not understand Chinese either is the best counterargument to Searle's thought experiment or its conclusion effective use of words alone doesn't constitute sentience. At no point does the difficulty in establishing what Chinese people do and don't understand rescue the possibility the non-Chinese speaker knows what's going on outside his room, and most of the arguments to the effect that Chinese people understand Chinese (they map real world concepts to words rather than words to probabilities, they invented Chinese, they're physiologically quite similar to sentient me, they appear to act with purpose independently from communication) are also arguments to the effect that text-based neural networks probably don't.

In a trivial sense, it's true I can't inspect others' minds, and despite what everyone says I could be the only thinking human being in existence. But I have a lot of reason to suspect that physiologically similar beings (genetically almost identical in some cases) who describe sensations in language they collectively invented long before I existed which very strongly matches my own experiences are somewhat similar to me, and that an algorithm running on comparatively simple silicon hardware which performs statistical transformations on existing descriptions of these sensations written by humans is simply creating the illusion of similarity. Heading in the other direction, humans can also be satisfied by the output of "article spinners" used by spammers to combine original texts and substitute enough synonyms to defeat dupe detectors, but I'm pretty sure the quality of their writing output shouldn't be given precedence over our knowledge of the actual process behind their article generation when deciding if they're sentient or not...

70.
twblalock almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
John Searle's Chinese Room argument seems to be a perfect explanation for what is going on here, and should increase in status as a result of the behavior of the GPTs so far.

https://en.wikipedia.org/wiki/Chinese_room#:~:text=The%20Chi....

71.
dzdt about 2 years ago | on “ChatGPT passes the 2022 AP Computer Science A free response section”
[me]

Write a five paragraph essay on Daniel Dennett's responses to Searle's Chinese room thought experiment.

[GPT (text-davinci-003)]

In his responses to John Searle's Chinese Room thought experiment, philosopher Daniel Dennett offers a defense of strong artificial intelligence (AI). In his argument, Dennett maintains that Searle's experiment does not adequately account for how human beings use language, and that strong AI can potentially be achieved if the right conditions are met.

Dennett's first response to the Chinese Room argument is that language use is not merely an informational process, as Searle suggests. Rather, Dennett argues that language use involves a complex system of meaning and interpretation, and that meaningful language use cannot be reduced to a strictly algorithmic process. For example, Dennett points out that two people may use the same words but interpret them differently, depending on various contextual factors. Thus, the Chinese Room thought experiment fails to account for the complexity of language and its use.

Dennett also challenges the assumption that a computer can never understand language as well as a human being. To this end, he argues that if a computer is given enough data, it can learn to process language in a similar way to humans. This could involve the computer being programmed with a large corpus of data, such as literature, so that it can learn to understand language in context.

Dennett's third point is that the Chinese Room argument neglects the fact that computers can be programmed to interact with their environment. In other words, a computer can be programmed to respond to various external stimuli, just as a human being would. This means that a computer could potentially interact with its environment in a meaningful way, just as a human being can.

Finally, Dennett argues that the Chinese Room argument assumes that a computer must understand language in the same way as a human being, which is an oversimplification of how language works. Rather, he suggests that a computer could potentially understand language in its own way, and be able to process information in a meaningful way.

In sum, Daniel Dennett's responses to Searle's Chinese Room thought experiment offer a defense of strong AI, arguing that computers can potentially be programmed to understand language in a meaningful way. Dennett's argument draws upon various theoretical and practical considerations, and provides an interesting counterpoint to the conventional view that computers can never truly understand language in the same way as a human being.

72.
mort96 almost 2 years ago | on “The AI Mirror Test, which smart people keep failing”
My thoughts on the Chinese room thought experiment is: the person in the room does not know Chinese, but the person+room system knows Chinese. I believe the correct analogy is to compare the AI system to the person+room system, not to just the person.

How do you back up the statement that "for us to consider a machine sentient, it needs to function in a similar way to us"? On what basis do you categorically deny the validity of a sentient being which works differently than a human?

73.
hoseja over 2 years ago | on “Consciousness is not computation”
Why keep bringing up the Chinese Room. The guy is just a substrate, it's the necessarily intelligent set of rules he's performing (for astronomically large time) that is conscious.

74.
int_19h 7 months ago | on “Is artificial consciousness achievable? Lessons from the human brain”
> If someone now asks you questions about colored objects you can answer them, but I assume you grant that neither the colorblind person, nor the machine, nor the two as a system have conscious experiences of color vision as you have.

It really depends on the setup. If the system is primed with knowledge of what color various things are (so e.g. it can say that grass is green because it is in the knowledge base), then, no, it does not experience color vision. It's just regurgitating facts.

On the other hand, if you actually have some kind of sensor that is capable of perceiving color, and you provide the output of that sensor to the colorblind person inside the room, who interprets the signals (say, represented as numbers) according to the rules, and those rules result in the system as a whole being able to say things like "apple is red" when presented with a red apple, then yes, I would in fact argue that the system does consciously experience color vision.

> And I don't think your assertion about Searle's belief is correct.

Searle claimed that computers "merely" use syntactic rules to manipulate symbol strings, but have no "understanding" of semantics, and that Chinese room demonstrates that this is not sufficient for consciousness. This was not just about correctly modelling outward functions, though - quite obviously, the room has a lot going on inside, and of course you can model neural nets without physically simulating neurons, either. Quite frankly Searle's attempt to make some kind of qualitative distinction between biology and computation is nonsensical, because it's the same physics all the way down, and it is all representable as computation.

75.
Jipazgqmnm almost 2 years ago | on “Theory of Mind May Have Spontaneously Emerged in Large Language Models”
It's called Chinese Room: https://en.wikipedia.org/wiki/Chinese_room

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese?

To me: If you can't tell, it effectively doesn't matter.

76.
mhink over 1 year ago | on “Simply explained: How does GPT work?”
In my understanding of the Chinese Room example, the resolution to the argument is that the *human* may not understand Chinese, but the *system as a whole* can be said to understand it.

With this in mind, I think asking whether ChatGPT *in and of itself* is "conscious" or has "agency" is sort of like asking if the speech center of a particular human's brain is "conscious" or has "agency": it's not really a question that makes sense, because the speech center of a brain is just one part of a densely interconnected system that we only interpret as a "mind" when considered in its totality.

77.
ben_w 9 months ago | on “Claude 3 catches researchers testing it”
> There is no reason to believe that human intelligence is a stochastic parrot. We have a large body of research to indicate emotion and thought are interdependent [1]. Cognition is not a simple, discrete quantity.

And? My thoughts can create a mood, my mood can influence my thoughts. Do you seriously believe that this isn't a trivially easy thing to get even just a LLM to do by itself with the right system prompt, let alone more complex models? Sure, my guess is that it would probably just be role-playing rather than having qualia, but so what?

> The Chinese room argument is arguing intentionality

I agree with IshKebab about the Chinese room. Just because Searle was trying to argue that because the human within the room doesn't know Chinese therefore the system as a whole doesn't, doesn't mean his argument works. I'd say it doesn't prove anything either way, because the human in that room serves the same role as a homunculus in a human brain, leading to an infinite regression problem: https://en.wikipedia.org/wiki/Homunculus_argument

78.
ineptech over 1 year ago | on “What Kind of Mind Does ChatGPT Have?”
Sure, maybe? I don't have a clear idea of where consciousness begins. If I were trying to make a conscious LLM, I would use a sort of feedback loop of automated prompts talking about its input in realtime. Like, if you showed it a picture of a tree, it might auto-prompt itself with, "What do I think of that tree?" and after it answers itself, "Why do I think that?", etc, and use the responses to update its own weights. I can imagine something akin to a personality developing out of that, and those automatic prompts sure sound similar to the "inner voice" in my head.

But what I meant was, even though I don't know exactly what's necessary for consciousness to exist, in humans or computers or dolphins, I feel like LLMs are just obviously too simple to be it. I've had a casual interest in NLP since the 90s and have followed AI and NLP progress to some extent, and up to a few years ago, I had this vague notion that language processing might be so complex that you need a human-like consciousness to wield it. I think chatgpt is the ultimate refutation of that. It's proof that, whatever part of our brains processes natural language and converts our stored knowledge of the world into words, it can run standalone - it doesn't need eyes or ears or thoughts or a childhood or some digital version of hormones or a thalamus or anything like that. Just input data and an algorithm to store relationships between tokens is enough.

That's a really interesting result, philosophically. Think for a moment about how we retain the information we take in. Consider, e.g. a college student who reads a dozen books about WW1, and then writes an essay about it a month later. It's not realistic that they can remember every sentence and every statistic; to retain that information, they have to condense it, summarize it, notice trends, and presumably forget large parts of what they read and don't retain. That is more or less what we call learning, right? Well, what chatgpt tells us that either that that learning process is entirely unnecessary to answer questions about WW1 based on books about WW1, or that the way a gpt turns training data into weights is roughly analogous to it. Either way, that's a surprising result. And (to bring this back full circle) I think it's surprising for the same reason many people thought Searl's Chinese Room thought experiment was unrealistic: the assumption that, since the process by which we turn input into stored knowledge and thence into a series of parseable tokens in English is hideously complex and mysterious, that a computer program that can do the same will need to be complex and mysterious as well, so complex that we can't rule out the possibility of consciousness. Whereas in reality, you can look at chatgpt and say, "No I pretty much understand exactly how this works, and you can tell it's not conscious because it's not using any cpu."

I may be talking to myself in a dead thread, and am certainly rambling, but it helps me sort things out and develop my understanding - very much like that loop of self-prompting I mentioned earlier :)





whitten 16 days ago | on “DeepThought-8B: A small, capable reasoning model”
The Chinese Room is a theoretical room that contains a “Chinese speaker” but actually when given a text to ‘understand’ actually just looks up the text in a huge number of words inside until it finds a way to find a response and then just outputs the response as its reply

2.
jedharris 10 months ago | on “Sora: Creating video from text”
That matches how philosophers typically talk about the Chinese room. However the Chinese room is supposed to "behaves as if it understands Chinese" and can engage in a conversation (let us assume via text). To do this the room must "remember" previously mentioned facts, people, etc. Furthermore it must line up ambiguous references correctly (both in reading and writing).

As we now know from more than 60 years of good old fashioned AI efforts, plus recent learning based AI, this CAN be done using computers but CANNOT be done using just ordinary if - then - else type rules no matter how complicated. Searle wrote before we had any systems that could actually (behave as if they) understood language and could converse like humans, so he can be forgiven for failing to understand this.

Now that we do know how to build these systems, we can still imagine a Chinese room. The little guy in the room will still be "following pre-written scripted algorithmic procedures." He'll have archives of billions of weights for his "dictionary". He will have to translate each character he "reads" into one or more vectors of hundreds or thousands of numbers, perform billions of matrix multiplies on the results, and translate the output of the calculations -- more vectors -- into characters to reply. (We may come up with something better, but the brain can clearly do something very much like this.)

Of course this will take the guy hundreds or thousands of years from "reading" some Chinese to "writing" a reply. Realistically if we use error correcting codes to handle his inevitable mistakes that will increase the time greatly.

Implication: Once we expand our image of the Chinese room enough to actually fulfill Searle's requirements, I can no longer imagine the actual system concretely, and I'm not convinced that the ROOM ITSELF "doesn't have a mind" that somehow emerges from the interaction of all these vectors and weights.

Too bad Searle is dead, I'd love to have his reply to this.

3.
gessha 7 months ago | on “Scientists Find an 'Alphabet' in Whale Songs”
Reminds me of the Chinese room [1] argument: Does a computer really understand Chinese language if it can respond to Chinese inputs with Chinese outputs?

[1] https://en.wikipedia.org/wiki/Chinese_room

4.
SilasX over 1 year ago | on “Bytes are all you need: Transformers operating directly on file bytes”
That sounds like a variant of the Chinese Room problem: if a non-Chinese speaker follows a rule book to text-chat in Chinese with someone on the other side, does s/he, in actuality, “speak Chinese”?

https://en.m.wikipedia.org/wiki/Chinese_room

5.
willprice89 about 1 year ago | on “Three things that LLMs have made us rethink”
I've always felt the Chinese Room thought experiment was founded on a complete misunderstanding of bilingualism. The only "algorithm" that can effectively transform text between English and Chinese would be one that understands the source and target texts like a human. So the Chinese Room is really just a more complicated Turing Test - a computer can only convince an observer that it "understands Chinese" if it is also able to convince an observer that it understands the conversation the same way as a human, in either language.

6.
tim333 over 1 year ago | on “Simply explained: How does GPT work?”
I'm not sure many people take the Chinese Room argument seriously these days. I mean it basically proposes a room that is equivalent to a lookup table and says it can't understand much hence computers in general can't understand. The "hence..." bit makes no sense really.

7.
larryfreeman almost 2 years ago | on “'I Worked on Google's AI. My Fears Are Coming True'”
I believe that it is straight forward to show how Searle's Chinese Room argument can be applied to ChatGPT.

The idea is not the "process" for building the rules but the "process" where the rules are applied. True sentience requires a "true" understanding of the information that is being presented (otherwise, it is not reflective of sentience but rather trickery).

The Chinese Room applies because ChatGPT or any other computer is generating text purely through logic, that is, logical rules.

To make this point clear, let's assume just for argument's sake that at some quantum level, the electrical activity underlying a computer is partially sentient (I am not making this claim -- just using it as an example). Even if this were true, this would not prove that the sentience is involved with the text being generated by the same computer.

This underlying sentience would be like the "person" in the Chinese room. The text generated are the Chinese words which would be completely independent from the "sentience" following the rules.

Generating text at sufficient complexity to fool the average person does not prove that "sentience" is occurring. Generating text consistently in a way that is only explainable through sentience is what is required. ChatGPT, because it is easily shown to be flawed, does not rise to this level of evidence.

In the Chinese Room example, the complexity of what could be displayed by the rulebooks would have a limited complexity (though, the surprise of ChatGPT is that this complexity is greater than many of us would have supposed). Just like the story of Clever Hans the Horse (see https://en.wikipedia.org/wiki/Clever_Hans), once the limitation of the process is realized, it becomes clear that there is not "true" understanding of what is generated which means that the rules, in themselves, do not reflect "true" sentience.

8.
Jipazgqmnm almost 2 years ago | on “Theory of Mind May Have Spontaneously Emerged in Large Language Models”
It's called Chinese Room: https://en.wikipedia.org/wiki/Chinese_room

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese?

To me: If you can't tell, it effectively doesn't matter.

9.
dragonwriter almost 2 years ago | on “What ChatGPT can't do”
> ChatGPT seems like it is "just" a very advanced Chinese Room(?).

The basic definition of a Chinese Room is that the output is indistinguishable from a human; the whole point of the thought experiment is to illustrate a claim that external behavior cannot be taken as proof of actual understanding.

A very advanced Chinese Room would be a system without general understanding that nevertheless behaved as if it were a generally superhuman intelligence.

10.
simonh over 2 years ago | on “The singularity is close?”
The Chinese room argument is a parlour trick that uses scale as a distraction. It posits a person in a room manipulating symbols to produce intelligent seeming outputs. It says, see, it’s absurd to think a person in a room with a stack of symbols could emulate intelligence.

But let’s say the room contains many billions of people, it is the size of a planet, and it contains racks of many trillions of symbols, and it spends millions or billions of years to produce an output. That’s more like the scale of a sophisticated computer system, or a brain.

Does that sound much like a man in a room with some symbols? No. Does it sound like that could do complex calculations and produce sophisticated and perhaps even intelligent outputs? Well, given enough time and scale, yes why not?

The Chinese room is pure misdirection and it amazes me anyone falls for it. There’s really no actual argument there.

11.
earth_walker almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
My completely naive reply to the Chinese room is that as stated doesn't learn from the inputs and outputs.

Let's modify it slightly to include a second stream of responses to the room's answers. Then make a guidebook that adds new rules based on the growing history of interaction.

Now the rulebook doesn't just compute a 1-1 map of input to output: each interaction updates not only its experience, but all present and future answers. An input to the system invokes memory, taps into experience, evaluates nuance.

Given lots of time and interaction and a good broad set of questions and responses, does the system understand Chinese? If it does not, then what is missing between it's understanding and ours?

12.
fallingknife about 1 year ago | on “How to Think Computationally about AI, the Universe and Everything”
The Chinese room problem always seemed pointless to me precisely because we have to make assumptions about how it works.

The whole thing basically boils down to "there's this room that can speak perfect Chinese, and we don't know how it works, or how your brain works, but somehow we can say with absolute certainly that they couldn't possibly be the same."

13.
visarga about 2 years ago | on “CICERO: An AI agent that negotiates, persuades, and cooperates with people”
What does this say about the Chinese room argument?

Is the model a Chinese room or does it understand the game. If it's just a Chinese room, how come it is so effective, if it understands the game how can it be possible with just a rule machine?

14.
CamperBob2 3 months ago | on “Chain of Thought Empowers Transformers to Solve Inherently Serial Problems”
It's a Chinese Room - or a stochastic parrot.

Show me a resident of a Chinese Room who can do this: https://chatgpt.com/share/66e83ff0-76b4-800b-b33b-910d267a75...

The Chinese Room metaphor was always beneath Searle's intellectual level of play, and it hasn't exactly gotten more insightful with age.

15.
dTal 4 months ago | on “Anthropic publishes the 'system prompts' that make Claude tick”
I think you have misunderstood Searle's Chinese Room argument. In Searle's formulation, the Room speaks Chinese perfectly, passes the Turing test, and can in no way be distinguished from a human who speaks Chinese - you cannot "pop the illusion". The only thing separating it from a literal "robot that speaks Chinese" is the insertion of an (irrelevant) human in the room, who does not speak Chinese and whose brain is not part of the symbol manipulation mechanisms. "Internal cause and effect" has nothing to do with it - rather, the argument speciously connects understanding on the part of the human with understanding on the part of the room (robot).

The Chinese Room thought experiment is not a distinct "scenario", simply an intuition pump of a common form among philosophical arguments which is "what if we made a functional analogue of a human brain that functions in a bizarre way, therefore <insert random assertion about consciousness>".

16.
roddylindsay over 1 year ago | on “We need a more sophisticated debate about AI”
I think Searle would agree with you that the room does in fact speak Chinese. His point is that the neither the person, the machine, or "the room" understands Chinese, at least in the usual sense of that word when it comes to understanding as implemented by humans.

17.
sovande almost 2 years ago | on “CheatGPT”
Isn’t this the whole point of John Searl’s “the Chinese room” thought experiment? But does it matter what is actually going on inside the room, if the effect and function is indistinguishable? Edit: after conferring with ChatGPT, Searle’s point like yours is that the man in the room doesn’t understand Chinese, he is just manipulating symbols, but from the outside, the man in the room seems to speak fluent Chinese.

18.
netsharc almost 2 years ago | on “Ask HN: How have you applied AI in your life, work or hobbies?”
Ha, ChatGPT helping you create a literal implementation of the Chinese room, extending it to different languages.

Edit: come to think of it, actually not (because the operator in the Chinese room has no understanding), but is helping you ChatGPT understand the questions (I presume?).

19.
Terr_ 4 months ago | on “Are emergent abilities in large language models just in-context learning?”
I personally think that it's an eye-of-the-beholder problem. The algorithms of these Chinese Rooms [0] are emitting phrases and followups that we instinctively assume are proof of reasoning because we find it hard to imagine other ways the same result could occur, since we can't imagine brute-forcing it.

There's an awful lot of exuberance/investment riding on the idea that a Text Predictor engine can be trained well enough to (usefully) impersonate a General AI, and that if you train it even more then it can usefully impersonate a General AI usefully impersonating a Specialized AI. ("Here are my tax details, how much do I owe?")

[0] https://en.wikipedia.org/wiki/Chinese_room

20.
twblalock almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
John Searle's Chinese Room argument seems to be a perfect explanation for what is going on here, and should increase in status as a result of the behavior of the GPTs so far.

https://en.wikipedia.org/wiki/Chinese_room#:~:text=The%20Chi....

21.
Animats almost 2 years ago | on “What is ChatGPT doing and why does it work?”
No, ChatGPT is not a "Chinese Room". It's not big enough.

The classic "Chinese Room" is a pure lookup, like a search engine. All the raw data is kept. But the network in these large language models is considerably smaller than the training set. They extract generalizations from the data during the training phase, and use them during generation. Exactly how that happens or what it means is still puzzling.

22.
mort96 almost 2 years ago | on “The AI Mirror Test, which smart people keep failing”
My thoughts on the Chinese room thought experiment is: the person in the room does not know Chinese, but the person+room system knows Chinese. I believe the correct analogy is to compare the AI system to the person+room system, not to just the person.

How do you back up the statement that "for us to consider a machine sentient, it needs to function in a similar way to us"? On what basis do you categorically deny the validity of a sentient being which works differently than a human?

23.
mcguire almost 2 years ago | on “Can large language models reason about medical questions?”
I asked ChatGPT about the Chinese Room and asked for references. The first it came up with was "Minds, Brains, and Programs" by John Searle (1980).

The next two were, "The Chinese Room: Searle and Artificial Intelligence" edited by John Preston and Mark Bishop (2002) (Preston and Bishop did publish Views into the Chinese Room in 2002, but I can't find anything by the given title) and "The Chinese Room Argument" by David Chalmers (2006) (Chalmers has written on the topic, and published three articles in 2006, that's not one of them.

Also:

"Two cities: one, a city of “knights”, and the other of “knaves”. The residents of the two cities like to visit each other. One day, a man enters one of the cities, but doesn’t know which. He decides to ask the first person he meets where he is. The answer he receives is that he is in the “knaves” city."

produced the result:

"The man cannot be sure that the person he met was telling the truth, because the person could be a knight from the city of knights, who always lie, or a knave from the city of knaves, who always tell the truth.

"To determine which city he is in, the man could ask the second person he meets the same question. If the first person was a knight, then the second person would also be a knight, and would lie about being in the city of knights. If the first person was a knave, then the second person would also be a knave, and would tell the truth about being in the city of knaves.

"By asking the same question to multiple people and analyzing their responses, the man can eventually determine which city he is in. This problem, known as the "Knights and Knaves" puzzle, is a classic example of logical reasoning and problem-solving."

It seems to recognize the "knights and knaves" form of puzzle, but doesn't get the knights and knaves right. And it's not getting the reasoning part at all even given varying prompts.

24.
gameman144 almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
The Chinese Room thought experiment seems like a weird example, since the same could be said of humans.

When responding to English, your auditory system passes input that it doesn't understand to a bunch of neurons, each of which is processing signals they don't individually understand. You as a whole system, though, can be said to understand English.

Likewise, you as an individual might not be said to understand Chinese, though the you-plus-machine system could be said to understand Chinese in the same way as the different components of your brain are said to understand English.)

Moreover, even if LLMs don't understand language for some definition of "understand", it doesn't really matter if they are able to act with agency during the course of their simulated understanding; the consequences here, for any sufficiently convincing simulation, are the same.

25.
mannykannot about 2 months ago | on “Language is not essential for the cognitive processes that underlie thought”
The brain is faster than the Chinese room, but other than that, yes, that's the so-called systems reply; Searle's response to it (have the person in the room memorize the instruction book) is beside the point, as you can teach people to perform all sorts of algorithms without them needing to understand the result.

As many people have pointed out, Searle's argument begs the question by tacitly assuming that if anything about the room understands Chinese, it can only be the person within it.

26.
rolisz over 1 year ago | on “GPT-4 Is a Reasoning Engine”
Sure, but Searle's interpretation of the Chinese Room Experiment was never very convincing to me. I think the room + the person inside it, as a system, do speak Chinese.

27.
wruza over 1 year ago | on “Ask HN: AI read books, Human also. What's the difference?”
The standard argument against the chinese room is that if the 'symbols' are at lower level, audio, video stimuli, and you have enough scale, then you can imagine intelligence, emerging somehow.

Qualian chauvinism. The ‘observer’ field doesn’t have to be locally or real-ly connected. E.g. a current is real, but it doesn’t flow through a real-connected surface.

Chinese room is conscious and intelligent by definition if the above is true, just not human-like conscious and intelligent.

The juice of this question is that we may want to determine it in a practical binary sense, when there may be a spectrum or an entire space of it.

Sadly it can all boil down to a boring “shut up and interact” principle again.

28.
j16sdiz 9 months ago | on “Is the Sun Conscious? (2021) [pdf]”
Yes. And that is one of the reasons why Chinese room thought experiment is interesting.

We know how a Chinese-understanding human would respond -- they respond exactly like the room does -- but we don't know if he actually understanding anything.

29.
jll29 over 1 year ago | on “Artificial Consciousness Remains Impossible (Part 2)”
Searle's Chinese Room thought experiment is about mastering the Chinese language.

There are two issues I have with it as it was originally presented:

1. Mastering a language is not the same as having consciousness.

2. Who "knows Chinese" in the Chinese Room thought experiment? I would say neither Searle nor Searle in/as part of the Chinese Room "speak" Chinese. But it is

fair to say that the book that the fictional Searle follows can be

seen as a model of the Chinese language; or at least the combination of

the book and Searle as its "processor" collectively are an implemented

operational model of the Chinese language. And a model of Chinese is

NOT the same as being skilled at conversing in Chinese (executing the

model in a particular way). Other posters here have drawn analogies from

music evoking certain subjective emotions, and again a semantic network

that has concept nodes labelled with the names of these emotions is not

the same as experiencing these emotions, although the semantic network

can be said to constitute a model of sorts of the music's effects. But

again, model(x) != qualia(x).

30.
nullc about 1 year ago | on “Three things that LLMs have made us rethink”
The logical error is that it's confusing a part with the whole.

Clearly a single neuron in your brain does not understand English. Yet no one disputes that you understand English.

The non-chinese-speaker is just a component of the ChineseRoom system, that it doesn't understand Chinese is no more remarkable than a single neuron of yours not understanding English. It doesn't say anything about the overall system understanding Chinese or not.

I think it appeals to people with a more superficial understanding of computing that under appreciates the fact that instructions on paper processed by a universal circuit (or by a human emulating one) are morally equivalent to hardwired logic (if slower). So they disregard the contribution of the room, and focus on the fact that the meat-bot inside doesn't understand Chinese so the room must not either.

31.
mewpmewp2 10 months ago | on “Sora: Creating video from text”
Question is - how much do you need to understand something in order to mimick it?

The Chinese Room seems to however point to some sort of prewritten if-else type of algorithm type of situation. E.g. someone following scripted algorithmic procedures might not understand the content, but obviously this simplification is not the case with LLMs or this video generation, as the algorithmic scripting requires pre-written scripts.

Chinese Room seems to more refer to cases like "if someone tells me "xyz", then respond with "abc" - of course then you don't understand what xyz or abc mean, but it's not referring to neural networks training on ton of material to build this model representation of things.

32.
cercatrova over 2 years ago | on “A demo of GPT-3's ability to understand long instructions”
This is a philosophical question, really. Is there ever true understanding, or just pattern matching? The Chinese Room thought experiment talks about this:

> Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that they are talking to another Chinese-speaking human being.

> The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI."

> Searle then supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient papers, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output, without understanding any of the content of the Chinese writing. If the computer had passed the Turing test this way, it follows, says Searle, that he would do so as well, simply by running the program manually.

> Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that is then interpreted by the user as demonstrating intelligent conversation. However, Searle himself would not be able to understand the conversation. ("I don't speak a word of Chinese," he points out.) Therefore, he argues, it follows that the computer would not be able to understand the conversation either.

> Searle argues that, without "understanding" (or "intentionality"), we cannot describe what the machine is doing as "thinking" and, since it does not think, it does not have a "mind" in anything like the normal sense of the word. Therefore, he concludes that the "strong AI" hypothesis is false.

https://en.wikipedia.org/wiki/Chinese_room

33.
simonh over 2 years ago | on “The Chinese Room Thinks”
This is a decent refutation of Searle, mainly because Searle's characterisation of the Chinese Room is that it just encodes and processes syntax. Engaging in a conversation requires much more than syntax, it requires analysing and decomposing the meaning of what is being said, which for example could require constructing a model of a situation and reasoning about activities and changes in a situation. It might require critiquing suggested strategy for playing Chess for example, or commenting on economics. Actual conversation takes a lot more than just syntax, so by characterising it in this simplistic way he's stacking the deck in the reader's mind against the Chinese room.

This result is incredibly impressive. There's a lot of analysis going on here, far beyond mere processing of syntax. In this sense it exposes the inadequacies of Searle's critique of AI. It won't kill the Chinese Room argument though, it's supporters will simply extend it beyond just syntax. They will still claim there is no thinking mind producing these results and they're probably right, after all as I understand it this system is essentially a hierarchy of nets that filer input to generate output, there's no persistent running cognitive process that does anything like thinking on an ongoing basis.

The Chinese Room argument is still bunk though for many other reasons.

34.
david927 almost 2 years ago | on “A Russian Story Antedating John Searle's Chinese Room”
Searle's Chinese Room is becoming very relevant. Personally, it's how I've been describing to non-technical friends the difference between ChatGPT and AGI.

It's fascinating how early this author got to these ideas!

35.
roddylindsay over 1 year ago | on “We need a more sophisticated debate about AI”
This is hardly an indefensible position. John Searle, for one, has argued this point for more than 40 years:

"Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese."

Later he wrote:

"I demonstrated years ago with the so-called Chinese Room Argument that the implementation of the computer program is not by itself sufficient for consciousness or intentionality (Searle 1980). Computation is defined purely formally or syntactically, whereas minds have actual mental or semantic contents, and we cannot get from syntactical to the semantic just by having the syntactical operations and nothing else. To put this point slightly more technically, the notion “same implemented program” defines an equivalence class that is specified independently of any specific physical realization. But such a specification necessarily leaves out the biologically specific powers of the brain to cause cognitive processes. A system, me, for example, would not acquire an understanding of Chinese just by going through the steps of a computer program that simulated the behavior of a Chinese speaker."

36.
notahacker almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
"if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis"

The italics summarise it pretty neatly. It's an argument explicitly framed against Turing's more dubious thought experiment. If even a conscious being in the room can follow instructions, retrieve data and perform operations on it related to symbol manipulation flawlessly without having any sort of "understanding" of anything the symbols actually correspond to, there's no reason to deduce that the running part of a silicon-based machine must from the quality of the symbol outputs it can emit when plugged into a big enough library. Critics' insistence that this makes the "error" of neglecting the possibility that ongoing "understanding" (as opposed to inert symbolic representation of an absent writer's understanding) takes place in the books are actually irrelevant to this point, as well as more than a bit weird. Living outside a Chinese room, I also improve my communication skill and interpret others' understanding by interacting with books, but I wouldn't consider the books themselves a constituent part of my thought processes.

As you point out yourself, GPT has structural properties which make it very easy to classify as an entirely different thing from a human mind despite the similarity of outputs it is capable of producing, and the hypothetical room is even more dissimilar. The possibility it can produce output tokens which correspond to abstractions which humans interpret as consistent with human thought is not evidence that "thought" resides in patterns of abstract representation, not the physics of the organism. We know language is lossy.

37.
nullc almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
I've never spoken directly with Searle, but I have with several of his students and all have been emphatic that the Chinese Room Argument is demonstrates that a computer program can't have intelligence.

Quoting Searle himself,

"The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have."

I think the most succinct description of his error is substituting the (lack of) understanding of a part of the system (the man) for the understanding of the entire system (the rules and file cabinets, etc.). But I'm interested in learning that I'm mistaken.

You could turn his position around and say it's not the computer itself that's intelligent when a Chinese room system exhibits intelligence but the program -- and I suppose I'd agree with that, but it's also just semantics, uninteresting, and I don't believe he's ever taken that position.

I do agree that "merely replicating a behavior" doesn't prove much, but I don't think the Chinese room speaks to that substantially. It might if it demanded that the room implement only a very simple input to output map, but it doesn't: it allows the room to implement anything a computer program can implement. (A fact I use in my post to point out that the room could (in our land of hypotheticals) implement the molecular dynamics of an entire human being)

GPT has structural properties that make it very easy to classify it as an entirely different thing than a human mind. GPT is frozen in time, it cannot have an internal existence due to how its structured. It doesn't even have memory. It cannot learn (unless you include the whole company training it as part of 'it') except in the sense that it can immediately adapt to the output right in front of it, but can't preserve the knowledge. Theoretically if you made it arbitrarily large you could say it was close enough to having memory by always evaluating its complete history, but because its size grows quadratically with its window that isn't practicaly (and might not be possible to train-- it's totally credible that beyond some size these models will lose performance we just haven't gotten there yet). Figuring out how to train these models make good use of 'memory' is an ongoing challenge, since efficient memory isn't differentiable just ordinarily training with memory as part of the process doesn't work. Except by 'thinking out loud' in its output GPT also has a fixed upper bound on the time it can spend thinking any thought which is seemingly unlike a human mind.

38.
notahacker almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
The responses to the Chinese Room experiment always seem to involve far more tortuous definition-shifting than the original thought experiment

The human in the room understands how to find a list of possible responses to the token 你好吗, and how select a response like 很好 from the list and display that as a response

But he human does not understand that 很好 represents an assertion that he is feeling good[1], even though the human has an acute sense of when he feels good or not. He may, in fact, not be feeling particularly good (because, for example he's stuck in a windowless room all day moving strange foreign symbols around!) and have answered completely differently had the question been asked in a language he understood. The books also have no concept of well-being because they're ink on paper. We're really torturing the concept of "understanding" to death to argue that the understanding of a Chinese person who is experiencing 很好 feelings or does not want to admit they actually feel 不好 is indistinguishable from the "understanding" of "the union" of a person who is not feeling 很好 and does not know what 很好 means and some books which do not feel anything contain references to the possibility of replying with 很好, or maybe for variation 好得很, or 不好 which leads to a whole different set of continuations. And the idea that understanding of how you're feeling - the sentiment conveyed to the interlocutor in Chinese - is synonymous with knowing which bookshelf to find continuations where 很好 has been invoked is far too ludicrous to need addressing.

The only other relevant entity is the Chinese speaker who designed the room, who would likely have a deep appreciation of feeling 很好, 好得很 and 不好 as well as the appropriate use of those words he designed into the system, but Searle's argument wasn't that programmers weren't sentient.

[1]and ironically, I also don't speak Chinese and have relatively little idea what senses 很好 means "good" in and how that overlaps with the English concept, beyond understanding that it's an appropriate response to a common greeting which maps to "how are you"

39.
ekidd almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
> they're a Chinese Room

Searle's "Chinese Room" thought experiment is designed to appeal to your intuition. But it appeals in an incredibly unrealistic way. If you fill a room full of people shuffling Han characters (why would anyone do this?) you cannot possibly have anything resembling intelligence.

According to random guesses found on the internet, ChatGPT requires at least eight A100 GPUs to generate text. If you believe nVidia's marketing numbers, this gives you about 2.5 petaflops.

That's 2.5 quadrillion operations per second, plus communications overhead. If you decide to implement that calculation, imagine 350,000 versions of planet Earth, each with 7 billion people performing one operation per second. And some kind of faster-than-light communication, I suppose.

It's absolutely obvious to me that nothing like "thought" could possibly occur in any realistically-imaginable room full of people shuffling symbols. But if you fill 350,000 planets with people shuffling symbols frantically... I'm no longer sure? I don't trust my intuition at all? My brain is made up of a lot of atoms, and they somehow produce thought, after all.

Now, ChatGPT is not conscious. We're still several major breakthoughs away from any kind of "real" AI, I think. All we have now is a language module, a large memory of knowledge about the world, and some very inconsistent reasoning abilities. Although I have a nasty suspicion that at least a few of the missing parts will be easy to invent once someone tries...

40.
stared almost 3 years ago | on “Can GPT-3 AI write comedy?”
I am not convinced by the Chinese room. If it walks like a duck and quacks as a duck - it is a duck (the same rule holds for Python and Dennett).

I mean, I don’t believe GPT-3 experiences the subjective sense of existence. Yet, most human cognition is also learning patterns and repetition. Most people use words they don’t know definition of. Most people use grammar rules intuitively. Most people repeat what they have heard without scrutiny.

41.
CamperBob2 3 months ago | on “Chain of Thought Empowers Transformers to Solve Inherently Serial Problems”
It's not incredulity, just pointing out the obvious. Searle placed very specific limitations on the operator of the Room. He rests his whole argument on the premise that the operator is illiterate in Chinese, or at least has no access to the semantics of the material stored in the Room. That's plainly not the case with ChatGPT, or it couldn't review its previous answers to find and fix its mistakes.

And you certainly would not get a different response, much less a better one, from the operator of a Chinese Room simply by adding "Think carefully step by step" to the request you hand him.

It's just a vacuous argument from square one, and it annoys me to an entirely-unreasonable extent every time someone brings it up. Add it to my "Stochastic Parrot" and "Infinite Monkeys" trigger phrases, I guess.

42.
shagie over 1 year ago | on “Richard Stallman's thoughts on ChatGPT, AI and their impact on humanity”
The argument of the Chinese room is the strong claim.

From https://en.wikipedia.org/wiki/Chinese_room#Complete_argument the conclusion of the complete argument is:

> (C1) Programs are neither constitutive of nor sufficient for minds.

> This should follow without controversy from the first three: Programs don't have semantics. Programs have only syntax, and syntax is insufficient for semantics. Every mind has semantics. Therefore no programs are minds.

---

I personally don't agree with it and believe that there is a flaw in:

> (A2) "Minds have mental contents (semantics)."

> Unlike the symbols used by a program, our thoughts have meaning: they represent things and we know what it is they represent.

While a person may know what they are thinking, examining the mind from the outside it isn't possible to know what the mind is thinking. I would contend that from the outside of a mind looking at the firings of neurons in a brain it is equally indecipherable to the connections of a neural net.

The only claim that "we know what it is they represent" is done from the privileged position of inside the mind.

I would argue that intelligence is more related to the Kolmogorov complexity exhibited by something.

( David Dowe: Minimum Message Length, Solomonoff-Kolmogorov complexity, intelligence, deep learning... https://youtu.be/jY_FuQbEtVM?t=886 )

That the model of GPT is much smaller than its input.

The Chinese room lookup table is enormously large.

If we attempt to relegate GPT as no better than a Chinese room, we can show that the Chinese room look up table is impossible with the amount of data that GPT has access to as part of its model.

If we say that its not a lookup table but instead an enormously complex interplay of inputs and variables, then the distinction between the room that GPT exists in and our own mind breaks down trying to distinguish which is which.

If we want to switch to consciousness, then possibly the argument can progress from there because GPT doesn't have any state once it is run (ChatGPT maintains state by feeding its output back into itself and then summarizing it when it runs out of space). However, in doing this we've separated consciousness and intelligence which means that the Chinese room shouldn't be applied as an intelligence test but rather a consciousness test.

Are GPT 3 and 4 conscious? I'll certainly agree that's a "no". Will some future GPT be conscious and if so, how do we test for it? For that matter, how do we test for consciousness for another entity that we're conversing with (and its not just Homer with a drinking bird tapping 'suggested replies' in Teams ( https://support.microsoft.com/en-gb/office/use-suggested-rep... ))?

43.
HDThoreaun over 1 year ago | on “Simply explained: How does GPT work?”
I don't think that's the correct take for the room. Say the human speaks english. If you asked them what the conversation was about, and they had the full resources of the room at their disposal could they tell you? No, because the room doesn't actually allow them to understand chinese, it's just a symbol lookup table. The lookup table doesn't mean the system understands chinese, just the relationship between symbols that can lead to a coherent output.

44.
karpierz almost 2 years ago | on “ChatGPT: The Slickest Con Artist of All Time”
I think you're misunderstanding Searle's Chinese Room. It has a response for every sequence of conversation, ever. It doesn't store every conversation that has happened; it stores every possible conversation that's possible, whether it'll ever happen or not.

It would be able to handle the following exchange:

Person: "Here's a cool question, ready?"

Room: "Ready."

Person: "What was the last message I sent to you?"

It can respond appropriately to the following sentence:

Person: "Hey, I'm gonna say something. Here is a sentence. Can you repeat the previous sentence back to me?"

Otherwise, why bother with all of this AI stuff? Just build Searle's Chinese Room as an index and you have a perfect chatbot.

45.
drdeca almost 2 years ago | on “What is ChatGPT doing and why does it work?”
I don’t think the “Chinese Room” is supposed to necessarily be pure lookup. The point is that the person is the only one doing stuff, and they don’t understand Chinese, and so there’s nothing understanding Chinese. This doesn’t at all use the instructions in the room being just a static lookup table.

46.
epups almost 2 years ago | on “The new Bing runs on OpenAI’s GPT-4”
>Quoting Searle himself, "The point of the argument is this: if the man in the room does not understand Chinese on the basis of implementing the appropriate program for understanding Chinese then neither does any other digital computer solely on that basis because no computer, qua computer, has anything the man does not have."

> I think the most succinct description of his error is substituting the (lack of) understanding of a part of the system (the man) for the understanding of the entire system (the rules and file cabinets, etc.). But I'm interested in learning that I'm mistaken.

As you know, there have been many replies to this thought experiment, and some of the most interesting ones (to me) go in the direction you went here, ie, where is "understanding" occuring? The most basic version of the Chinese Room does intend to make you see yourself literally as a man who does not understand any Chinese and is just asked to look up symbols in a list. Perhaps that man doesn't understand Chinese, but the room as a whole at least gives the impression that it does.

However, I think the most important aspect is not this "intuition pump" as Daniel Dennett calls it. To me, what is key here is that we can all agree that such a Chinese Room, or ChatGPT for that matter, does not necessarily replicate the fundamental mechanisms of human cognition. Then, it follows that other human properties such as awareness or qualia do not necessarily emerge from such cognitive architectures in the same way that it emerges from our brains.

To me, Searle's point is ultimately that we don't know enough about the human mind to be able to judge whether it can be replicated artificially. And now that we have almost literally developed a Chinese Room, we can see that clearly. The arguments you bring up in your last paragraph are a great example of that, it's just very hard to conceive that this thing is conscious at all, even though it is capable of producing output that could convince people of that.

Regarding Searle's quote that you brought up, I think "solely on that basis" is doing a lot of heavy lifting there, but it does align with what I said previously. He is saying that simply producing intelligent output, like in 1974 translation would represent, does not mean you are reasoning in a human way.

47.
virchau13 over 2 years ago | on “Consciousness is not computation”
I like Scott Aaronson's response[1] to the argument:

> So, class, how might a strong AI proponent respond to this argument? Duh: you might not understand Chinese, but the rule book does! Or if you like, understanding Chinese is an emergent property of the system consisting of you and the rule book, in the same sense that understanding English is an emergent property of the neurons in your brain. Like many other thought experiments, the Chinese Room gets its mileage from a deceptive choice of imagery -- and more to the point, from ignoring computational complexity. We're invited to imagine someone pushing around slips of paper with zero understanding or insight -- much like the doofus freshmen who write (a+b)^2=a^2+b^2 on their math tests. But how many slips of paper are we talking about? How big would the rule book have to be, and how quickly would you have to consult it, to carry out an intelligent Chinese conversation in anything resembling real time? If each page of the rule book corresponded to one neuron of (say) Debbie's brain, then probably we'd be talking about a "rule book" at least the size of the Earth, its pages searchable by a swarm of robots traveling at close to the speed of light. When you put it that way, maybe it's not so hard to imagine that this enormous Chinese-speaking entity -- this dian nao -- that we've brought into being might have something we'd be prepared to call understanding or insight.

[1]: https://www.scottaaronson.com/democritus/lec4.html

48.
masswerk over 2 years ago | on “A demo of GPT-3's ability to understand long instructions”
Well, this, the Chinese Room, is still pretty much a behavioristic work-around (as it is still attempting to argue without any reference to internal states.)

Even, if we don't (clearly) understand what "understanding" means, or, at least, aren't able to provide a sane definition, we do know about the semantics of the term and the kind of connotations that come with it. Like a reflexive component. (Which wasn't much of a problem in the age of behaviorism, as this had to be ignored by requirement anyway. If there is no acknowledged difference between a human and a pigeon, what is the problem with computers, as far as the model is concerned?) So we do have a notion of the semantic field and its implications. And these are, well, quite disastrous for this purpose.

49.
gowld almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
The Chinese Room is famous because it was the first popular example of a philosopher not understanding what a computer is.

50.
bitwize 4 months ago | on “Apple Intelligence beta flagged a phishing email as "Priority"”
I first noticed this with Watson (IBM's language processing system) when it played Jeopardy!: when it was right, it was spot on and usually faster than the human contestants; but when it was wrong, it was way, way off base.

Part of that has to do with the fact that language is not the same for an LLM as it is for a person. If I say to you the sentence "The cat sat on the mat", that will evoke a picture, at the very least an abstract sketch, in your mind based on prior experience of cats, mats, and the sitting thereupon. Even aphantasic people will be able to map utterances to aspects of their experience in ways that allow them to judge whether something makes sense. A phrase like "colorless green dreams sleep furiously" is arrant nonsense to just about everybody.

But LLMs have no experiences. Utterances are tokens with statistical information about how they relate to one another. Nodes in a graph with weighted edges or something. If you say to an LLM "Explain to me how colorless green dreams can sleep furiously", it might respond with "Certainly! Dreams come in a variety of colors, including green and colorless..."

I've always found Searle's argument in the Chinese Room thought experiment fascinating, if wrong; my traditional response to it was "the man in the room does not understand Chinese, but the algorithm he's running might". I've been revisiting this thought experiment recently, and think Searle may have been less wrong than I'd first guessed. At a minimum, we can say that we do not yet have an algorithm that can understand Chinese (or English) the way we understand Chinese (or English).

51.
joe_the_user over 2 years ago | on “Is DALL-E 2 ‘gluing things together’ without understanding their relationsh...”
The "Chinese room" argument, so far as I understand it, applies to any discreet computation process simulating consciousness.

The argument of the article is that DALL-E doesn't respond appropriately to a particular kind of input - two entities in some kind of spatial relationship (that it hasn't often seen). Dall-E's not extrapolating the three-D world but stretching a bunch 2-D images together with some heuristics. That works to create a lot of plausible images sure but it implies to this ability might not, say, be able to be useful for the manipulation of 3-D space.

So, given a "Chinese room" is just a computation, it's plausible that some Chinese room could handle 3-d image manipulation more effectively than this particular program.

Which is to say, "no, the criticism isn't this is a Chinese room, that is irrelevant".

52.
jmugan about 2 years ago | on “CICERO: An AI agent that negotiates, persuades, and cooperates with people”
It's the Chinese room but it is effective because the game space is relatively small (compared to the real world) and you can cover that space with RL guiding NLP language generation.

53.
WillDaSilva almost 2 years ago | on “What is ChatGPT doing and why does it work?”
> [...] it's still a Chinese Room.

So are you, and so am I.

That's not to say that ChatGPT is sentient or has a significant amount of personhood, but we shouldn't wholly dismiss its significance in this regard, particularly not using that faulty argument.

54.
gwd over 1 year ago | on “A GPT-4 capability forecasting challenge”
> Searle's main point is that if I have a book that tells me how to respond and I never learn Chinese, then I do not understand Chinese. If you see a flaw in this reasoning, I am very interested.

To a degree, I feel like the Chinese Room argument is begging the question. When I imagine Searle sitting in a room, with a book of instructions and paper and everything he needs to execute GPT-4's equivalent, I basically see an actual computer. That is literally what he is; there is no difference. So then to ask, "Does this system understand Chinese?" is literally exactly the same question as "Does GPT-4 understand Chinese?" You haven't actually illuminated the question in any meaningful way, except to give people not familiar with how microprocessors work a better intuitive understanding. (Which, upon reflection, probably is a fairly useful thing to do.)

I looked a bit at the "1990's version" of his argument on the Wikipedia page you quoted. Going back to my earlier example, this is sort of what his argument sounds like to me:

A1) Electronic gates just on and off switches.

A2) Numbers and addition are semantic.

A3) On and off switches are neither constitutive of, nor sufficient for, semantics.

Therefore, computers cannot add; they only simulate the ability to add.

Now I'm not up on the fine details of what "syntactic vs semantic" means in philosophy, so maybe #2 is't accurate. But in a sense it doesn't matter, because that communicates how I feel about Searle's argument: "I've made some distinction between two classes of things that you don't understand; I've defined one to be on one side, and the other to be on the other side; and therefore computers can't understand."

My best guess as to the "syntactic / semantic" thing is this: In some sense, even his premise, that "Progams are syntactic", isn't actually accurate: Computers operate on bits which are operated on by gates: gates and bits themselves don't inherently have symbols; the symbols are an abstraction on the bits. Even bits are abstractions on continuous voltages; and voltages are ultimately abstractions on quantum probabilities.

What a given set of voltages "means" -- whether they're numbers to be added, or words to be word-checked, or instructions to be executed, or a JPEG to be decompressed, depends entirely on how they're used. If you jump into the middle of a JPEG, your computer will happily try to execute it, and if you dump the program into your video buffer, you'll get a bunch of strange dots on your screen.

Furthermore, when you build an "adder" out of logic gates, you can build the gates such that they correspond to our intuitive idea of binary addition, with individual carries for each bit and so on. But this is inefficient, because then you have to wait for the carries to cascade all the way through the whole thing you're trying to add. Instead, you can brute-force a set of logic gates such that given these 16 bits in, and these 9 bits out (8 plus overflow), you just get the right answer; this will be a lot faster (in the sense that the signals have to go through fewer gates before stabilizing on the final answer), but the gates inside then don't make any sense -- they're almost a "compression" of the longer, carry-based method.

Does that mean that an adder made this way isn't "actually" adding? In the end it doesn't really matter: 16 bits come in, and 9 bits come out the way you want them to. It doesn't really matter that much what happened in the middle.

Putting all that together: It seems to me the "semantics" of a set of bits is based on how they end up interacting with the real world. If I can ask GPT-4 what's missing in my pancake recipe, and it can tell me "you're missing a leavening agent like baking powder", then it seems to me there must be semantic content in there somewhere, and all the arguments about syntax not being sufficient for semantic turn out to have been proven wrong by experiment.

55.
throwawaygal7 over 2 years ago | on “The singularity is close?”
I think the Chinese room experiment is pretty effective, but a person might say the room is conscious and one might not. We all agree that, if we had a Chinese room in front of us, the only real way to tell would be a Turing style test.

56.
int_19h about 2 years ago | on “This Week's Hype”
The reason why Chinese room is a fallacy is because it's begging the question. It creates an elaborate metaphor for a computer only to exclaim that of course it's not intelligent, because that's obvious. If you don't start with a premise that "of course it's not intelligent", it's not at all obvious even for the analogy.

57.
barrkel over 1 year ago | on “Richard Stallman's thoughts on ChatGPT, AI and their impact on humanity”
The Chinese Room thought experiment is not convincing to software engineers generally. It relies heavily on an intuition that looking things up in a book is clearly not "thinking". Software engineers know better: that "looking things up", if you can do it billions and trillions of times a second, can simulate a process which has a close correspondence to reasoning.

58.
danparsonson 3 months ago | on “Chain of Thought Empowers Transformers to Solve Inherently Serial Problems”
> ... He rests his whole argument on the premise that the operator is illiterate in Chinese, or at least has no access to the semantics of the material stored in the Room.

...and yet outputs semantically correct responses.

> That's plainly not the case with ChatGPT, or it couldn't review its previous answers to find and fix its mistakes.

Which is another way of saying, ChatGPT couldn't produce semantically correct output without understanding the input. Disagreeing with which is the whole point of the Chinese Room argument.

Why cannot the semantic understanding be implicitly encoded in the model? That is, why cannot the program I (as the Chinese Room automaton) am following be of sufficient complexity that my output appears to be that of an intelligent being with semantic understanding and the ability to review my answers? That, in my understanding, is where the genius of ChatGPT lies - it's a masterpiece of preprocessing and information encoding. I don't think it needs to be anything else to achieve the results it achieves.

A different example of this is the work of Yusuke Endoh, whom you may know for his famous quines. https://esoteric.codes/blog/the-128-language-quine-relay is to me one of the most astonishing feats of software engineering I've ever seen, and little short of magic - but at its heart it's 'just' very clever encoding. Each subsequent program understands nothing and yet encodes every subsequent program including itself. Another example is DNA; how on Earth does a dumb molecule create a body plan? I'm sure there are lots of examples of systems that exhibit such apparently intelligent and subtly discriminative behaviour entirely automatically. Ant colonies!

> And you certainly would not get a different response, much less a better one, from the operator of a Chinese Room simply by adding "Think carefully step by step" to the request you hand him.

Again, why not? It has access to everything that has gone before; the next token is f(all the previous ones). As for asking it to "think carefully", would you feel differently if the magic phrase was "octopus lemon wheat door handle"? Because it doesn't matter what the words mean to a human - it's just responding to the symbols it's been fed; the fact that you type something meaningful to you just obscures that fact and lends subconscious credence to the idea that it understands you.

> It's just a vacuous argument from square one,

and it annoys me to an entirely-unreasonable extent every time someone brings it up. Add it to my "Stochastic Parrot" and "Infinite Monkeys" trigger phrases, I guess.

With no intent to annoy, I hope you at least understand where I'm coming from, and why I think those labels are not just apt, but useful ways to dispel the magical thinking that some (not you specifically) exhibit when discussing these things. We're engineers and scientists and although it's fine to dream, I think it's also fine to continue trying to shoot down the balloons that we send up, so we're not blinded by the miracle of flight.

59.
emtel almost 3 years ago | on “Neural nets are not “slightly conscious,” and AI PR can do with less hype”
The Chinese Room is a really, really bad thought experiment. It's just an attempt at misdirection. It asks you to imagine a computer (one that operates on slips of paper according to rules that are written down in a big book), puts a human inside that computer, and then says that because the human doesn't understand what the computer program is doing, the computer program isn't conscious.

But this is just question-begging. The experiment purports to resolve the question "can a computer be conscious", and answers it by showing you a computer and claiming that it is not conscious.

Worse, the Chinese Room is straight-up Cartesian dualism. By placing the human inside the computer, Searle is preying on your Cartesian intuition that there must be a theatre of the mind where reality is experienced. And since Searle's homunculus doesn't understand Chinese, nothing can be experienced, therefore no conscious experience is happening in this system.

If I'm wrong, and the Chinese Room thought experiment is conclusive, it must be possible to reformulate it without the human. After all, the human does nothing but follow rules in a purely mechanistic fashion. Can anyone offer a reformulation of the experiment, that has no human, that they still feel is convincing?

60.
bigtex88 almost 2 years ago | on “Bing: “I will not harm you unless you harm me first””
How are humans any different? Searle did an awful job of explaining why the AI in the room is any different than a human mind. I don't "understand" what any English words mean, but I can use them in the language-game that I play. How is that any different than how the AI operates?

The "Chinese room problem" has been thoroughly debunked and as far as I can tell no serious cognitive scientists take it seriously these days.

61.
abudabi123 about 2 years ago | on “New research suggests our brains use quantum computation”
Searle's Chinese Room?

Say, you put three individuals in the room. Say, Steve Wozniak, Steve Jobs, Steve Balmer. Say, the experiment is plus or minus Jack Ma. The room follows the script. I guess 30 minute runs of the experiment generates a kind of collective consciousness in the conversation content between participants that is completely off script.

62.
throw_pm23 almost 2 years ago | on “The False Promise of ChatGPT”
How do you know I understand X and Y and not just apply some mechanistic rules for producing this text? Even in the Chinese Room, to make it reasonably efficient, you'd need some shortcuts, some organization, some algorithm to do it. How is that different from some kind of understanding?

63.
Hugsun 7 months ago | on “Can large language models reason?”
I know about the Chinese Room. The pivotal difference is that the intelligence is performing a mechanical task and none of its intelligence is inserted into the inputs.

No synthesis is performed.

In the API example, it's literally people answering the queries with the answer they want. They're using their intelligence to synthesize the responses.

64.
nopassrecover over 1 year ago | on “GPT-4 performs better at Theory of Mind tests than actual humans”
It's interesting how quickly the Chinese Room has moved from thought experiment to reality https://en.wikipedia.org/wiki/Chinese_room

65.
smoldesu almost 2 years ago | on “Jailbreaking ChatGPT with Dan (Do Anything Now)”
The point of the Chinese Room Test is to distinguish between Turing's definition of intelligence and the human definition of intelligence. It poses two separate possibilities; is this AI understanding the language, or simulating the ability to understand the language?

You might find the answer to be arbitrary or meaningless, but it is an important distinction. I could pretend to be a random number generator well enough to fool anyone who's distinguishing me from a computer. That doesn't mean that I'm a reliable source of entropy, or equally as random as the other, similar result. Treated as a black-box they are the same; treated as a mechanical system, they could not be more different.

66.
ad404b8a372f2b9 almost 2 years ago | on “Should GPT Exist?”
(With regards to the quoted text, "it" referred to natural language, not ChatGPT. You were questioning the sufficiency of natural language, not whether ChatGPT could do those things.)

With regards to ChatGPT: Given a set of previously unseen axioms it can (sometimes) make a previously unseen and logically consistent prediction. This is what I have observed.

Whether this is true logical reasoning, truly new ideas, or just an arrangement of statistically plausible characters that belies its lack of understanding is a metaphysical question akin to the Chinese room.

67.
HeavenBanned over 2 years ago | on “Thinking as Computation (2011) [pdf]”
The Chinese room never made sense to me because, even though the person passing the arcane symbols through the door doesn't _understand_ what is being communicated doesn't mean that's not how it works in _our_ brain. Our individual neurons (or aggregate neurons depending on how they're working and firing based on symbol triggering) don't know what symbols they're processing either. So unless Searle is saying humans don't know any languages on some fundamental level, it's kind of useless because then we'd just end up admitting (something unsavory) that nobody knows anything barring some homunculus in our brains or even worse, we can never truly have free will. Those are scary implications.

68.
mhink over 1 year ago | on “Simply explained: How does GPT work?”
In my understanding of the Chinese Room example, the resolution to the argument is that the *human* may not understand Chinese, but the *system as a whole* can be said to understand it.

With this in mind, I think asking whether ChatGPT *in and of itself* is "conscious" or has "agency" is sort of like asking if the speech center of a particular human's brain is "conscious" or has "agency": it's not really a question that makes sense, because the speech center of a brain is just one part of a densely interconnected system that we only interpret as a "mind" when considered in its totality.



mtlmtlmtlmtl over 2 years ago | on “Google fires engineer who called its AI sentient”
If you reject physicalism, you must posit some non-physical "stuff" or mechanism to explain the "qualia" that you reject as being physical. That is inherently dualism. But dualism is a dirty word in philosophy these days, so thwy don't call themselves as much.

The hard problem of consciousness is an inherently dualist conception.

Goff for instance subscribes to the patently absurd view of panpsychism, where matter is posited to have subjective experience "built in" somehow. This is such an absurd view. He first posits that there must be some fundamental subjective experience. But then he can't actually come up with a cogent theory for it, so he then just posits that mind is fundamental to matter. So he's effectively just inventing new properties of nature out of whole cloth. But then even still he has no solid conception of what those properties are, what they do or how they interact to form a conscious mind in a brain. How is any of this helpful in any way? He took one dubious problem and projected it onto the entire universe, and gave himself another problem of emergence to boot. This is not progress, more like digging a hole for himself.

As for Searle, I'm not hugely familiar with his work, but I find his Chinese Room experiment, or rather his conclusions about it, misguided at best and wilfully ignorant at worst. The system reply, which I think is just obviously true, is simply dismissed with very little argument.

Again, I fail to find justification for fundamentally subjective experience other than it sure feels that way. That's more of a theological argument than a philosophical one.

The idea that Dennet is easier to read because he doesn't conform to the truth is pretty strange. He's clearly a very skilled writer and speaker. He's very good at avoiding a dense jungle of -isms, and when he does use them, he defines them very precicely and carefully. This to me is good philosophy. Dennet does a good job of laying out and explaining these ideas in a way that isn't completely convoluted. Argument is the core methodology of philosophy, and if a philosopher fails to represent their argument in clear way, why should I even take them seriously?

Philosophers are great at dressing up bad arguments in fancy, mystical, ill-defined terminology like "qualia". This to me is the philosophical equivalent of code smell. Whenever I read these closet dualists' arguments I have to pinch my nose.

2.
pas over 2 years ago | on “Under anesthesia, where do our minds go?”
I'm not familiar with philosophy this much, but it seems strange to assign structure, organization and other conceptual meta-things the same thing status as we assign to matter.

The mind is a description of the organization/structure/function of a chunk of matter. And the matter part completely determines the non-material part, hence materialism, no?

3.
hackandthink 9 months ago | on “The complete story of Gödel incompleteness”
"It's that, before Tarski and Gödel, no one clearly distinguished syntax and semantics"

I think this is right, but Hilbert came already close much earlier.

But "Before this, logic had been strictly syntactical and proof theoretic" is at

least misleading.

Traditional logic was more conceptional. Frege's "formalism" was a great innovation, but he remained within the conceptual tradition (Frege-Hilbert controversy).

"What Hilbert offers us, in 1899, is a systematic and powerful technique that can be used across all formalized disciplines to do just this: to prove consistency and independence. In doing so, he lays the groundwork, in concert with various of his contemporaries, for the emergence of contemporary model-theoretic techniques."

https://plato.stanford.edu/entries/frege-hilbert/

4.
somewhereoutth over 1 year ago | on “Language models still struggle with the concept of negation”
The distinction between semantics and syntax is pretty tight, no philosophy required. The former considers the domain being represented, whereas the latter is strictly the symbols used in the representation.

So to be precise it mechanically builds a deeply layered syntactic model. LLMs just regurgitate syntax, any semantics can only be imagined by us and overlaid on the syntactic results produced.

5.
agumonkey over 2 years ago | on “An Intuition for Lisp Syntax (2020)”
It's funny cause to me visual differences in syntax are just useless information overload. And another dimension of constraints to deal with. That's why I clinged to low syntax languages.. you write in semantics almost. The rest is problem solving (or even extending metadomain with macros or else to help yourself)

6.
techno_tsar over 1 year ago | on “American IQ scores have rapidly dropped, proving the 'Reverse Flynn effect'”
The analytic/continental split happened around the 19th century. Both traditions read more or less the same thinkers in Western philosophy (e.g. Plato, Aristotle, Aquinas, Descartes, the Enlightenment Rationalists, the Enlightenment Empiricists, the Enlightenment Idealists, Kant) up until then. Once you hit the 19th century, stick with the readings that deal with the problems set out by Frege, Russell, and Moore, who are trying to get as close to science and formalizing problems in a clear, mathematical way as possible. As a result, the philosophical agenda uses conceptual engineering as a general approach. Generally speaking, any topic that fits into a "Philosophy of X" (e.g. Philosophy of mind, language, ethics, action, etc) is part of the analytic tradition.

The continental thinkers (starting with Nietzsche, Husserl, Heidegger) are asking questions like "What is one's place in the world?" or "How do forces of history affect the deepest parts of people?" or "How does our Western scientific worldview limit other kinds of knowledge?" On the surface, this is going to seem a lot more 'subjective' and is partly why it gets obscure very, very fast. Personally, I think it's still worth reading, but that's because I find those issues interesting to begin with.

7.
Hermitian909 almost 3 years ago | on “The Society of the Spectacle”
While Philosophy is often split along analytic/continental lines, another way to split it is along the lines of whether the philosophers feel it is important to use precise language that tries to improve mental clarity and shared understanding and those who feel that there are certain ineffable qualities to ideas that are lost when you attempt to make that level of precision, both because of laziness in readers and because of inherent qualities to the ideas. Situationalists are very much in the latter camp and have said that at times they were deliberately obtuse so as to avoid being misunderstood.

8.
Archelaos over 2 years ago | on “The Logical Writings of Karl Popper”
In the mid 1990s I wrote my Master's thesis about the difference between Analytical and Continental philosophy. To summarise it: The difference is more sociological (who quotes whom), historical (which are the classics you refer to, such as Kant vs. Hume, etc.) and a matter of style (a tendency (not more) to use formal logical to make something explicit or do without it) than actual philosophical differences.

For every content criterion that people have brought up to mark the difference between Analytical and Continental philosophy, one could usually find a prominent philosopher in either camp who match or does not match it:

Analytical philosophy: Formal logic? -- The early Wittgenstein is fond of it, but the late Wittgenstein moves away from it.

Analytical philosophy: Problems of philosophy as language problems? -- But Nietzsche and his fans (such as Mauthner).

Analytical philosophy: Importance of natural sciences for philosophy? -- But Cassirer.

Continental philosophy: Focus on hermeneutics? -- But Rorty.

Continental philosophy: Organic thinking (Hegel, etc.)? -- But Quine's holism.

Continental philosophy: Existentialism -- But many on the continent were not convinced. But many in the USA were interested. But Wittgensteins's mysticism.

...

9.
bbor 10 months ago | on “Learning resources for curious software engineers”
Any thoughts for a Philosophy section? i.e. meta-CS. I'm thinking about stuff like this, though I guess it would better as blog posts or second-hand critiques:

- Chomsky 1956, _Three models for the description of language_ https://chomsky.info/wp-content/uploads/195609-.pdf

- Naur 1985, _Programming as theory building_ https://pages.cs.wisc.edu/~remzi/Naur.pdf

- Simon & Newell 1970, _Human Problem Solving_ https://iiif.library.cmu.edu/file/Newell_box00018_fld01306_d...

etc. Maybe too broad a topic? Basically this list feels like it's lacking answers to some fundamental "why is it like this" questions, if that makes sense.

10.
AJLKW 10 months ago | on “Dialectic with ChatGPT4: Transcendental Properties of Language > Algorithms”
Philosophy has been stymied by too many people treating the discipline as subjective reader-response criticism. This has alienated the more rigorous types who are interested in language. They have hijacked the conversation and made it about automatic, axiomatic, algorithmic, closed systems. This is stupid. Any undergrad who studies logic knows this cannot lead to the completeness that commerce expects from LLMs. Here is original, cybernetic philosophy that emerged from a dialogue with ChatGPT-4.

11.
adamnemecek over 2 years ago | on “How to read mathematics (2015)”
Constructivism. Linear logic, geometry of interaction, transcendental syntax.

12.
eynsham 7 months ago | on “Meringue Philosophy”
1. Some philosophers think many metaphysical debates meaningless. Consider the debate about ordinary objects such as chairs. These typically are the result of composing more fundamental entities (such as those of the Standard Model). Mereological nihilists say: there are no ordinary objects (such as chairs). Others disagree. /Relativists/ about metaphysics say that these disputes are merely verbal. There is no fact of the matter to discover and expound. Whether or not there is such a thing as a chair is simply a matter of convention.

2. I disagree. But I think one point is right. Relativists point out that it is often (or maybe even always) possible to translate statements between the languages promoted by different metaphysical schools. For example, nihilists translate ‘the chair is on the ground’ to ‘some particles arranged chairwise are such that the particles at the bottom touch the ground’ (or something similar). For this reason, they claim not much hinges on whether nihilists are actually right.

3. Despite my antirelativist metaphysical inclinations, I think it is possible to translate statements in the language given by the ‘same ontology that constitutes reality’ systematically into languages that reflect different metaphysical views.

4. Some of those erroneous metaphysical views may correspond to how we naturally tend to think about certain areas, or the needs of computation (e.g. in being better complexity-wise). For this reason I am sceptical about the (admittedly very interesting) technique advanced in this essay, since it seems to assume that no conflict between these demands will arise.

5. A final worry is that metaphysics does not really speak with one voice. Many philosophers are quite sympathetic to the sort of picture outlined at the start of the essay, but e.g. mereological nihilism is surprisingly popular! If resolving these questions definitively is too much even for philosophers, those applying their conclusions will have some trouble too.

13.
zer00eyz 8 months ago | on “Borgo is a statically typed language that compiles to Go”
And in the other corner you have Chomsky with universal grammar... and in another you have Platonic Forms...

I love the "draw me a tree" idea of a Platonic form, we all have an idealized model of what that is, that is uniquely our own. With that in mind isnt everything subject to some sort of semantics?

14.
hyperpape over 2 years ago | on “Analytic versus Continental Philosophy (2009)”
This article is not entirely bad, but I think it still overstates the distinction.

It is true that there's something of a divergence, where analytic philosophers are much more likely to read Russell, Wittgenstein, Quine and Lewis, while continental philosophers are more likely to read Heidegger or Derrida. There are accompanying differences in style and "feel". But it is extremely hard to generate robust generalizations about differences. For any high level philosophical position, you can find authors on both sides arguing for or against it.

Additionally, while early in the 20th century, you could try to argue that analytic philosophy had a core of logical analysis (in the sense of Russell/early Wittgenstein, Logical Positivism, Carnap) and ordinary language, starting in the 1960s, the field increasingly fragments, and becomes more varied in the topics it studies, the approaches it takes, and so on. By the present day, we have "analytic philosophers" like Kieran Setiya writing a book on Mid-Life Crises, or Laurie Paul writing about whether one can anticipate the experience of parenthood (https://lapaul.org/papers/whatCantExpect.pdf).

All in all, it's true that there are two relatively independent traditions, it's hard to say much more about them than "people tend to read different books, and that has some influence on how they approach matters."

15.
potkin 11 months ago | on “A Terribly Serious adventure: Philosophy and war at Oxford”
Wittgenstein Russell and Frege were really the founders of "analytic" philosophy as a field or set of methods. Since their time philosophy has become increasingly specialised and I don't think there are contemporary equivalents. So much philosophy of [X] but whether or not that work is seen as serious by practitioners of [X] is debatable.

Robert Brandom at Pittsburgh is a bit of a grand old man of philosophy and still publishing - works in in philosophy of language and philosophy of logic as well as Hegel scholarship, and his "inferentialist" approach to linguistic meaning has been at least cited by people working in proof-theoretic semantics and linguistics. Martha Nussbaum in political philosophy and ethics and David Chalmers in philosophy of mind/cognitive science also come to mind. But again I'm not sure that Chalmers is taken seriously outside of philosophers' circles.

16.
ogogmad over 2 years ago | on “The Logical Writings of Karl Popper”
(I like mathematical logic).

Continental philosophy eschews formal logic and focuses more on literature and the arts as the source of philosophical truth. Some insights from Emmanuel Kant are used to justify this ([1]). Can someone explain this stance better than [1] does?

While there has been a great deal of

sympathetic interest among analytic philosophers in the idea that philosophy should be

continuous in method and subject-matter with the natural sciences -- what is commonly

referred to as "positivism" -- Continental philosophy has generally dismissed such ideas as no

more than a reversion to a pre-Kantian conception of the philosophical enterprise.

[1] - https://scholar.harvard.edu/files/michaelrosen/files/contine...

17.
DiscourseFan almost 2 years ago | on “Does mathematics need a philosophy?”
I have a great respect for this author, but I do have to note that the debate of "constructivism" vs. "platonism" doesn't really exist in Philosophy per se, because it relies on a kind of basic misunderstanding of Plato in general. At least since Kant, the general consensus in Philosophy about Plato's "forms" or ideal objects is that they are conceptual objects, famously Kant argued that while reality in-itself is inscrutable, the means of judgement whereby we organize the world into concepts (beginning with the intuitions of time and space) are not, and by analysis of judgement we can begin to better understand the process of reasoning and therefore more rigorously understand the means by which empirical judgements are made (and thus further our understanding of empirical reality).

Of course, many have attempted to answer the question of the psychological/biological explanation for how humans have shared concepts like this and shared, intelligible language. The most prominent thinkers of the 20th century, on both "sides" if you will, were Lacan on the "continental" end, and Chomsky on the "analytic" side. (Even though most Anglo-American--i.e. Analytic--philosophers would not accept Chomsky's logic as valid, and Lacan often referenced Anglo-American philosophy in his work...the categories are basically meaningless at this point.) But there are many other names, many people asking the above question, and I will not say who's thought attracts me the most, but I will say that most people I meet who study math are not familiar with them or the intellectual trends they are a part of, and they do not have a rigorous enough understanding of what philosophy is, really, to make claims about the usefulness of philosophy for their work--or even debate it, for that matter.

18.
gumby about 1 year ago | on “A Philosophy of Software Design”
When I think of philosophy of software I think of the operative semantics of programming languages, model semantics, Quine, Church, Barwise & Perry, et al.

19.
bmitc almost 2 years ago | on “Does mathematics need a philosophy?”
Being stirred by your last sentence, I have a lot of trouble reading some philosophy. I have read your comment several times, and I am still not sure what it is saying.

> I do have to note that the debate of "constructivism" vs. "platonism" doesn't really exist in Philosophy per se, because it relies on a kind of basic misunderstanding of Plato in general.

What is the misunderstanding, exactly? Did you end up addressing this? The next sentence is a long run-on, and I'm getting lost in what you're saying Plato actually said versus what you think people think he said. And I think I agree with another commenter that you are confusing Platonic and platonic.

And what does philosophy say about constructivism? If it doesn't address it versus some notion of idealized objects existing, then why does that matter to mathematics? It is a legitimate line of thinking and isn't an exclusively philosophical subject.

> Of course, many have attempted to answer the question of the psychological/biological explanation for how humans have shared concepts like this and shared, intelligible language. The most prominent thinkers of the 20th century, on both "sides" if you will, were Lacan on the "continental" end, and Chomsky on the "analytic" side. (Even though most Anglo-American--i.e. Analytic--philosophers would not accept Chomsky's logic as valid, and Lacan often referenced Anglo-American philosophy in his work...the categories are basically meaningless at this point.)

There are just names and labels here. What are you saying here?

20.
6AA4FD over 2 years ago | on “The Logical Writings of Karl Popper”
I can give it a shot. I don't want to butcher it, but I feel like attention spans on here are pretty short for philosophy so here's the short version. Many continental philosophers appear to be skeptical of the separation of form and content necessary for logic to "work" in the context of another subject of study (Deleuze), interested in presenting tensions between ideas that do not clearly adapt themselves to exclusive truth or falsity (Derrida), or interested in presenting things that do not pretend to be particularly abstract or logical (Levinas).

On a personal note, I want to say that I drifted to continental philosophy in my undergrad after studying and appreciating formal logic. I realized that mainstream analytical philosophy had a lot less to do with logic than I had imagined (no symbolization, no commonly agreed upon rules of deduction), and at a certain point the question of "why logic" presented itself. I haven't found many opportunities in my writing to use the more technical concepts of modality or nth-orders, let alone anything from category theory.

e: Replaced induction with deduction, a typo

21.
mjhay 6 months ago | on “The Geometry of Categorical and Hierarchical Concepts in Large Language Mod...”
Yeah, it didn't used to be that big of a divide. Nowadays it seems like analytic philosophers are doing endless retreads, and continental ones are also doing endless retreads, but with more confusing sentence structure.

From a 10,000 foot view, I think nailing down a more "objective" understanding of dialectics (idealist, material, whatever) is a promising direction to ameliorate this meta-problem. People arguing in journals is pretty much a dialectic problem, so understanding that can go a long way to understanding issues beyond that.

22.
techno_tsar over 1 year ago | on “American IQ scores have rapidly dropped, proving the 'Reverse Flynn effect'”
I put relativism is scare quotes because Continental philosophy doesn't espouse relativism, but is often accused of it. It does espouse a cynicism for structuralism and has a penchant for scientific anti-realism, but both those views (when we re-express it in ordinary "analytic" language) are contentious while respectable, but not incredulous, and have sophisticated arguments.

But -- and as a reply to the complaints of its obscurantism -- it's also not obvious that arguments in Continental philosophy can be re-expressed in an atomic way without losing its fundamental essence. If we could, then and only then can we accuse thinkers of being fluffy and imprecise obscurantists. However, many of these philosophies are ones that criticize reductive, logocentric discourse to begin with! In other words, that is a feature, not a bug of the philosophical tradition.

23.
acoard over 2 years ago | on “Did the early medieval era ever take place?”
As a philosophy major who was never particularly interested in ethics, I certainly don't think it's trivial. If you trivial as in "clearly obvious" then I certainly disagree. But let me try and charitably interpret your position.

I do think that in practice the different ethical systems don't produce wildly different outcomes, from utilitarianism to deontology to even virtue ethics. Of course, in the realm of thought experiments they can be widely different. But in the real world, the hardest thing is to be moral (i.e. do the right thing) rather than defining your ethics (having a clear code). In practice, the "evil"/conflict/strive in the world I see is due to selfishness or amorality, not disagreements between utilitarians and kantians. Is this what you're getting at? That the distinctions between the different ethical systems collapse into similar practical situations? That being a "good person" is more important than the particular ethical code you follow? (That's my take, basically).

I'll also say that I found meta-ethics incredibly illuminating. It essentially builds up a language to discuss different ethical systems and makes conversations around them way easier.

Big up on consciousness and philosophy of mind though. I also found philosophy of science fascinating. And shoutout to Hobbes! He would have loved HN, he was such a contrarian.

24.
woopsn 8 months ago | on “Structuralism as a Philosophy of Mathematics”
This is really an important point for mathematical philosophy. It is a single enterprise going back to ancient Babylon, China, India, Greece, Egypt, and before. The elementary meta-theory needs to be syntonic to mathematical activity and knowledge predating (or otherwise practiced without) formalism, structuralism, categoricity, platonism, etc.

25.
sargstuff 8 months ago | on “What does Goldman Sachs want in a coder? For them to have studied philosoph...”
counter point:

Boolean logic/heart of computation philosophy isn't what's left over after higher abstraction levels have evolved from binary -- still need associated 'rules/terms' ways to impliment/make use of logic & rules/terms.

Just provides common reference/bridge between things that have abstracted rules/terms and implimentation/usage under different disciplines.

Would hazzard to guess idea of requiring classical concept of philosopy would be to have common reference point(s) between engineering & non-engineering disciplines.

26.
beepbooptheory over 1 year ago | on “Yann LeCun and Andrew Ng: Why the 6-Month AI Pause Is a Bad Idea [video]”
I've noticed people claiming that this is a real dichotomy at play. Just feels like people who stopped at philosophy 101!

Like, there is a good argument to be made that believing that consciousness, intelligence, etc are distinct, enumerable things "contained" within whatever system you choose in fact makes you much more of a Cartesian than any of the details around soul and body. Presenting even the dilemma this way shows you haven't left the enlightenment at all!

Just if you are interested in this stuff, I would check out what people have thought about since the 1600s. The more you feel like these things are already solved, the more it shows the poverty of your current conceptions around them.

27.
masswerk over 1 year ago | on “Simply explained: How does GPT work?”
Why not both? Things like philosophy or metapsychology tend to be prismatic, each framework comes with advantages and disadvantages and boundaries of its own.

(A turn towards the dogmatic is something I'm pretty much expecting from the current launch of AI anyway, simply, because the productions systematically favor the semantic center. So it may be worth putting some generality against this, rather than being overly selective.)

28.
prometheus76 13 days ago | on “'With brain preservation, nobody has to die'”
Categories, numbers, logic, grammar, etc. all map onto physical systems, but aren't necessarily directly linked. This is "metaphysics" in philosophy, and is essential in order to even reason about physical systems. Just the concept of a "physical system" is actually metaphysical, but without metaphysics, you can say nothing to anyone about anything. Metaphysics is just generally taken as a "given", but is worth evaluating itself.

Then you'll run into the "realism" vs. "nominalism" debate and you'll understand the philosophical underpinnings of the current culture wars in America.

29.
AIorNot about 2 months ago | on “The Thought Experiments That Fray the Fabric of Space-Time”
Besides the silly, but inevitable HN complaints about the format of the webpage presentation, (great presentation btw)

The fundamental challenges these experiments (and others) surface is a deep challenge to the traditional narratives of Materialism or 'Physicalism' as our understanding of what existence is. In essence science and human knowledge has lept forward technologigcally over the past 400 and esp the past 100 years because we started assuming the world was physical in nature, material and metaphysically, ie that it reduced to fundamentally physical things we could quantify and measure.

Yet, the older I get the more inclined I am to believe in some form of Idealism.. Not only in Idealism but I'm leaning towards the belief that some kind of fundamental universal Consciousness is the only fundamental property or baseline to the universe or to existence.

Time and Space is not fundamental. Locality isnt true.

30.
bandie91 almost 2 years ago | on “Light from an ionized state of helium in a distant galaxy”
will, intellect, truth values, causality, logical principals, about-ness, universales. are these, are everything concievable just emergent properties in your view? are not they real apart from material things?

31.
kijin 8 months ago | on “Daniel Dennett has died”
I think it depends a lot on which tradition of philosophy one is first exposed to. Most STEM people will find Anglo-American analytic philosophy (where Dennett firmly belonged) much easier to approach than continental philosophy or the classical stuff, but unfortunately casual readers tend to get exposed to a lot more of the latter.

It's like the first programming language you learned. It will shape your perception of what programming is all about for a long time afterward, and might even turn you away from programming altogether. But there are lots of programming languages, and they're just different ways to make the same silicon do something interesting!

32.
jameshart 9 months ago | on “Natural language instructions induce generalization in networks of neurons”
Strong Sapir-Whorf (linguistic determinism - language constrains thought) became pretty much seen as a joke by the 1980s. Linguistic relativism (weak Sapir-Whorf - language shapes thought) is still respectable (because, I mean, of course it does).

Actually, this research might just as well be evidence for linguistic universalism (Chomsky - language enables thought).

In general linguistic philosophers have been coming out with either laughably obvious or utterly untestable hypotheses for a century and it’s amusing to see how these AI studies shake up the hornets.

33.
mannykannot 7 months ago | on “This Message Does Not Exist”
On the one hand, these puzzles often arise as counter-examples to a theory within analytical philosophy, and unless you understand what they are a counter-example to, they seem pointless and even ridiculous. For example, when Russell posed his famous paradox to Frege, he was not trying to find out who shaves the barber.

On the other hand, perhaps we should consider whether analytical philosophy, following the linguistic turn [1], is creating problems for itself when it tries to find metaphysical truth by analyzing human language as if it were a formal system. For example, when David Chalmers says "even God could not create a male vixen", is he mistaking an accidental lexicographical fact for a metaphysical insight?

[1] https://en.wikipedia.org/wiki/Linguistic_turn

34.
ants_everywhere 4 months ago | on “The most cited authors in the Stanford Encyclopedia of Philosophy”
Philosophy basically forked after Kant pointed out that the human brain does a sort of computation on sensory data.

Kant basically handed it off to the scientists, mathematicians and computer scientists except for a lot of fiddly bits around justifications and foundations. This is essentially the analytic school.

The other school jumped the shark and aligned with the Romantics and produced Hegel and Fichte and their descendants. This group tends to be opposed to the idea that truth exits and politically has a reactionary longing toward an imagined primitive past. And that longing combined with the belief in the inevitability of history and the valuelessness of human life has led to immense amounts of destruction. For the most part there just isn't anything to cite there that counts as philosophy in the sense it was understood since ancient times.

35.
avgcorrection about 1 year ago | on “A Science of Life Without Free Will by Robert Sapolsky”
If there ever was a Western Philosophical mind-disease then it would be the idea that you monorail your whole entire—absolutely all of it—personal philosophy by believing in one single thing.

- Do you like Buddhist mindfulness? Well then you must necessarily commit yourself to the idea that people should be happy doormats for other people and for their circumstances

- Do you believe that sociological factors are a thing? Well then you must necessarily think that poor people and other “poor people” have no agency and are only a victim of circumstances (conservative rebuke)

- Do you believe in Science (waves hands)? Well then you necessarily commit yourself to a certain kind of Determinism that me and a cackle^W gaggle of scientists (who aren’t philosophers) find to be Obvious

Doing biology while factoring in Free Will sounds nonsensical. So don’t do it. But what you do outside of biology (on your free time)? Well, that’s something different.

I think Chomsky said something like this about the Free Will debate: if Free Will isn’t real then debating it won’t matter, per definition. Unless you are fine with the discourse devolving into “I am making these moral arguments because I am compelled to by circumstance”, in which case everything you are saying is—by your own admission—vacuous.

36.
blatant303 over 1 year ago | on “Will we run out of ML data? Evidence from projecting dataset size trends (2...”
First principles don't work in the space of systems geared towards extreme generalization such as LLMs. You need to be ready to compare anything with anything and build bridges between many principles. In fact there is a deep link between the progress of structuralism in mathematics culminating with homotopy type theory and its parallel (r)evolution in the humanities with the discovery of manuscripts by the founder of structural linguistics, Ferdinand de Saussure.

Identity is what provides the irreducible basis, in the sense that we cannot enter into the consideration of specific facts that are placed under this identity, and it is this identity that becomes for us the true concrete fact, beyond which there is nothing more.

...

For example, for a musical composition, compared to a painting. Where does a musical composition exist? It is the same question as to know where 'aka' exists. In reality, this composition only exists when it is performed; but to consider this performance as its existence is false. Its existence is the identity of the performances.

...

For each of the things we have considered as a truth, we have arrived through so many different paths that we confess we do not know which one should be preferred. To properly present the entirety of our propositions, it would be necessary to adopt a fixed and defined starting point. But what we are trying to establish is that it is false to admit in linguistics a single fact as defined in itself. There is, therefore, a necessary absence of any starting point, and if some reader is willing to follow our thoughts carefully from one end to the other of this volume, they will recognize, we are convinced, that it was, so to speak, impossible to follow a very rigorous order. We will allow ourselves to present, up to three or four times in different forms, the same idea to the reader because there really is no starting point more appropriate than another on which to base the demonstration.

...

As language offers no substance under any of its manifestations, but only combined or isolated actions of physiological, physical, and mental forces, and as nevertheless all our distinctions, our terminology, and all our ways of speaking are based on this involuntary assumption of a substance, we cannot refuse, first and foremost, to recognize that the most essential task of the theory of language will be to untangle what our primary distinctions are all about.

...

There are different types of identity. This is what creates different orders of linguistic facts. Outside of any identity relationship, a linguistic fact does not exist. However, the identity relationship depends on a variable point of view that one decides to adopt; therefore, there is no rudiment of a linguistic fact outside the defined point of view that presides over distinctions.

Source: http://www.revue-texto.net/docannexe/file/116/saussure255_6....

TL;DR: identity is equivalent to equivalence

37.
furcyd over 2 years ago | on “The Three Crises in Mathematics by E Snapper (1979) [pdf]”
A clear discussion of logicism, intuitionism, and formalism.

38.
mjburgess 11 months ago | on “A Terribly Serious adventure: Philosophy and war at Oxford”
I think this bares little relationship to research philosophy. Social epistemology, the logic of imagination, hyperinstensionality, computational theories of cognition, physical church-turing thesis, philosophy of physics, of mind, of...

There is a significant amount of question-answering going on.

That these answers come to be fundamental to other disciplines is the success of philosophy, and obvious. Those other disciplines are where philosophical premeses are taken as certain.

39.
masswerk almost 2 years ago | on “Caricaturing Noam Chomsky”
Lacan was much influenced by Hegel in his philosophical foundations, or rather, by the French interpretation as by Alexandre Kojève (who was in turn inspired by some, who started rather in a Neo-Kantian tradition, like Heidegger and Alexandre Koyré.) Moreover, Lacan had quite extensive empirical ground (having worked in psychiatric institutions for quite some time) and was informed by the state of art in Cybernetics and nascent AI research. (E.g., his prime example for fascination was derived from interlocking automata.) What all these have in common, is that reasoning about conceptuality, language, consciousness, expression and ethics, even hard along the edges of what is called the "language barrier", isn't exactly new. (Also, I'm personally part of a European tradition and can't help it.) Last but not least, there was already this post-structuralist notion of "text", which emphasized how text was essentially writing itself, without suggesting that this would imply us letting go of our agency.

Anyways, what I tried to suggest was that there is some base for reasoning that what we're encountering here is more like a cargo cult of conceptuality than conceptuality itself at work. And that it is still on us to decide, if want to be persuaded by this to land our planes of reason next to it, or not.

40.
eternalban almost 2 years ago | on “Can large language models reason about medical questions?”
Two questions that I keep asking myself:

- What distinguishes thinking and computing?

- What is more fundamental? Form or meaning?

> is this all just a game of syntax that _feels_ like semantics from the inside?

The point is that in case of LLMs, there is no mystery here: it is syntactic templates (x) statistics. But do we know that 'syntax' is all there is to our mental reasoning? We don't know this (yet). I have flirted with the notion - heretical for me since I 'believe' in primacy of meaning, not form - and can kinda squint and see meaning possibly being a 'virtual' phenomena and possibly simply a 'narrative' of 'arcs' connecting 'facts' mapped to a syntactic template. That meaning is simply a process. [So, not even "42" ..]

41.
usgroup about 2 years ago | on “Discovering latent knowledge in language models without supervision”
Exciting times. The philosophical ramifications of the syntax/semantics distinction is not something people think much about in the main. However, due to GPT et al they will do soon :)

More to the point, consistency will improve accuracy in so far as inconsistency is sometimes the cause for inaccuracy. However, being consistent is an extremely low bar. On a basic level even consistency is a problem in natural language where so much depends on usage -- it is near impossible to determine whether sentences are actually negations of each other in the majority of possible cases. But the real problem is truth assignment to valid sentences else we could all just speak Lojban and be done with untruth forever.

42.
mhh__ 4 months ago | on “Defenders think in lists. Attackers think in graphs. So attackers win”
> Philosophy has an affinity with despotism, due to its predilection for Platonic-fascist top-down solutions that always screw up viciously. Schizoanalysis works differently. It avoids Ideas, and sticks to diagrams: networking software for accessing bodies without organs. BWOs, machinic singularities, or tractor fields emerge through the combination of parts with (rather than into) their whole; arranging composite individuations in a virtual/ actual circuit. They are additive rather than substitutive, and immanent rather than transcendent: executed by functional complexes of currents, switches, and loops, caught in scaling reverberations, and fleeing through intercommunications, from the level of the integrated planetary system to that of atomic assemblages. Multiplicities captured by singularities interconnect as desiring-machines; dissipating entropy by dissociating flows, and recycling their machinism as self-assembling chronogenic circuitry.

43.
beaconstudios over 2 years ago | on “Are you nice or kind?”
The actual place where the 'whys' end is the axiomatic foundations of the model. The axioms for the categorical imperative are self-referential in the abstract (universal rules are good because they are good), whereas the axioms for utilitarianism are rooted in maximising subjective human experience. One is fundamentally materialist, the other idealist. You are right in that you can't argue on those grounds because the two sides value different things, in the same way that religious versus material justifications for actions cannot be reconciled. That does not mean that they have equal merit.

44.
skissane 11 months ago | on “The question of poverty vs. instability”
> There’s holes in all kinds of things people believe. If you’re talking about actual belief systems then you can’t assume rational actors and logical thinking. Philosophy really doesn’t have the tools to explain what’s going on. We’re in the realm of psychology / neuroscience.

No doubt the average person's beliefs are full of holes, but education in philosophy and logic can make one aware of those holes, aware of one's hidden assumptions, the heretofore unconsidered alternatives to one's positions. I question whether psychology or neuroscience can offer us the same things.

> Mind uploading blurs the line between life and afterlife because it wasn’t conceived of when those ideas were created.

Nobody knows whether mind uploading is really possible. It is a purely speculative technology, its development could be centuries or millennia away, if it ever is developed at all. I also think its philosophical significance is overrated, since in principle it is just as compatible with idealism or dualism as it is with materialism.

> IMO the conceptual framework that created the idea of a material world is really a delineation between magic rituals / religion and what people actually observed.

I disagree. Most non-materialist arguments have as their starting point epistemology, not anything to do with magic, rituals or religion. Should our ontology straightforwardly mirror our epistemology (i.e. a first-person perspective is epistemologically fundamental and hence should also be ontologically fundamentally)–or invert it?

45.
aaroninsf about 2 years ago | on “Bitcoin Is Digital Scarcity”
Almost every aspect of this critique is only coherent if you already accept the premises of crypto speculators and the political halo around it.

Favorite LOL example: the unconsidered presumption that "centralized" is a pejorative.

Second favorite example: that contemporary national debt is worthy of a stunned "don't have."

Welcome to contemporary economic policy 101. Please find a seat close to the front so you can read the slides.

I haven't thought much before about the overlap between analytic realism a la Quine and anglo-american analytic language philosophy c. 1970,

and the libertarian foundations of the crypto woo woo, but I'm motivated now to take a closer look.

It certainly makes the halo around the smart contract and the reverential tone of formulas like "in the code" and "on the chain" a lot more intelligible.

(TLDR: in 1970 there were still philosophers who took seriously the that in some sense "truth" and in particular its assertion through language, corresponded to Things in the world, and indeed was a Thing, rather than e.g. language merely being a mechanism for agents to collaborate in information exchange through negotiated patterns... the difference being, in the real world, language is just a game played by agents who each model one another and presume intent and agency...)

46.
meroes almost 3 years ago | on “Why Quantum Mechanics?”
This goes against the idea of strong emergence (mainly the bottom up causation part). I’m not arguing for or against it, but the SEP has an entry about it https://plato.stanford.edu/entries/properties-emergent/

“This often takes the form of rejecting physical realization, affirming fundamental higher-level causal powers, or both.”

I find it fascinating to wonder about causal powers in multiple directions. Does it even make sense? Does it conflict with physicalism? What is the physical anymore? Chomsky says there is no physical for the past few hundred years. It’s worth reading about at least.

47.
visarga 5 months ago | on “The Genomic Code: The genome instantiates a generative model of the organis...”
DNA "generates" the body, which generates behaviour, which affects gene survival, closing the loop.

<rant>

It's a syntactic process with the ability to update syntax based on outcomes in the environment. I think this proves that syntax is sufficient for semantics, given the environment.

Wondering why Searle affirmed the opposite. Didn't he know about compilers, functional programming, lambda calculus, homoiconicity - syntax can operate on syntax, can modify or update it. Rules can create rules because they have a dual status - of behaviour and data. They can be both "verbs" and "objects". Gödel's incompleteness theorems use Arithmetization to encode math statements as data, making math available to itself as object of study.

So syntax not fixed, it has unappreciated depth and adaptive capability. In neural nets both the fw and bw passes are purely syntactic, yet they affect the behaviour/rules/syntax of the model. Can we say AlphaZero and AlphaProof don't really understand even if they are better than most of us in non-parroting situations?

</>

48.
fsckboy over 1 year ago | on “On the aesthetics of the syntax of declarations (2018)”
I think philosophically this article should clarify the semantics of the languages, what needs to be specified in the declarations. Then talk about the syntax if there is any further need to. Languages are all about semantics; in principle there is no reason languages couldn't offer each user his own choice of syntax.

49.
garbagetime almost 2 years ago | on “A Philosophy Professor’s Final Class”
Nope. His work on the questions of the nature of thought, language, and mind is both extremely influential & deeply philosophical.

Some of his work on linguistics you can argue is not philosophy. Some I think clearly is.

50.
Karrot_Kream over 1 year ago | on “Unpredictable abilities emerging from large AI models”
This is a long running strain of criticism against continental philosophy. I don't mean to write this to refute you, just that many people do think that the continental philosophers are tilting at windmills and aren't really based around objective reality in the way a lot of earlier (pre-Hegel) Western philosophers were. There's also people who would claim that Derrida's linguistic relativism is in fact bolstered by LLMs.

51.
_t89y 9 months ago | on “Is Cosine-Similarity of Embeddings Really About Similarity?”
Having a mid-century theory of natural language semantics isn't necessarily a bad thing. You just have to pick the right one.

52.
quetzthecoatl 5 months ago | on “General Theory of Neural Networks”
weren't the sophia/gnosis, emnations and eons were from greek philosophy? also any philosophy/hottakes that stress on duality (what's seen here and what's out there that is causing what's seen here - such as manichean, advaita etc).

53.
lukev 11 months ago | on “Differences Between miniKanren and Prolog”
I share your wish that there were materials that more clearly mapped between these concepts. There's several implicated but separate concerns:

- A syntax for expressing data or rules

- A semantics for data

- A semantics for logic (e.g. a logical formalization)

- A specific set of logical axioms & rules

- An algorithm for executing a logic program

- A storage system for facts/data

- A query system for facts/data

Most of the things you mention are a bundling of some or all of the above concepts. This makes sense because it's easier to provide working implementations that way.

Some systems offer a limited amount of pluggability or alternative implementations for the same concept (e.g, different "solvers" for OWL.)

But I find that there's a pronounced lack of shared formalisms (or even shared knowledge) even around things that logically should be pretty convergent. Especially if they come from different domains originally (logic programming vs. theorem proving vs. constraint programming vs. database theory, for example.)

54.
ff00 about 1 year ago | on “Whither philosophy?”
I think distinction should be made between academic tradition of philosophy and personal need/practice of philosophy.

My favorite essay on this topic is https://courses.aynrand.org/works/philosophy-who-needs-it/ .

It is based on a lecture given by Ayn Rand to the graduating class of West Point Military Academy in March 1974.

Some people will be alarmed by the name, Ayn Rand, so I will post few snippets and then you can choose to ignore or read the ~10 minutes essay.

The assignment I gave myself for tonight is not to sell you on MY philosophy, but on philosophy as such.

The best way to study philosophy is to approach it as one approaches a detective story: follow every trail, clue and

implication, in order to discover who is a murderer and who is a hero. The criterion of detection is two questions:

Why? and How? If a given tenet seems to be true - why? If another tenet seems to be false - why? and how is it

being put over? You will not find all the answers immediately, but you will acquire an invaluable characteristic: the

ability to think in terms of essentials.

Now some of you might say, as many people do: "Aw, I never think in such abstract terms - I want to deal with

concrete, particular, real-life problems - what do I need philosophy for?" My answer is: In order to be able to

deal with concrete, particular, real-life problems - i.e., in order to be able to live on earth.

You might claim - as most people do - that you have never been influenced by philosophy. I will ask you to

check that claim. Have you ever thought or said the following? "Don't be so sure - nobody can be certain of

anything." You got that notion from David Hume (and many, many others), even though you might never have

heard of him. Or: "This may be good in theory, but it doesn't work in practice." You got that from Plato. Or: "That

was a rotten thing to do, but it's only human, nobody is perfect in this world." You got that from Augustine. Or:

"It may be true for you, but it's not true for me." You got it from William James. Or: "I couldn't help it! Nobody

can help anything he does." You got it from Hegel. Or: "I can't prove it, but I feel that it's true." You got it from

Kant. Or: "It's logical, but logic has nothing to do with reality." You got it from Kant. Or: "It's evil, because it's

selfish." You got it from Kant. Have you heard the modern activists say: "Act first, think afterward"? They got it

from John Dewey.

Some people might answer: "Sure, I've said those things at different times, but I don't have to believe that stuff all

of the time. It may have been true yesterday, but it's not true today." They got it from Hegel. They might say:

"Consistency is the hobgoblin of little minds." They got it from a very little mind, Emerson. They might say: "But

can't one compromise and borrow different ideas from different philosophies according to the expediency of the

moment?" They got it from Richard Nixon - who got it from William James.

55.
RaftPeople 6 months ago | on “A landscape of consciousness: Toward a taxonomy of explanations and implica...”
Some of the arguments I've read by philosophers seem like they are focusing on pure logic to prove a point or find weakness in another, but the linkage between those logic stmts and reality don't always seem to be considered.

It's almost like they are purely focused on symbolic logic, even if the stmts and symbols don't truly map to reality unambiguously, or without contradictions.

56.
behel about 2 years ago | on “Philosophy of mathematics – a reading list (2020)”
There are people like the late Edward Nelson who represent radical formalism: https://web.math.princeton.edu/~nelson/papers/rome.pdf

and question the consistency of the commonly used axiomatic systems like peano arithmetic: https://www.lesswrong.com/posts/gsvQSpeDHKXxjXwuM/edward-nel...

(Although his inconsistency proof of PA turned out to be flawed.)

One should also mention Wittgensteins remarks: https://plato.stanford.edu/entries/wittgenstein-mathematics/

And in general thoughts going into the direction of ultrafinitism are quite provocative.

See for example D. Zeilbergs opinions: https://sites.math.rutgers.edu/~zeilberg/OPINIONS.html

57.
mensetmanusman over 1 year ago | on “What is it like to be a bat? (1974) [pdf]”
“ Just because these sentences make sense in our everyday language, does not mean they are suitable for technical and rigorous philosophical discussions. ”

Where would one find the authority to say what is suitable or not for philosophic discussions then? This is where schools of thought arise from, because some are less afraid of the unknowns that arise under various axiomatic constraints.

Every axiom is a mystery of existence itself in any manner.

58.
DiscourseFan over 1 year ago | on “India ruling party's IT cell used AI to show smile on arrested protesters' ...”
So I mean, postmodernism is a pretty "diverse" field, and while there are some who would claim to be full relativists, they aren't usually taken very seriously.

What someone like Herbert Marcuse would claim is that there is something like a "definite negation," or Jacques Derrida would call a "trace;" that is, there, sitting behind every signifier (of de Saussure's structural linguistics) its negation of another signifier that defines its position in an ordered chain or a symbolic network. They would not argue that things are relative; rather, social life is regulated by these vast signifying networks that one becomes "subjectivated" in (begins to understand oneself as a subject), where one's "agency" also stands at the limits of this intelligibility: as they say, everything you "decide" is already pre-conscious; every notion of what is "possible," is limited by what is already "intelligible," i.e. what operates in the social-symbolic function as it, like an algorithm, constantly moves and exchanges its various symbolic orderings in what we call the "economy." To be a true agent means to do something which radically restructures this network and radically redefines all its terms: Foucault's "fearless speech," Badiou's "Event." The latter thinker actually explicitly rejects relativism (and is in some ways a platonist--actually many so-called "postmodern" thinkers, Whitehead among them, are considered platonists.)

The position you claim is from "post-modernism":

>"claims of truth are just assertions of power"

finds its origins in Nietzsche, who famously claimed that truth, morality, beauty, etc. were invented by the weak because they feared the powerful and they were jealous of the fact that they couldn't kill their enemies, eat fully, and have sex all the time (Master Morality, which was just doing whatever you wanted). So they rose up, took over, and invented these categories and systems of valuation in order to justify a new regime of power where nobody was able to do what they wanted, and everything was regulated by vast systems of control justified by these metaphysical notions (he refers to this as "Christian Morality.") He claims that Christian morality, after centuries of struggle, has finally won out in the Bourgeois revolutions of the 18th and 19th centuries, but now "God" has been subordinated to "Science" while retaining all the metaphysical baggage of christianity "truth, beauty, morality," etc. This is from Nietzsche's Genealogy of Morals, the argument from which was expanded in Foucault's Discipline and Punishment (the most widely read book from what most people would call "postmodernism").

>truth that is actually true, true in the old sense of corresponding to the reality of what is

The so-called "correspondence theory of truth" is considered a fallacy of the layman in academic philosophy. Truth has always been, in logic, a category: a value, specifically a boolean value. One can make claims about the structure and order of logic and the relation of that to psychological, social-processes (as Frege, Husserl, and many other "anti-psychologists" of the 19th and 20th centuries did); but to claim that language, a signifying act, somehow directly corresponds to the physical objects that it categorizes, can't really be valid: as Shakespeare said "a rose, by any other name, would smell as sweet." There is no reason why the words we use have any correspondence to experience--even if there is surely some reason why words can evoke experiences, emotions, like when one hears the name of a past-lover. But that can be attributed to what the "postmodernists" claim: the name evokes an entire symbolic structure and brings it to the surface of consciousness.

59.
ff00 about 1 year ago | on “Philosophy: Who Needs It (Essay 1974)”
Few recent threads on philosophy prompted me to make this submission.

Copying comment from other thread.

I think distinction should be made between academic tradition of philosophy and personal need/practice of philosophy. My favorite essay on this topic is https://courses.aynrand.org/works/philosophy-who-needs-it/ .

It is based on a lecture given by Ayn Rand to the graduating class of West Point Military Academy in March 1974. Some people will be alarmed by the name, Ayn Rand, so I will post few snippets and then you can choose to ignore or read the ~10 minutes essay.

The assignment I gave myself for tonight is not to sell you on MY philosophy, but on philosophy as such.

The best way to study philosophy is to approach it as one approaches a detective story: follow every trail, clue and implication, in order to discover who is a murderer and who is a hero. The criterion of detection is two questions: Why? and How? If a given tenet seems to be true - why? If another tenet seems to be false - why? and how is it being put over? You will not find all the answers immediately, but you will acquire an invaluable characteristic: the ability to think in terms of essentials.

Now some of you might say, as many people do: "Aw, I never think in such abstract terms - I want to deal with concrete, particular, real-life problems - what do I need philosophy for?" My answer is: In order to be able to deal with concrete, particular, real-life problems - i.e., in order to be able to live on earth. You might claim - as most people do - that you have never been influenced by philosophy. I will ask you to check that claim. Have you ever thought or said the following? "Don't be so sure - nobody can be certain of anything." You got that notion from David Hume (and many, many others), even though you might never have heard of him. Or: "This may be good in theory, but it doesn't work in practice." You got that from Plato. Or: "That was a rotten thing to do, but it's only human, nobody is perfect in this world." You got that from Augustine. Or: "It may be true for you, but it's not true for me." You got it from William James. Or: "I couldn't help it! Nobody can help anything he does." You got it from Hegel. Or: "I can't prove it, but I feel that it's true." You got it from Kant. Or: "It's logical, but logic has nothing to do with reality." You got it from Kant. Or: "It's evil, because it's selfish." You got it from Kant. Have you heard the modern activists say: "Act first, think afterward"? They got it from John Dewey. Some people might answer: "Sure, I've said those things at different times, but I don't have to believe that stuff all of the time. It may have been true yesterday, but it's not true today." They got it from Hegel. They might say: "Consistency is the hobgoblin of little minds." They got it from a very little mind, Emerson. They might say: "But can't one compromise and borrow different ideas from different philosophies according to the expediency of the moment?" They got it from Richard Nixon - who got it from William James.

60.
scoofy 7 days ago | on “A liar who always lies says "All my hats are green."”
I'll add the exchange back here to continue this thread

-----

>>scoofy: I mean, it's important to remember that the axioms of first-order logic are arbitrary. We could easily argue that the truth value of an empty group is undecidable, and that would better correlate to natural language logic.

The fact that we compact these edge cases into arbitrary truth values is just for ease of computing.

This is also relevant to the arbitrary choice of the 'inclusive or' as a default over an 'exclusive or', which most people use in natural language.

---

>foxglacier: This addresses my previous reply to you, thanks. I wonder though if there's a problem in that common natural language is inherently limited to common concepts. Scientists famously use confusing language in their papers but they're writing for people who use the same language so it's OK. For example, they use "consistent with zero" to mean "might be zero" even though a common-language reader can interpret it as "not zero". I suppose logicians use "or" to mean inclusive or in their papers too.

-----

"Absurd" here I wouldn't say is a term of art. I just mean things that not only don't mean anything, but can't mean anything. Here, existence is always extremely relevant. This goes back to Kant's idea that existence can't/shouldn't be a predicate. The idea of talking about the actual color of a nonexistent hat is absurd in that a nonexistent hat can not have a color, period, because having a color presumes existence.

So, when I talk about the logic of natural language, we have to get really philosophical. I presume that there as at least significant equivalence from formal logic to natural language, if not ultimately being fully equivalent. Formal logic is effectively a model trying to capture logical reasoning, and there are some notable differences for simplicity's sake (the Frege-Russell ambiguity thesis is a common example: https://link.springer.com/chapter/10.1007/978-94-009-4780-1_... ), however, most-if-not-all of these formal logic ambiguity concerns are trivial for natural language to deal with as any ambiguity can be clarified by an interlocutor.

Where things get really weird, however, is as you go up to the axioms of logic, and try to justify them. The idea that foundations of logic itself is determined either inductively or instinctually is just bizarre. And mapping an inductive/instinctual logic to a formal system runs into a lot of philosophical problems that aren't really practical to worry about. It just gets weird and solipsistic, as it does when you get too caught up in philosophy.

61.
taoxin 12 months ago | on “VanJS – A no-JSX framework based on vanilla JavaScript”
Here is the break down of my understanding about the philosophy described in 道德经:

1. There might be some universal rules about the world.

2. There are limitations in human's understanding and human language.

3. For all the rules that can be described in human language (which is subject to the limitations in 2), they can't hold universally.

Putting into the context where the words are cited. Any claim (in human language) about programming like "you should program in this way" is most likely not universally sound.

Of course a philosophy book 2000+ years ago can be interpreted in a different way. Happy to see the elaboration of your understanding "No method that can be explained is universally applicable".


api over 1 year ago | on “Stochastic Parrot”
I’d argue that all these models are stochastic parrots because they’re not embodied in any way. There is no way they can actually understand what they are talking about in any way that is tied back to the physical world.

What these LLMs and diffusion models and such actually are is a lossy compression method that permits structural queries. The fact that they can learn structure as well as content allows them to reason as well, but only to the extent that the rules they’re following existed somewhere in the training data and its structure.

If one were given access to senses and memory and feedback mechanisms and learned language that way, it might be considered actually intelligent or even sentient if it exhibited autonomy and value judgments.

2.
Workaccount2 over 1 year ago | on “Stochastic Parrot”
I worry that the "stochastic parrot" was premature, an idea sown early in development that will now carry along through any advances made.

Basically there is this innate idea that if the basic building blocks are simple systems with deterministic behavior, then the greater system can never be more than that. I've seen this is spades within the AI community, "It's just matrix multiplication! It's not capable of thinking or feeling!"

Which to me always felt more like a hopeful statement rather than a factual one. These guys have no idea what consciousness is (nobody does) nor have any reference point for what exactly is "thinking" or "feeling". They can't prove I'm not a stochastic parrot anymore than they can prove whatever cutting edge LLM isn't.

So while yes, present LLMs likely are just stochastic parrots, the same technology scaled might bring us a model that actually is "something that is something to be like", and we'll have everyone treating it with reckless carelessness because "its just a stochastic parrot".

3.
FabHK over 1 year ago | on “Stochastic Parrot”
The notion of the stochastic parrot is that the system produces plausible sounding words, but does not understand, does not exhibit intelligence.

Consciousness is orthogonal to the discussion (the term doesn't appear in the linked article).

4.
koalala over 1 year ago | on “Stochastic Parrot”
From the article: A "stochastic parrot", according to Bender, is an entity "for haphazardly stitching together sequences of linguistic forms … according to probabilistic information about how they combine, but without any reference to meaning."

It seems to me that the great success transformers are now enjoying is precisely due to the fact that 'probabilistic information about how they combine' _is_ meaning.

5.
mach1ne over 1 year ago | on “Stochastic Parrot”
>"stochastic parrot" is a term coined by Emily M. Bender in the 2021 artificial intelligence research paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"

This might be the first time the term was seen in an ’official’ context, but is it really the origin? It feels like the term has been hovering around for longer, and even Google Trends shows significant search trends way before 2021

6.
rini17 over 1 year ago | on “What is wrong with TOML? (2019)”
Having syntactic affordances for every nuance of semantics is what led to the current state of zoo. What is wrong with having trivial syntax and distinguishing semantics by labeling parts of the syntax tree with symbols?

7.
notShabu over 1 year ago | on “Rodney Brooks on GPT-4”
Has there been any research in the possibility that much of what we say/speak (including this convo) is actually just stochastic parroting?

e.g. instead of the stochastic parrots mimicking intelligence maybe intelligence doesn't exist, it's just stochastic parrots of various levels of sophistications organized into a hierarchy. "Intelligence" is necessarily socially defined with the more complex parrots being unpredictable and "intelligent" from the POV of lower parrots. Vicer versa, looking down, the lower parrots seem to act like "NPCs"

8.
jbay808 over 1 year ago | on “It's not just statistics: GPT-4 does reason”
I won't take a hard stance on what counts as "reasoning", which I picked in the title for lack of a better summarizing word; I am open to alternatives. So if you think that making abstractions and implementing a sorting algorithm does not count as reasoning, I will not disagree with that position. Where I am going to take a hard stance is on what does a stochastic parrot cannot do. And a stochastic parrot, defined as "stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning", cannot sort lists of 127 characters.

9.
vidarh 10 months ago | on “Every model learned by gradient descent is approximately a kernel machine (...”
The notion of a stochastic parrot is just a subset of this. It is what I'm arguing against, not for, on the basis that talking about "just" a statistical model is meaningless exactly because something being a statistical model does not imply it is just a stochastical parrot. As such it's a wildly misleading claim to make.

Yes, we're more than parrots, but that does not mean we can be described equally validly as "statistical models". Suggesting something is "just" a statistical model is a statement that is close to semantically void. It tells us near nothing about the computational limits - up or down - of something. Even a very simple system for running a statistical model with a loop around it providing IO can be made Turing complete.

10.
visarga over 2 years ago | on “Socratic Models – Composing Zero-Shot Multimodal Reasoning with Language”
"Stochastic parrots" is a disparaging term coined by SJW propaganda. As if the brain is not stochastic, or we don't parrot from cultural sources. Language models have been accused of bias and lack of explainability, but humans are biased too and can't really explain how we take decisions.

Overall this term says "limited to the intelligence of a parrot" which is false, models can solve math and coding problems, generate passable art, translate and speak in hundreds of languages and beat us at all board and card games. When was a parrot able to do that?

11.
visarga 5 months ago | on “The Genomic Code: The genome instantiates a generative model of the organis...”
I think of syntax as applying a rule. A program is just syntactic operations. The idea is that it has two aspects: "program in execution" and "program as data". Rules expressing behavior and rules expressed as data, in which case it can be processed by other rules.

One concrete example is a bootstrapped compiler. It is both data and execution. It can build itself, putting its output as input again. Another example is in math - Gödel's arithmetization, which encodes math statements as numbers, processing math syntax with math operations. And of course neural nets, you can describe them as purely syntactic (mechanical) operations, but they also update rules and learn. In the backward pass, the model becomes input for gradient update. So it is both rule and data. DNA too.

These systems that express rules or syntax that is adaptive, I think they make the leap to semantics by grounding in the outside environment. The idea that syntax is shallow and fixed is wrong, in fact syntax can be deep and self generative. Syntax is just a compressed model of the environment, and that is how it gets to reflect semantics.

This was an argument against Stochastic Parrots and Chinese Room (syntax is not sufficient for semantics) maxim. I aimed to show that purely mechanical or syntactic operations carry more depth than originally thought.

12.
zer00eyz 8 months ago | on “Borgo is a statically typed language that compiles to Go”
And in the other corner you have Chomsky with universal grammar... and in another you have Platonic Forms...

I love the "draw me a tree" idea of a Platonic form, we all have an idealized model of what that is, that is uniquely our own. With that in mind isnt everything subject to some sort of semantics?

13.
oh_sigh about 1 year ago | on “Is my toddler a stochastic parrot?”
Stochastic parrot was coined by bender, gebru, et al, but it was probably "borrowed" from Regina Rini's "statistical parrot", coined 6 months before stochastic parrots hit the scene.

https://dailynous.com/2020/07/30/philosophers-gpt-3/#rini

14.
visarga 5 months ago | on “The Genomic Code: The genome instantiates a generative model of the organis...”
DNA "generates" the body, which generates behaviour, which affects gene survival, closing the loop.

<rant>

It's a syntactic process with the ability to update syntax based on outcomes in the environment. I think this proves that syntax is sufficient for semantics, given the environment.

Wondering why Searle affirmed the opposite. Didn't he know about compilers, functional programming, lambda calculus, homoiconicity - syntax can operate on syntax, can modify or update it. Rules can create rules because they have a dual status - of behaviour and data. They can be both "verbs" and "objects". Gödel's incompleteness theorems use Arithmetization to encode math statements as data, making math available to itself as object of study.

So syntax not fixed, it has unappreciated depth and adaptive capability. In neural nets both the fw and bw passes are purely syntactic, yet they affect the behaviour/rules/syntax of the model. Can we say AlphaZero and AlphaProof don't really understand even if they are better than most of us in non-parroting situations?

</>

15.
pizza over 1 year ago | on “Many in the AI field think the bigger-is-better approach is running out of ...”
Sounds very much like {Matilde Marcolli, Noam Chomsky, Robert Berwick}’s brand new paper on the mathematics of Syntactic Merge [0]:

workspace :: nested forests of binary trees of syntactic structures (with no label order) (= here, thoughts and meanings are assumed to be composites of mental syntax objects that can be re-combined with others. Big assumption? Maybe)

externalization process :: some mental faculty that decides the order to use when outputting thoughts into ordered strings (eg vocalization of thoughts into sentences)

The thing is, you can put a probability theory onto anything that you can count or record states of. Of course, counting the external observations may lack the richness of the internal process. I feel that this linguistic program will work its way into a lot of future LM tooling, in some incarnation or another.

[0] https://arxiv.org/abs/2305.18278

16.
orbital-decay 10 months ago | on “Why Are LLMs So Gullible?”
I think that neither side has a convincing argument in these discussions, actually, I'm pointing out that both extremes - anthropomorphism and stochastic parrot - lack any foundation, so I wouldn't be so confident. The behavior of neither statistical models nor biological systems is well understood. It's entirely possible that every trait you consider human can naturally emerge from a dumb statistical model, and indeed certain processes are remarkably similar as the models get bigger, smarter, and have better training data. It's possible that it can't, however.

17.
dragonwriter over 1 year ago | on “Rodney Brooks on GPT-4”
> One way to resolve this surprise is to find some reason to believe these strange abilities are fundamentally not an understanding of the world. Thus stochastic parrots, this article, Yan LeCun and Chomsky, etc.

I mean, it is kind of obvious that predicting text based on a large corpus of text written by people with a model of the world will, if it works at all, look like having a model of the world.

The question is whether that involves having a model of the world, and secondarily, if there is a difference, what is the practical impact of the difference?

Or maybe that’s not really the question, because the whole “model of the world” thing is metaphysical omphaloskepsis that is inherently unanswerable because we can’t actually unambiguously reduce it to something with testable empirical predictions, reflecting a rationalization for elements of our culture and particularly our own view of our own special place in the universe, and the different answers to it likewise have no real meaning but simply reflect people’s bias for whether or not they want to share that special place, either in general or with the particular computing systems under discussion in particular.

18.
selalipop over 1 year ago | on “Code Interpreter will be available to all ChatGPT Plus users over the next ...”
What about those of us who do understand them and just don't agree?

After all you could simplify it to a layperson as: 'the LLM is just doing fancy autocomplete based on how stuff appeared in the training data so that means they're not creative'

The first part is not really up for debate, but it's the second part is where some of us disagree. Creativity doesn't mean novel in existence, it means novel within some context: https://www.researchgate.net/publication/254301596_The_Stand...

—

At some point, the push back against these models being creative starts to feel like it's just as emotion driven as the people who are over-anthropomorphizing the models: "If I accept something I know is just a ball of linear algebra is creative, then it's cheapening the definition of creativity."

People bring up the stochastic parrot argument forgetting that the original paper was predicated on the dangers of not considering the power that lies in something that's "just" a stochastic parrot.

19.
jameshart 9 months ago | on “Natural language instructions induce generalization in networks of neurons”
Strong Sapir-Whorf (linguistic determinism - language constrains thought) became pretty much seen as a joke by the 1980s. Linguistic relativism (weak Sapir-Whorf - language shapes thought) is still respectable (because, I mean, of course it does).

Actually, this research might just as well be evidence for linguistic universalism (Chomsky - language enables thought).

In general linguistic philosophers have been coming out with either laughably obvious or utterly untestable hypotheses for a century and it’s amusing to see how these AI studies shake up the hornets.

20.
jacknews over 1 year ago | on “Ask HN: AI read books, Human also. What's the difference?”
I think people parroting the 'stochastic parrot' view of these large language models are failing to understand the scope for complex emergent behavior when these models have 100s of billions of parameters.

It's like saying 'it's just atoms and molecules bouncing off each other and sometimes splitting or joining', what do you mean 'storms, clouds, rain', or 'collections of them can make copies of themselves', etc.

It's Searle's old Chinese-room argument. That the machine doesn't really understand the words it's manipulating.

The standard argument (or maybe just my argument) against the chinese room is that if the 'symbols' are at lower level, audio, video stimuli, etc, and you have enough scale, then you can imagine intelligence emerging somehow, like flocking birds.

LLMs are trained at the language level, but even then, I think with big enough scale, they do in fact 'learn' concepts and form a 'world model' from the material they are trained on. How else are you able to ask for something in one language and it can output the results in another.

In some ways we can be grateful. Imagine trying to impose ethics on an 'intelligent insect', trained on much lower level inputs. These LLMs are being trained directly on human culture as input, and if we're careful with the input they will reflect the best of it.

21.
couchand about 1 year ago | on “Misalignment and Deception by an autonomous stock trading LLM agent”
It takes a particular perspective to tell a (presumed) human being "you are not special" and implying that a stochastic parrot has the same level of personhood.

A statistical model of language patterns is not a person. A statistical model of language patterns is not a living thing. Please go touch grass.

22.
usgroup about 2 years ago | on “Discovering latent knowledge in language models without supervision”
Exciting times. The philosophical ramifications of the syntax/semantics distinction is not something people think much about in the main. However, due to GPT et al they will do soon :)

More to the point, consistency will improve accuracy in so far as inconsistency is sometimes the cause for inaccuracy. However, being consistent is an extremely low bar. On a basic level even consistency is a problem in natural language where so much depends on usage -- it is near impossible to determine whether sentences are actually negations of each other in the majority of possible cases. But the real problem is truth assignment to valid sentences else we could all just speak Lojban and be done with untruth forever.

23.
whimsicalism over 1 year ago | on “Rodney Brooks on GPT-4”
> Has there been any research in the possibility that much of what we say/speak (including this convo) is actually just stochastic parroting?

I don't think that is really a well defined question. What is a stochastic parrot?

24.
sgoto over 1 year ago | on “Linguistics and Programming Languages (2006)”
A few early explorations on a similar idea:

https://code.sgo.to/2021/07/02/english--.html

https://code.sgo.to/2020/08/31/syntactic-theory.html

https://code.sgo.to/2020/09/16/semantics.html

25.
pests about 1 year ago | on “Update on KDP Title Creation Limits”
I agree with you but at what point does it change? Aren’t we all just stochastic parrots? How do we ourselves choose the next word in a sentence?

26.
convolvatron almost 2 years ago | on “Evidence of a predictive coding hierarchy in the human brain listening to s...”
the idea that some linguistic facilities are innate? or the government binding model of grammar or something else?

for the first two, I think this orthogonal

27.
solarmist about 2 years ago | on “Finding Language in the Brain”
This is an extremely dismissive comment, plus you are way off basis that it is underdeveloped. It is just new/underdeveloped for OP.

Literal books have been written about this exact idea by Zellig Harris. The reason it is not more well known is that Chomsky (his student) became famous and reinterpreted a couple of his ideas (using mathematical logic for the basis of his work rather than Harris's set-theoretic base). But while Chomsky had to keep creating castles in the sky (including the idea of universal grammar/external metalanguage) to patch the holes in his theory Harris's theory needs none of that and is self-organizing and self-sufficient.

An alternate (and consistent with OPs idea) list of layers of processing a language.

1. Phonemic distinctions.

2. Words and morphemes, with their main meanings. (The word with its morphemes is an umbrella that also encompasses gesture, visual, and any other perceptual information related to the word. The words we use are not separate entities from our experiences. In the same way, mirror neurons aren't a thing. We mirror because we only have one set of machinery, and so to understand the other person is literally to hallucinate experiencing it to some degree.)

3. Word dependencies (the argument requirement of each word).

4. The selection of each word (the dependencies that have greater than average likelihood).

5. The canonical or preferred linearization (word order) and its alternatives.

6. The main reductions (variant shapes of words), their domain (a particular word or all words in a position) and the conditions for applying them.

Linearization is one of the last steps of producing a sentence. According to Operator grammar theory, most language complications come from the simplifications we produce as shortcuts (such as pronouns, contractions, conjunctions, etc).

So your first two questions are just the memory of the words/ideas/objects/etc combined with an operator and our perceptions in context do the choosing. Very much like an AST, but statistical (for word likeliness), as opposed to deterministic.

The most differences start to arise when we start the process of moving that sentence externally. We apply reductions and linearization which are language specific.

The intersection between HPCT (Hierarchical Perceptual Control Theory) and Operator Grammar explains the details pretty well. And Zellig Harris presents a complete analysis of English Grammar in "A Grammar of English on Mathematical Principles". It is a very different way of looking at language grammar, which makes much more sense to me biologically he spends significant time showing it can also reproduce traditional grammar. I.e. Traditional grammar could be viewed as a change of basis from Operator grammar.

Take a look at An embodied grammar of words[1] for an overview.

[1] https://www.researchgate.net/publication/324362482_An_embodi...

28.
agumonkey over 2 years ago | on “An Intuition for Lisp Syntax (2020)”
It's funny cause to me visual differences in syntax are just useless information overload. And another dimension of constraints to deal with. That's why I clinged to low syntax languages.. you write in semantics almost. The rest is problem solving (or even extending metadomain with macros or else to help yourself)

29.
mannykannot almost 2 years ago | on “Caricaturing Noam Chomsky”
I think Chomsky has a point here, but somewhat ironically, it applies much more broadly, raising doubts about linguistics being the route to understanding the mind.

One thing grammar does not do is constrain grammatically-correct language to produce only the truth. It is as easy for humans as it is for LLMs to say false things, to construct false reasons for believing those falsehoods, and to be inconsistent about it. The argument Chomsky uses to claim LLMs will never lead to and understanding of the mind can equally be deployed against the prospect of understanding the mind through studying grammar.

If anything, the success of LLMs, in producing grammatically-correct language about as well as humans do, is evidence for this position. They also present a significant challenge to the Poverty of Stimulus claim[1], which is often used in support of the universal grammar hypothesis.

In the early 20th. century, western metaphysics took what has been called the 'linguistic turn'[2]. Personally, I am deeply skeptical of the proposition that studying human language will - or could, in principle - lead to knowledge about how the world must be, and the ability of people to hold inconsistent beliefs and to argue for falsehoods is one basis for that skepticism.

[1] https://en.wikipedia.org/wiki/Poverty_of_the_stimulus

[2] https://en.wikipedia.org/wiki/Linguistic_turn

30.
winddude over 1 year ago | on “Rodney Brooks on GPT-4”
> One way to resolve this surprise is to find some reason to believe these strange abilities are fundamentally not an understanding of the world. Thus stochastic parrots, this article, Yan LeCun and Chomsky, etc.

This one. This is the correct resolution.

31.
thom almost 2 years ago | on “The AI hype bubble is the new crypto hype bubble”
The stochastic parrot claim seems to crash people’s brains because they assume this is all about words. The fact that these models very clearly learn not just patterns of words, but patterns of ideas, and ideas about ideas, seems a very profound result. Personally I do remain unimpressed by how the current models ‘think’ on top of this knowledge, but I’d have to work very hard to be as cynical as to imply this was all a scam, or somehow not progress (whatever you think of the destination).

32.
photochemsyn about 2 years ago | on “Classifying Minds”
A slight aside - rather than language being the 'core of consciousness', it's the only tool available for one conscious entity to communicate the phenomenon of consciousness to another conscious entity.

If consciousness is the self-awareness that one is using sensory information to construct an internal model of reality, of the world surrounding one self, and that the internal model may be (or certainly is) an imperfect representation of that world, then language, symbolic concepts etc., are required to express that notion to others.

One other note - many if not most wild animals are highly socialized, just not in the sense that they'd obey instructions given by humans (but perhaps those given by a pack/herd leader, or a sentry on the lookout for predators, etc.)

I agree about Kant, though, and the failure of the rigid materialist viewpoint. Kant's materialism was based in an outmoded view of mathematical-physical rigor, which has been overturned everywhere, from non-Euclidean geometry to the decidability (Church-Turing) and incompleteness (Godel) issues, to quantum intederminancy and chaos/sensitive dependence - the rug was pulled out from under the Kantian materialists some time ago.

33.
contingencies over 1 year ago | on ““BASIC Computer Games” code in modern languages”
Sapir-Whorf in the conventionally conceived sense of language-as-local-limitation is clearly false, partly as it invalidates itself as an origin story. (How would you invent a language if you can't think outside of it?)

Sapir-Whorf in the sense of language-as-influence however is clearly true, even if academics occasionally state otherwise, because at their core all languages used by communities of a size greater than one individual rely upon semantics that are socially dictated.

Personally, perl screwed me up a lot more than BASIC - but who can argue with the expressive power of regex for text extraction and matching problems?

Wikipedia has a fun excerpt featuring PG (and Ruby) on just this question

https://en.wikipedia.org/wiki/Linguistic_relativity#Programm...

34.
chx over 1 year ago | on “Stackoverflow is investing into baking GenAI”
This is such baloney. Another way it resembles crypto.

Humans sense, interpret, organize, and consciously feel and experience internal and external stimuli none of which is applicable to stochastic parrots.

35.
williamcotton over 1 year ago | on “Modern language models refute Chomsky’s approach to language”
He starts with the original Transformational-generative grammar in the late 50s, and then as more and more exceptions and complexities of language were found that didn't fit neatly into the transformational-generative model, the theory was extended to account for these exceptions, creating the Extended Standard Theory (syntactic constraints, X-bar theory), later revised to Revised Extended Standard Theory (restrictions upon X-bar theory, complementizer). Then we've got the addition of the Principles and Parameters theory (head-initial, head-final) in the 80s, which led to the Minimalist program as things had started to get out of hand.

https://en.wikipedia.org/wiki/Generative_grammar

https://en.wikipedia.org/wiki/Transformational_grammar

https://en.wikipedia.org/wiki/X-bar_theory

https://en.wikipedia.org/wiki/Principles_and_parameters

https://en.wikipedia.org/wiki/Government_and_binding_theory

https://en.wikipedia.org/wiki/Minimalist_program

36.
blatant303 over 1 year ago | on “Quantum Mysticism”
Around the year 2012 I got caught in a storm of synchronicities. I was led there through a bunch of things and ideas that happened in my life at that time. I found some "strange attractors" (looks like it, I don't have the rigorous phase space characterizations, nor the knowledge to do it). Pick one language. Find two frequency lists. Remove words that aren't present in both and establish ranks (Or establish ranks then remove words that are not share, the order is critical, it was a long time ago and my memories have faded). Then do as if you were plotting a Zipf distribution but instead of comparing rank against frequency, compare each rank with (absolute) distance to the rank in other frequency list. You observe some structures merging amid a mist of points and continuity lines emerge. Because some frequencies come multiple times, the order of the list is partial but the plot demands total order anyway. There is some arbitrary ordering to do, so randomize those choices and make the seed vary: the points in the cloud change, but the metastable structure remains. Change your sorting algorithm, the shape of the structure changes. It remains (meta)stable.

Around the same time I play a philosophical game with a friend. Pivot subjects and nouns around their verb and observe. Don't say "I smoke a cigarette" but say "the cigarette smokes me". It often makes sense. And not just because the verb meaning implies the subject and object are naturally interchangeable. It creates meaning figuratively. I start envisioning a generalized commutation operation for linguistics (was studying this field at the time) that would not just be a tool to probe the structure of meaning. It would be its very foundation. The commutation would be like a geometric pivot: it's continuous. I sketch ideas about mapping the space of these commutation that create meaning figuratively, even oracularly. And I have that picture in mind that I can now convey by evoking scales weighting scales ... weighting the scales they sit on. I need a "fractal" byte, where each bit can be seen as a sub-byte, ad-infinitum, and operate computations at every scale at the same time. That's when it striked me what I needed was a quantum bit of some sort. At this point I have no interest in QM, it's just that I have that sudden realization that I need that stuff I read about in pop science magazines. It's as if the quantum came to me, not the other way around. Saying this now that I introduced that idea of linguistic pivot may make you smile. But it was the bread butter of my life at that time, it was the consubstantial mud at the bottom of synchronicity pit I was at the bottom of. Anyway it's not like I was coming up with a new idea, a representation of something (sketchy or not). No it was about incarnating that very idea, discovering the element that motivated your search in the first place. Imagine: a theory of how ideas emerge in theoreticians was emerging in me. It's everywhere in fact. Einstein elaborated a philosophical relativity theory with a "bro" when he was younger. Heisenberg too. There is some will for eternal return a need to close his inner loop in Strogatz when he concludes his chaos theory popularization book by saying sync may even explain how he wrote that book. Ditto with that woman quanta magazine interviewed about her work around geometrical forms whose interaction in a system make the entropy decrease. She says her objects of study are at play in the chaos that is her department as she oversees 40 phd students. This is what I call incarnation.

The story continues: 2012. Start studying computer science. Math teacher is a chaos theorician. He introduces himself and presents his main contribution to science. He draws on the blackboard what I would draw if I had to schematize those Zipfian pseudo-attractors I was trying to capture at that time. Incarnation strikes again, yet another synchronicity to add to the list. I thought I had caught an attractor, and now it seemed as if it was the the attractor that had caught me. Nonetheless nothing came out with my interaction with that researcher. I kept the secret of my formula (explained above), he thought I was spooky.

Some 10 years after the facts what can I tell ? Quantum linguistics is a booming field and there are people running question answering algorithms, on quantum hardware, with quantum native theories of language. The dude behind this says "language is quantum native". His master, Aerts, has views that goes along the line of "quantum physics is in fact a quantum linguistics construct". I'm exaggerating, but just a bit. Concretely, what do I have ? Nothing but juxtaposition and future-proves-past kind of non-points. Like this:

2019

https://deepai.org/publication/the-role-of-grammar-in-transi...

> Sentence formation is a highly structured, history-dependent, and sample-space reducing (SSR) process. While the first word in a sentence can be chosen from the entire vocabulary, typically, the freedom of choosing subsequent words gets more and more constrained by grammar and context, as the sentence progresses. This sample-space reducing property offers a natural explanation of Zipf's law in word frequencies, however, it fails to capture the structure of the word-to-word transition probability matrices of English text. Here we adopt the view that grammatical constraints (such as subject--predicate--object) locally re-order the word order in sentences that are sampled with a SSR word generation process. We demonstrate that superimposing grammatical structure -- as a local word re-ordering (permutation) process -- on a sample-space reducing process is sufficient to explain both, word frequencies and word-to-word transition probabilities. We compare the quality of the grammatically ordered SSR model in reproducing several test statistics of real texts with other text generation models, such as the Bernoulli model, the Simon model, and the Monkey typewriting model.

37.
meheleventyone about 1 year ago | on “OpenAI researchers warned board of AI breakthrough ahead of CEO ouster”
I don’t think we should throw out the stochastic parrot so easily. As you say there are “clear signs of creativity” but that could be it getting significantly better as a stochastic parrot. We have no real test to tell mimicry apart from reasoning and as you note we also can only speculate about how any of it works. I don’t think it’s reductionist in light of that, maybe cautious or pessimistic.

38.
nmca 9 months ago | on “Self-Retrieval: Building an information retrieval system with one LLM”
I like methods of this flavour, pioneered as I understand it by Fabio Petroni. Very elegant, particularly because you can change the distribution over substrings in O(params) time instead of O(index size).

What's funny about them is that it's a fairly involved procedure that turns your language model into an actual stochastic parrot, both showing that such a model useful and demonstrating that the original parrot concept was rather ill conceived.

39.
AIorNot about 2 months ago | on “The Thought Experiments That Fray the Fabric of Space-Time”
Besides the silly, but inevitable HN complaints about the format of the webpage presentation, (great presentation btw)

The fundamental challenges these experiments (and others) surface is a deep challenge to the traditional narratives of Materialism or 'Physicalism' as our understanding of what existence is. In essence science and human knowledge has lept forward technologigcally over the past 400 and esp the past 100 years because we started assuming the world was physical in nature, material and metaphysically, ie that it reduced to fundamentally physical things we could quantify and measure.

Yet, the older I get the more inclined I am to believe in some form of Idealism.. Not only in Idealism but I'm leaning towards the belief that some kind of fundamental universal Consciousness is the only fundamental property or baseline to the universe or to existence.

Time and Space is not fundamental. Locality isnt true.

40.
pizza 8 months ago | on “Self-reasoning tokens: teaching models to think ahead”
Not exactly the same game but you might be interested in Mathematical Structure of Syntactic Merge, Marcolli, Chomsky, Berwick (2023).

When we speak we give a string. When we think we don't have to use a string. But we do have to have a functionality to map something that has no single ordering to something that has an ordering (externalization) - a sentence. And vice versa we have a functionality to turn strings into things without a specific ordering (internalization) - thoughts.

41.
solarmist about 2 years ago | on “Finding Language in the Brain”
This seems very consistent with An embodied grammar of words[1] a less well-known but very compelling pair of theories (Operator Grammar and Perceptual Control Theory).

[1] https://www.researchgate.net/publication/324362482_An_embodi...

42.
daveguy 10 months ago | on “Why Are LLMs So Gullible?”
The stochastic parrot is much more grounded in the physical reality of how the models are structured, trained, and produce outputs than the "it's like people" argument. Statistical models are much better understood than biological systems. That said, your penultimate statement is true:

> It's entirely possible that every trait you consider human can naturally emerge from a dumb statistical model, and indeed certain processes are remarkably similar as the models get bigger, smarter, and have better training data.

I just don't think it's likely given the biological complexity we are washing over with the statistical model.

43.
bbor 10 months ago | on “Learning resources for curious software engineers”
Any thoughts for a Philosophy section? i.e. meta-CS. I'm thinking about stuff like this, though I guess it would better as blog posts or second-hand critiques:

- Chomsky 1956, _Three models for the description of language_ https://chomsky.info/wp-content/uploads/195609-.pdf

- Naur 1985, _Programming as theory building_ https://pages.cs.wisc.edu/~remzi/Naur.pdf

- Simon & Newell 1970, _Human Problem Solving_ https://iiif.library.cmu.edu/file/Newell_box00018_fld01306_d...

etc. Maybe too broad a topic? Basically this list feels like it's lacking answers to some fundamental "why is it like this" questions, if that makes sense.

44.
adamnemecek over 2 years ago | on “How to read mathematics (2015)”
Constructivism. Linear logic, geometry of interaction, transcendental syntax.

45.
tgv over 1 year ago | on “Scaling Transformer to 1M tokens and beyond with RMT”
The "stochastic parrot" primarily serves to distinguish the (murky) workings of the human brain and that of an ANN. Increasing context doesn't change that. It might make some tasks feasible that cannot be performed now (although I can't see how to properly train it, but who knows), but it doesn't change the fact that the system as whole has a memory that works differently, and e.g. even doesn't monitor its own output vs. its intentions. It's far from a conscious entity.

> "train" a LLM at run time by filling the context with examples.

Which is precisely what a stochastic parrot needs.

46.
ribit almost 2 years ago | on “The Norvig – Chomsky debate (2017)”
What’s really interesting is that entire Chomskian syntax worldview is fairly pseudo-scientific in nature. Most of these papers are about taking an essentially Turing-complete computation system and tweaking it until it can solve a specific riddle. Rinse and repeat. Most of the arguments (like the poverty of stimulus) are purely authoritarian as well.

47.
mushufasa about 1 year ago | on “Chomsky explains why nobody is a moral relativist”
I think what Chomsky is saying here "rhymes" with his theory of Universal Grammar (UG).

The UG belief is that there are a finite set of "primitives" of language, and all human languages have grammar syntax that are the permutations of those primitives. This is actually pretty clearly the case for programming languages -- primitives like addition/subtraction, methods, objects, etc. -- are chosen by language developers in different doses, and we group languages together based on these grammatical syntax choices (e.g. functional family of languages, declarative/imperative languages).

It sounds like Chomsky is claiming morals work the same way; there are a finite set of things that humans find reprehensible or good. Per that, certain cultures at certain times may have different permutations of what they group into the 'reprehensible bucket,' but the set of choices is constrained.

48.
MAXPOOL over 1 year ago | on “Modern language models refute Chomsky’s approach to language”
> some evolved language structures in the brain.

That's Chomsky's argument. A small set of constraints for organizing language.

49.
mikhailfranco over 1 year ago | on “The Symbol Grounding Problem (1990) [pdf]”
It's metaphors all the way down, until you hit sensory grounding, space and time.

Discrete objects give integer arithmetic. Correspondence gives equality. Spatio-temporal behavior gives basic logic: concurrent AND, choice OR, inside/outside, under/over, up/down, more/less... Properties and behaviors cluster to give categories in a context. Action frames give role bindings for actors...

It's Lakoff&Johnson all the way down.

50.
PaulHoule almost 2 years ago | on “The False Promise of Chomskyism”
The irony of it is that Chomsky’s ideas are the foundation of parsers for programming languages. That is, you can make a link between the semantics of arithmetic and logic and the kind of grammar Chomsky talks about and you have… C, Pascal, Python, etc.

And people understand it!

The semantics of natural languages is over the experience of an animal and if you think of it as a “language instinct”, that instinct is a peripheral of a animal’s brain which has considerable capability for cognition (particularly in the case of mammals and birds) without language.

From that point of view natural language competence is a cherry on top of animal intelligence and you can’t simulate a language-using animal without simulating the rest of the animal.

ChatGPT does a surprisingly good job of faking linguistic competence with nothing but language so it certainly looks like a challenge to the Chomskyian point of view but I’d remind you that animals, pre-linguistic children, aphasics are all capable of cognition without language so “language is all you need” is still an incomplete position. (But boy there is that strange fact that transformers work for vision although research may be showing that is accidental?)

What does it mean for the mind?

A major part of animal behavior involves sequences. Think of tying a knot or a bird migrating, or for that matter serializing and deserializing mental structures into words. In the 1980s I read books about the brain that were lost at sea about how a neural network could model sequences and artificial neural networks have come a long way since then.

51.
int_19h 10 months ago | on “Generative Models: What do they know? Do they know things? Let's find out”
I find it amazing that, for all the evidence we have of generative models having some fairly complex internal model of the world, people still insist that they are mere "stochastic parrots" who "don't really understand anything".

52.
bbor over 1 year ago | on “Consciousness and Anaesthesia (2009)”
This is typical internet behavior but I just have to link some Chomsky talks on the topic. One's 7 minutes, and the other's 85 - I recommend the second, of course ;)

7 minutes: The Concept of a Person https://www.youtube.com/watch?v=Y0XdIT_Cn4E

85 minutes: Grammar, Mind and Body - A Personal View https://www.youtube.com/watch?v=wMQS3klG3N0

Especially relevant if you read that article yesterday "disproving" Chomsky by noting that ChatGPT exists, and wanted to consider what his rebuttal might be.

TL;DR:

My current view is one of consciousness operating on a spectrum,

He would definitely agree

there has to be some degree of definition that we can agree to as a starting point

He would definitely disagree, as I understand your sentiment; settling on a definition for the concept you're interested in is as impossible as settling on a definition for "meaning", "love", or "justice".

it's just a useful illusion/delusion and evolutionary adaptation for cooperative behavior.

Alternative hypothesis discussed in the 85min talk: what if cooperation is a useful corollary for consciousness/higher level reasoning via internal linguistic structures?

The whole thing is a mess. Most likely... we're not special

He would whole-heartedly agree, in the cosmic sense, and whole-heartedly disagree in the sense of our relation to animals.

53.
meroes almost 3 years ago | on “Why Quantum Mechanics?”
This goes against the idea of strong emergence (mainly the bottom up causation part). I’m not arguing for or against it, but the SEP has an entry about it https://plato.stanford.edu/entries/properties-emergent/

“This often takes the form of rejecting physical realization, affirming fundamental higher-level causal powers, or both.”

I find it fascinating to wonder about causal powers in multiple directions. Does it even make sense? Does it conflict with physicalism? What is the physical anymore? Chomsky says there is no physical for the past few hundred years. It’s worth reading about at least.

54.
foobarqux 8 months ago | on “Uniquely human intelligence arose from expanded information capacity”
The fundamental point isn't really a new argument and Chomsky pointed out the problems with it decades ago: It doesn't explain why certain cognitive capacities work one way and not another. For example why is language hierarchical and not linear [1]?

[1] The authors (sort of) dispute that language is hierarchical but you can make the same point about many cognitive faculties: Among the theoretically possible manifestations of a given cognitive capacity why is this the specific one that exists and how does it work?

55.
Dracophoenix 6 months ago | on “ChatGPT is biased against resumes with credentials that imply a disability”
It's an issue of correlating semantics with preconceived value-judgements (i.e. the is-ought problem). While this may affect language as a whole, there are (often abstract and controversial) terms/ideas that are more likely to acquire or have already acquired inconsistent presumptions and interpretations than others. The questionable need for weighting certain responses as well as the odd and uncanny results that follow should be proof enough that what is expected of a human being to "just get" by other members of "society" (an event I'm unconvinced happens as often as desired or claimed) is unfalsifiable or meaningless to a generative model.

56.
kragen about 1 year ago | on “They're Made Out of Meat (1991)”
i think the point of the story is exactly what you've gotten from it

namely, it's absurd to dismiss the possibility of self-awareness in some system because it's built out of parts different from the parts other self-aware systems you're familiar with are built out of

a useful thing to keep in mind when the stochastic parrots start squawking about how large language models aren't actually intelligent

shpxvat vf terng

57.
benreesman over 2 years ago | on “Imagen, a text-to-image diffusion model”
It’s just my opinion but I think the meme you’re talking about is deeply related to other branches of science and philosophy: ranging from the trust old saw about AI being anything a computer hasn’t done yet to deep meditations on the nature of consciousness.

They’re all fundamentally anthropocentric: people argue until they are blue in the face about what “intelligent” means but it’s always implicit that what they really mean is “how much like me is this other thing”.

Language models, even more so than the vision models that got them funded have empirically demonstrated that knowing the probability of two things being adjacent in some latent space is at the boundary indistinguishable from creating and understanding language.

I think the burden is on the bright hominids with both a reflexive language model and a sex drive to explain their pre-Copernican, unique place in the theory of computation rather than vice versa.

A lot of these problems just aren’t problems anymore if performance on tasks supersedes “consciousness” as the thing we’re studying.

58.
PaulHoule over 2 years ago | on “DeepMind: A Generalist Agent”
I will grant that the grounding is important.

The worst intellectual trend of the 20th century was the idea that language might give you some insight into behavior (Sapir–Whorf hypothesis, structuralism, post-structuralism, ...) whereas language is really like the evidence left after a crime.

For instance, language maximalists see mental models as a fulcrum point for behavior, and they are, but they have nothing to do with language.

I have two birds that come to my window. One of them has no idea of what the window is and attacks her own reflection hundreds of times a day. She can afford to do it because her nest is right near the bird feeder and doesn't need to work to eat, in fact it probably seems meaningful to her that another bird is after her nest. This female cardinal flies away if I am in the room where she is banging.

There is a rose-breasted grosbeak, on the other hand, that comes to the same window. She doesn't mind if I come close to the window, instead I see her catch the eye of her reflection and then catch my eye. She basically understands the window.

Here you have two animals with two different acquired mental models... But no language.

What I like about the language-image models is how the image grounds reality outside language, and that's important because the "language instinct" is really a peripheral that attaches to an animal brain. Without the rest of the animal it's useless.

59.
visarga 9 months ago | on “Our tools shape our selves”
Stiegler is getting pretty close to what I consider the true answer. It's not "technics", but "language" that underlies our evolution as a species. Language is our shared repository of experience, and an evolutionary system of ideas and values in its own right. All we know comes from the environment, but most of what we know is inherited by way of language.

So the environment and language represent present and past experience, they both contribute to how we choose our actions. By environment I mean not just nature, but also other people and all the things we build.

BTW, this related to LLMs in a way - they are trained mostly on language and very little on present time experience. When that changes, and present time experience will get its own rightful place, then LLMs will make better autonomous agents. Of course they are combinatorial parrots if all they know is past data, but they can escape this predicament when they feed on present data.

60.
hackinthebochs almost 2 years ago | on “Theory of Mind May Have Spontaneously Emerged in Large Language Models”
So much rides on your implicit notion of semantic relationship, but this dependence needs demonstration. The fact that some pattern of signals on my perceptual apparatus is caused by an apple in the real world does not mean that I have knowledge or understanding of an apple in virtue of this causal relation. That my sensory signals are caused by apples is an accident of this world, one we are completely blind to. If all apples in the world were swapped with fapples (fake apples), where all sensory experiences that have up to now been caused by apples are now caused by fapples, we would be none the wiser. The semantics (i.e. wide content) of our perceptual experiences is irrelevant to literally everything we know and how we interact with the world. Our knowledge of the world is limited to our sensory experiences and our deductions, inferences, etc derived from our sensory experiences. Our situatedness in the world is only relevant insofar as it entails the space of our sensory experiences.

>the model would need the ability to frame hypotheses about what this sensor data means, and test them by interacting with the world and seeing what the results were.

Why do we need to actively test our model to come to an understanding of the world? Yes, that is how we biological organisms happen to learn about the world. But it is not clear that it is required. Language models learn by developing internal models that predict the next token. But this prediction implicitly generates representations of the processes that generate the tokens. There is no in principle limit to the resolution of this model given a sufficiently large and diverse training set.

61.
specialist over 2 years ago | on “Ancient civilisation under eastern Turkey estimated to be 11k-13k years old”
Nice. Will read.

One of the many notions from Lingua ex Machina [2001] is the evolution from protolanguage to proper language. From nonrecursive verb-noun clauses to recursive subject-verb-noun clauses.

https://www.amazon.com/Lingua-Machina-Reconciling-Darwin-Cho...

That book had so many intriguing open ended questions, before we had the tech (ability) to verify stuff. IIRC, others were:

the way the neocortex is organized into hexagonal columns, which I think is what Jeff Hawkins (et al) is working on;

how notions compete inside our head using Darwinian processes;

how thinking (consciousness) may be like resonance across our brains, like a song.

Thanks again for the link. Am noob, so just barely grasp this stuff, but am excited nonetheless to learn about more recent findings.

We live in an age of miracles.

--

Summary copypasta from Amazon:

A neuroscientist and a linguist show how evolution could have given rise to structured language.

A machine for language? Certainly, say the neurophysiologists, busy studying the language specializations of the human brain and trying to identify their evolutionary antecedents. Linguists such as Noam Chomsky talk about machinelike "modules" in the brain for syntax, arguing that language is more an instinct (a complex behavior triggered by simple environmental stimuli) than an acquired skill like riding a bicycle.

But structured language presents the same evolutionary problems as feathered forelimbs for flight: you need a lot of specializations to fly even a little bit. How do you get them, if evolution has no foresight and the intermediate stages do not have intermediate payoffs? Some say that the Darwinian scheme for gradual species self-improvement cannot explain our most valued human capability, the one that sets us so far above the apes, language itself.

William Calvin and Derek Bickerton suggest that other evolutionary developments, not directly related to language, allowed language to evolve in a way that eventually promoted a Chomskian syntax. They compare these intermediate behaviors to the curb-cuts originally intended for wheelchair users. Their usefulness was soon discovered by users of strollers, shopping carts, rollerblades, and so on. The authors argue that reciprocal altruism and ballistic movement planning were "curb-cuts" that indirectly promoted the formation of structured language. Written in the form of a dialogue set in Bellagio, Italy, Lingua ex Machina presents an engaging challenge to those who view the human capacity for language as a winner-take-all war between Chomsky and Darwin.

62.
stevenhuang over 1 year ago | on “Weird GPT-4 behavior for the specific string “ davidjl””
Our brains are stochastic parrots. It basically is.

To think otherwise invites mysticism.

> anthropomorphize something that isn't there.

With above as counterexample, you just don't know this.

I otherwise agree in that LLMs in their current form are highly unlikely to give rise to AGI, for many reasons.

But as it stands your argument lacks rigour and actively makes assumptions on matters that remain an open subject of experimental and scientific inquiry (hard problem of consciousness et al).

To emphasize, I want to close that the epistemic position we aught to take is that of uncertainty. We shouldn't be sure something is there, just as we shouldn't be sure something isn't there.

We as yet don't know enough to say one way or the other. That's the point I want to emphasize. Stay open minded until the relevant fields start making stronger claims.

You may find this leading theory on how our brains work interesting: https://en.m.wikipedia.org/wiki/Predictive_coding

63.
TheOtherHobbes about 2 years ago | on “Our brains 'time-stamp' sounds to process the words we hear”
Dennett is making a (naive) philosophical point about the nature of consciousness, not a linquistic point about the nature of language perception.

You don't need a Cartesian Theatre, or even strict reassembly. I'd expect something more like a hierarchical/nested structure of template recognition for common word sequences and sentence structures.

Brains notice certainly when out of order are words. But brains can still make sense of them - with some extra effort - as long as there's still some templated structure left to work with.

You need to randomise much longer sequences before the templating breaks down.

None of this says anything relevant about what consciousness may or may not be. It's still the same old problem of qualia, only now it's about qualia that are perceived as linguistic and conceptual relationships, not trivial perceptual identifications. ("Dog", "orange", "philosopher", etc.)

64.
throwawaymaths over 1 year ago | on “GPT-4 can't reason”
why do you say that? You don't think stochastic pattern matching can feature reasoning as an emergent property? I do.

A stochastic parrot doesn't just mimic things totally randomly. It reinforces what it's seen.

65.
platz over 1 year ago | on “Ways to say no with grace and style (2021)”
The author clearly has a bias towards categorizing people as Essentialist or Non-Essentialist.

Exactly how in this context do those terms make sense ?

66.
flohofwoe over 2 years ago | on “Fourth-generation programming language”
IMHO the idea that the history and future of programming languages follows a pre-determined ladder, or 'generations' is misguided. More abstactions isn't necessarily better then more explicitness. Functional style isn't necessarily better than OOP, and OOP isn't necessarily better than 'imperative style'. All the different languages will not converge into a "perfect" language some time in the future, if anything, it looks like the opposite is true.

I sometimes wonder if Marxism has a bigger influence in some "language designer circles" than (for instance) Darwinism ;)



